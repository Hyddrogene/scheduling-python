### 6. Conclusions

Nous avons développé un cadre de modélisation ontologique généralisée et une solution pour résoudre une vaste gamme de problèmes de planification des emplois du temps dans ce travail. La nécessité d'une telle généralité est profondément ancrée dans la littérature, qui souligne la fragmentation dans la gestion des problèmes de planification des emplois du temps à travers divers domaines. Cela a également conduit à un ad-hocisme limitant le développement d'approches de solution robustes de haut niveau, qui ne parviennent pas à exploiter la structure commune sous-jacente des problèmes. Ce manque de focalisation sur la structure du problème est attribué à la dépendance excessive aux métaheuristiques, qui ont initialement montré une promesse face aux lacunes computationnelles des méthodes analytiques. Cependant, les inefficacités computationnelles des métaheuristiques sont également apparues à mesure que la taille des problèmes augmentait. Malgré cela, la tendance continue de croître monotonement, ce qui a soulevé la critique susmentionnée (Lee et al., 2005; Sørensen, 2015).

Pour faire face à ces problèmes, nous contribuons d'abord en consolidant les approches de modélisation des problèmes dans divers domaines en utilisant une nouvelle ontologie de modélisation de planification des emplois du temps unifiée, c'est-à-dire GTP. Deuxièmement, nous proposons une approche de solution en deux étapes unique, qui exploite la structure commune sous-jacente des problèmes pour déterminer des solutions élites qui fournissent un point de départ robuste pour les métaheuristiques afin de trouver une solution optimale ou quasi-optimale à un coût computationnel très faible. Pour démontrer l'efficacité de la méthodologie, nous avons utilisé un problème très récent de planification des cours. Les résultats indiquent que même pour des problèmes de grande taille, les solutions initiales générées étaient presque toujours raisonnablement proches de leur borne inférieure. Pour améliorer ces solutions en utilisant une métaheuristique intégrée, nous avons employé l'algorithme parallèle-EAR-CA-SA et montré comment, avec seulement quelques centaines d'itérations, les solutions sont améliorées à un point où l'écart avec la borne inférieure descend jusqu'à 3,5 %.

La principale limitation de notre travail est qu'il n'est testé que sur un domaine majeur de problème de planification des emplois du temps, à savoir la planification des cours. De plus, son intégration est testée avec une métaheuristique simple basée sur un seul élément, tandis que pour généraliser la performance de l'intégration EAR-CA, des tests avec d'autres méthodes basées sur un seul élément et des méthodes basées sur la population sont nécessaires. Cela met également en évidence les défis significatifs rencontrés dans la direction proposée par notre travail, à savoir consolider les efforts de modélisation d'un domaine très ancien mais hautement fragmenté. Ainsi, montrer de manière concluante son efficacité interdomaines est difficile. De plus, comme il existe une vaste suite de métaheuristiques, le même défi est rencontré pour démontrer la facilité d'intégration d'EAR-CA avec toutes ces techniques. Par conséquent, en termes de développement futur, nous visons à 1) tester l'efficacité de l'approche de solution proposée pour les scénarios multi-objectifs CCTSS, ce qui implique des solutions Pareto optimales non dominées (par exemple, lorsque des objectifs supplémentaires tels que la maximisation de l'utilisation des ressources et la minimisation des coûts d'exploitation sont pris en compte); 2) personnaliser et tester l'approche dans des domaines autres que la planification des cours afin que son efficacité générale puisse être analysée; 3) développer davantage la méthode pour son intégration avec d'autres métaheuristiques et hyper-heuristiques basées sur un seul élément et sur la population; et 4) concevoir des stratégies de résolution de conflits plus intelligentes et efficaces pour améliorer encore les performances de la méthode EAR-CA.

### 1. Introduction

La planification des emplois du temps est un problème récurrent de planification combinatoire impliquant des allocations de temps faisables pour des ressources rares pour des services assignés (Wren, 1995). Les formes les plus courantes de ce problème sont la planification des cours, des examens, des transports de passagers, des ligues sportives et des emplois du temps du personnel (Ernst, Jiang, Krishnamoorthy, & Sier, 2004; Pillay, 2016). Ces problèmes ont attiré une attention de recherche incessante dans le développement d'approches de solution efficaces, car les techniques analytiques standard deviennent inefficaces avec l'augmentation de l'échelle de ces problèmes (de Werra, 1997; Qaurooni & Akbarzadeh-T, 2013). Ainsi, l'accent s'est tourné vers des méthodes alternatives telles que les heuristiques et les métaheuristiques (Bashab et al., 2020). Bien que cette direction ait initialement montré une promesse, les tentatives de résoudre des problèmes beaucoup plus grands ont conduit à des critiques en raison de leurs coûts exorbitants d'échantillonnage des solutions faisables. Malgré cela, l'accent est resté sur l'essai de diverses métaheuristiques et non sur l'exploitation de la structure innée du problème pendant le processus de recherche (Lee, Ma, Lai, Hsueh, & Fanjiang, 2005; Sørensen, 2015). Nous soutenons en outre que cette utilisation ad hoc des techniques de solution, en plus de la fragmentation interdomaines de la modélisation existante, a dissuadé tout progrès consolidé du domaine - une préoccupation exprimée au cours de diverses époques de développement (Acampora, Loia, Salerno, & Vitiello, 2012; Lee et al., 2005; Miles, 1975).

Ces préoccupations profondément enracinées nous ont motivés pour une étude interdomaines, où des structures de planification des emplois du temps communes pourraient être consolidées en un cadre de problème unifié. Une telle consolidation vise à 1) réduire l'hétérogénéité ontologique interdomaines dans la littérature actuelle sur la planification des emplois du temps; 2) développer des approches conjointes de modélisation et de solution pour la planification des emplois du temps; 3) exploiter la structure générale du problème dans les approches de solution, et 4) créer une capacité d'adaptation rapide lors de la gestion de variantes de problèmes.

Ce document contribue ainsi par un cadre de modélisation et de solution en deux volets. Premièrement, il propose une nouvelle ontologie de modélisation unifiée pour la planification des emplois du temps - le premier effort vers une telle consolidation - en définissant un ensemble général de constructions et leur structure de relation. Deuxièmement, il offre une approche de solution en deux étapes unique pour résoudre le problème. La première étape de cette méthode exploite la structure du problème pour générer des solutions initiales faisables ou élites de haute qualité (Jaradat, Ayob, & Almarashdeh, 2016). Dans la deuxième étape, ces solutions élites sont améliorées en utilisant toute technique métaheuristique standard à un coût computationnel beaucoup plus faible. Par conséquent, ce cadre de haut niveau facilite la cartographie de l'anatomie de tout problème de planification des emplois du temps réel sur sa structure générale, suivie de l'application de la méthode de solution proposée.

En conséquence, pour construire l'ontologie de modélisation, nous avons analysé de nombreux problèmes de planification des emplois du temps de divers domaines d'application (détails dans la section 2), ce qui a permis d'identifier ses constructions et sa structure de relation - dorénavant appelée le Problème Général de Planification des Emplois du Temps ou GTP (défini formellement dans la section 3). En général, le GTP implique des entités (comme des patients, des étudiants ou des clients) qui cherchent à saisir (pour une fois ou de manière récurrente) des ressources rares (humaines ou physiques). Cette saisie est effectuée pour un temps limité afin d'accomplir des activités planifiées, sous réserve de règles d'intégrité ou commerciales (contraintes). Ces affectations entité-ressource sont mappées sur des maillages d'emplois du temps ayant des créneaux horaires ou des cellules discrets. La figure 1 illustre trois maillages de planification des cours connexes, un pour un groupe d'étudiants (entité), un pour un instructeur (ressource), et un pour une salle (ressource). Les blocs colorés montrent des créneaux horaires réservés pour les cours (activités) d'un cours spécifique. Bien que ces maillages soient identifiés indépendamment, leurs cellules colorées sont alignées dans le temps pour former une solution faisable. Pour la solution faisable complète de l'ensemble du problème, tous les maillages connexes doivent montrer des alignements temporels complets. Nous avons donc nommé le cadre comme le Cadre d'Alignement Cellulaire Entité-Activité-Ressource (EAR-CA) basé sur ces concepts de GTP et d'alignement cellulaire.

Comme indiqué précédemment, la méthode de solution EAR-CA a deux étapes, c'est-à-dire, trouver des solutions initiales élites puis améliorer ces solutions vers l'optimalité. Pour trouver des solutions élites, nous utilisons une approche simple de division et de conquête sur la structure du GTP (section 3.2), c'est-à-dire, les solutions

 pour toutes les activités sont construites séparément en utilisant les ensembles de maillages entité-ressource respectifs. Il est à noter que cette approche permet initialement des affectations simples, tandis que les conflits temporels/spatiaux peuvent apparaître de plus en plus vers la fin. Ces conflits sont ensuite résolus en utilisant des stratégies de résolution de conflits. Ces solutions élites sont ensuite améliorées en utilisant toute technique métaheuristique standard.

Pour appliquer le cadre EAR-CA à une application réelle, nous avons considéré une nouvelle variante du problème difficile de Planification des Cours Basée sur le Curriculum avec Sectionnement des Étudiants ou problème CCTSS (Siddiqui, Raza, & Tariq, 2018). Le CCTSS est une généralisation des problèmes de Planification des Cours Basée sur le Curriculum (CCT), de Planification des Cours Basée sur l'Inscription Postérieure (PECT) et de Sectionnement des Étudiants (SS) (détails dans la section 2), qui sont basés respectivement sur les exigences du curriculum, les choix des étudiants et le sectionnement des cours. Dans l'analyse numérique, nous avons d'abord considéré un scénario réel rencontré par une grande école de commerce, qui a ensuite été considérablement augmenté pour tester rigoureusement les performances de l'EAR-CA. De plus, l'effet des variations de la disponibilité des ressources est également analysé. Les résultats démontrent que pendant la première étape, la méthode EAR-CA est capable de trouver des solutions efficacement (détails dans la section 5). L'écart avec la borne inférieure pour la plupart de ces solutions élites était d'environ 20 % ou moins. Dans la deuxième étape, nous avons utilisé le recuit simulé pour exemplifier les améliorations des solutions élites en utilisant une métaheuristique à un coût computationnel minimal (section 3.3). Les résultats montrent que, en seulement 300 itérations, la plupart des solutions convergent avec un écart atteignant aussi bas que 3,5 % avec la borne inférieure.

Le reste du document est structuré comme suit : dans la section 2, nous présentons la revue de la littérature connexe, suivie des détails du cadre EAR-CA dans la section 3. La section 4 démontre comment un problème de planification des emplois du temps, c'est-à-dire le CCTSS, est mappé sur le GTP, tandis que la section 5 discute des performances numériques du cadre. Enfin, la section 6 présente les conclusions et les orientations futures de la recherche.


### 2. Revue de la littérature

Cette section organise notre discussion autour de deux thèmes. Le premier thème est centré sur l'identification des différentes applications de planification des emplois du temps, ainsi que leurs principales constructions. Cela fournit la base pour le développement de l'ontologie GTP. Le second thème tourne autour de l'utilisation et de l'évolution des méthodologies de solution. Étant donné qu'une recherche exhaustive de la littérature est prohibitive, nous nous sommes principalement appuyés sur la littérature récente dans les journaux répertoriés par Clarivate Analytics JCR (Journal Citation Reports - jcr.clarivate.com). Cependant, nous renvoyons également les lecteurs à ces revues (Ernst et al., 2004; Parbo, Nielsen, & Prato, 2016; Pillay, 2016; Schaerf, 1999) pour une compréhension complète des développements dans le domaine au fil des années.

#### 2.1. Problèmes courants de planification des emplois du temps

Nous avons d'abord examiné les travaux pour identifier les problèmes de planification des emplois du temps dans des domaines tels que l'éducation, la gestion du personnel, le transport de passagers et la planification des sports. La motivation était d'unifier leurs bases ontologiques en un modèle unique, c'est-à-dire le GTP. Une vue d'ensemble de ces domaines révèle qu'il y a toujours des entités cherchant des ressources rares, pour un temps limité, afin d'accomplir les activités requises, pour lesquelles il existe naturellement des maillages de planification des entités et des ressources pour leur coordination. Le seul facteur différenciant dans ces problèmes est leurs contraintes commerciales souples ou rigides. En identifiant ces contraintes comme des règles, une structure de problème standard émerge sous la forme du GTP. Le tableau 1 identifie les constructions du GTP couramment trouvées dans divers domaines obtenues à partir des études notables répertoriées.

Nous avons également trouvé des sous-domaines dans ces domaines avec des variations dans les entités, les ressources, les activités et les règles. Par exemple, deux pistes de haut niveau dans la planification des emplois du temps éducatifs sont construites autour des activités spécifiques des cours et des examens. Même dans la planification des cours, nous avons trouvé qu'il existe au moins trois sous-domaines, c'est-à-dire les emplois du temps construits autour du curriculum (CCT), des choix des étudiants après l'inscription (PECT), ou du sectionnement des étudiants (SS), ou une combinaison de ces derniers. Bien que les règles varient selon ces problèmes, nous avons également trouvé des points communs dans les contraintes et les objectifs. Ceux-ci incluent la non-contradiction, l'exhaustivité, l'uniformité, la compacité et l'utilisation des ressources (Bardadym, 1996; E. Burke, Jackson, Kingston, & Weare, 1997; Daskalaki, Birbas, & Housos, 2004; Post, Di Gaspero, Kingston, McCollum, & Schaerf, 2016).

De même, dans la planification des personnels et des tâches, nous avons trouvé des pistes axées sur la planification des horaires dans les soins de santé, la fabrication et l'industrie des services. Les différences résident principalement dans les règles commerciales de leurs scénarios respectifs. De même, plusieurs variantes existent pour différents modes de transport (par exemple, les bus et les trains) et leur scénario commercial sous-jacent dans le transport de passagers. Un paysage similaire a été trouvé dans la planification des sports, avec des variations enracinées dans les entités (équipes vs. appariement des équipes vs. joueurs individuels) et les ressources (installations), ainsi que dans les règles commerciales.

#### 2.2. Approches de solution pour la planification des emplois du temps

Dans la deuxième partie de la revue, nous nous sommes concentrés sur les méthodologies de solution utilisées pour résoudre les problèmes de planification des emplois du temps. Les travaux de Miles (1975), Schmidt et Ströhlein (1980) et Carter (1986) ont passé en revue les premiers développements dans le domaine. Dans une étude de suivi, Carter et Laporte (1995) ont résumé les approches de solution utilisées entre 1986 et 1996 en les classant en méthodes de cluster, méthodes séquentielles, métaheuristiques et techniques basées sur les contraintes. En s'appuyant sur ces travaux, E. Burke et al. (1997) ont signalé un changement dans les approches de solution, passant des heuristiques spécifiques aux problèmes vers les métaheuristiques en raison de l'augmentation de la taille et de la complexité des problèmes. Les métaheuristiques utilisées pendant cette période étaient principalement le recuit simulé, les algorithmes génétiques et la recherche tabou. E. K. Burke et Silva (2005) ont ensuite publié une revue couvrant les méthodes d'algorithmes mémétiques dans la planification des emplois du temps.

En se concentrant spécifiquement sur la planification des emplois du temps éducatifs, Silva, Burke et Petrovic (2004) ont passé en revue les métaheuristiques multi-objectifs pour la planification des emplois du temps éducatifs couvrant des techniques multi-phasées et multi-critères. De même, Lewis (2008) a résumé l'utilisation des algorithmes évolutionnaires, de l'optimisation par colonie de fourmis, de la recherche tabou, du recuit simulé et de la recherche locale itérée en les classant en approches d'optimisation à une ou deux étapes. Dans la planification des examens, R. Qu, Burke, McCollum, Merlot, et ont identifié la domination des métaheuristiques et de leur hybridation. Pour la planification des cours universitaires, MirHassani et Habibi (2013) ont couvert les approches de solution de 2000 à 2010 dans les catégories des méthodes séquentielles, de clustering, basées sur les contraintes et métaheuristiques. De même, Pillay (2014), Teoh, Wibowo et Ngadiman (2015), et Vrielink, Jansen, Hans et van Hillegersberg (2019) ont analysé et comparé l'utilisation de ces techniques et de plusieurs autres nouvelles techniques telles que les algorithmes des abeilles, les réseaux neuronaux, la logique floue, l'optimisation par essaim de particules, l'algorithme d'acceptation par seuil et les hyper-heuristiques.

Selon les conclusions des travaux ci-dessus, aucune approche ne se révèle de manière concluante supérieure aux autres techniques. En général, nous avons également observé une tendance initiale à employer des approches analytiques telles que la modélisation de décision basée sur l'optimisation ou les méthodes basées sur les systèmes experts (Lee et al., 2005). À mesure que l'accent s'est déplacé vers la résolution de problèmes de grande taille, une inclination vers des méthodes alternatives telles que les heuristiques et les métaheuristiques est apparue. Ce modèle a continué de croître depuis lors (Fig. 2). Selon notre analyse des 106 études récentes (Tableau 2), nous voyons que seulement 20 s'appuient sur des méthodes analytiques. Bien qu'il y ait des efforts pour développer des heuristiques personnalisées (25 sur 106 études), une forte dépendance reste sur les métaheuristiques (53 sur 106 études). Nous voyons également une utilisation mixte de la solution unique (par exemple, recuit simulé, recherche tabou, recherche locale itérée et déluge) et basée sur la population (par exemple, algorithme génétique, optimisation par essaim de particules). De plus, l'émergence de techniques hybridées est également en augmentation.

Comme la planification des cours est le domaine le plus actif, nous renvoyons en outre nos lecteurs à une analyse très récente de Thepphakorn et Pongcharoen (2020) sur l'utilisation de diverses techniques dans ce domaine. Ces auteurs ont fourni une analyse complète, résumée dans la figure 3 ci-dessous. De toute évidence, de nombreuses techniques ou leur hybridation ont été testées, dont 30 seulement une fois. Les plus populaires étant le recuit simulé basé sur une solution unique, la recherche tabou et la recherche locale itérée.

#### 2.3. Lacunes et contributions

Sur la base de cette analyse, nous identifions maintenant les lacunes et les contributions apportées par ce travail. Comme discuté dans la section 2.1, bien qu'une structure commune à tous les domaines de la planification des emplois du temps existe (Tableau 1), elle n'a pas été identifiée et modélisée formellement à ce jour. Ainsi, notre première contribution majeure de recherche vise à traiter l'hétérogénéité ontologique existante présente dans la littérature, qui a entravé le développement de toute approche de solution généralisable pour la planification des emplois du temps. Nous comblons cette lacune en proposant le modèle GTP, qui fournit des définitions formelles de ses constructions ontologiques et de leurs relations (section 3).

En termes de progrès sur les méthodologies de solution (section 2.2), la forte dépendance à l'égard de l'utilisation des métaheuristiques continue de croître sans aucune exploitation évidente de la structure sous-jacente du problème (Lee et al., 2005; Sørensen, 2015). Comme deuxième contribution majeure, nous abordons cette question à travers une

 nouvelle approche de solution en deux étapes. La première étape génère des solutions initiales d'élite en exploitant la structure GTP. Ces solutions sont ensuite améliorées via une métaheuristique standard à un coût computationnel beaucoup plus faible. Comme mentionné précédemment dans la section 1, résoudre un exemple réel de problème de planification des emplois du temps nécessite qu'il soit mappé sur le modèle GTP et qu'il utilise ensuite la méthode de solution en deux étapes EAR-CA.


### 3. Cadre EAR-CA

Cette section présente le cadre complet EAR-CA. Nous commençons par délimiter le modèle GTP dans la section 3.1. La première étape de la méthode de solution pour générer des solutions d'élite est présentée dans la section 3.2, suivie des détails des métaheuristiques basées sur EAR-CA dans la section 3.3, qui constitue la deuxième étape de la méthode de solution.

#### 3.1. Modèle du problème général de planification des emplois du temps (GTP)

Pour formuler formellement le GTP, nous définissons d'abord ses principales constructions et leur structure de relation (Fig. 4), qui constitue la base de l'approche de solution EAR-CA proposée.

Il y a six constructions dans le modèle GTP : les entités, les ressources, les activités, les e-maillages, les r-maillages et les règles. Chacune de ces constructions est liée aux autres de différentes manières. Par exemple, une ou plusieurs entités peuvent saisir une ou plusieurs ressources. Les entités et ressources liées réalisent alors simultanément certaines activités planifiées dans des créneaux réservés, en suivant des règles commerciales ou d'intégrité. La notion de créneaux correspond directement aux cellules dans les e-maillages et r-maillages. Autrement dit, pour qu'une activité soit réalisée, un nombre fini de cellules doit être simultanément réservé par les entités et ressources liées dans leurs e-maillages ou r-maillages correspondants. Comme noté précédemment, la nature de la relation peut changer, par exemple de un à un ou de un à plusieurs, selon l'instance spécifique de GTP. Cependant, la structure générale reste invariable. Ainsi, résoudre une instance de problème réel nécessite d'abord de mapper explicitement ses éléments sur cette structure générale de constructions et de relations. Pour faciliter un mappage approprié et bien défini, des définitions formelles des constructions ci-dessus, ainsi que leurs sous-concepts, sont présentées comme suit (les notations sont fournies en annexe) :

**Définition 1 :** Une entité ej ∈ Ej⊂E de type j (Ej étant l'ensemble des entités de type j = 1, ..., J, E le super-ensemble des entités) est un objet en compétition pour saisir des ressources rares pour un bloc de temps programmé (par exemple, des étudiants, des passagers et des patients recherchant des ressources respectives).

**Définition 1. a :** Un ensemble de propriétés d'entité pej ∈ P (P étant le super-ensemble) contient tous les attributs liés à une entité ej ∈ Ej (par exemple, l'année et la taille d'une classe, le type d'un patient, etc.).

**Définition 2 :** Une activité aej ∈ Aej (Aej est l'ensemble de toutes les activités ej ∈ Ej) représente un acte planifié de durée finie par l'entité ej ∈ Ej, nécessitant des ressources rares pour le compléter (par exemple, un cours pour des étudiants nécessitant un instructeur et des salles de classe, ou un patient nécessitant un examen médical par un médecin).

**Définition 3 :** Une ressource ri ∈ Ri⊂R de type i (Ri étant l'ensemble des ressources de type i = 1, ..., I, et R son super-ensemble) est un objet rare disponible pour effectuer certaines activités GTP pendant un créneau horaire programmé (par exemple, des travailleurs, des salles, des équipements).

**Définition 3. a :** Un ensemble de propriétés de ressource prj ∈ P (P étant le super-ensemble) contient tous les attributs pertinents du GTP pour une ressource ri ∈ Ri⊂R.

**Définition 3. b :** Une ressource potentielle ri aej ⊂R est une ressource spécifique de type i qu'une entité ej ∈ Ej peut utiliser pour compléter son activité aej.

**Remarque :** Pour une activité, nous pouvons avoir plusieurs ressources en compétition de types respectifs (par exemple, des salles ou des instructeurs enseignant un cours). Le GTP implique donc à la fois le choix d'une ressource parmi l'ensemble des ressources potentielles et leur assignation simultanée dans les créneaux horaires disponibles.

**Définition 4 :** Un e-maillage (e-maillage) est une matrice (m, n)ej pour une entité ej, où chacune des m × n cellules représente un créneau horaire qui peut être réservé (en parallèle avec d'autres cellules) pour ses activités aej ∈ Aej (Fig. 5).

**Définition 4. a :** Il y a |E| e-maillages distincts dans toute instance de problème GTP. Tous les e-maillages forment une couche de e-maillages (Fig. 6).

**Définition 5 :** Un r-maillage (ri-maillage) est une matrice (m, n)ri de ressource ri, où chacune des m × n cellules représente un créneau horaire qui peut être simultanément réservé pour une activité liée ae ∈ Ae.

**Définition 5. b :** Il y a |Ri∈I| r-maillages dans un système. Tous ces r-maillages forment une couche de r-maillages (Fig. 7).

**Remarque :** Tous les maillages dans le système, sans perte de généralité, sont supposés avoir la même structure m × n. De plus, tout scénario GTP peut être accommodé en sélectionnant correctement la taille du maillage et en formant des règles sur les assignations de cellules (par exemple, une activité peut utiliser deux créneaux horaires consécutifs si elle nécessite deux fois plus de temps qu'un créneau donné).

**Définition 6 :** uej ∪ uri ∈ U ∀e ∈ E, ri ∈ Ri sont l'ensemble respectif des règles (U étant l'ensemble de toutes les règles) que les entités et les ressources doivent suivre lors d'une assignation de cellule.

#### 3.2. Première étape de la méthode de solution EAR-CA

Après avoir défini les constructions GTP, nous présentons maintenant la première étape de la méthode de solution EAR-CA proposée, pour laquelle quelques concepts supplémentaires sont définis ci-dessous.

**Définition 7 :** Un lien entité-activité-ressource (EAR-Link) est une solution réalisable représentée comme un quadruple (ej, aej, {assigned ri}, {(p, q)}), où {(p, q)} est l'ensemble des emplacements de cellules réservées en commun.

**Proposition 1 :** Une solution de planification faisable (c'est-à-dire un EAR-Link) ne peut être obtenue qu'en alignant simultanément les cellules {(p, q)} des e-maillages et r-maillages liés (sous réserve des règles).

**Preuve :** Cette proposition est simplement motivée par la structure (Fig. 8) du problème. Par exemple, un cours ne peut pas avoir lieu à moins que les étudiants, les instructeurs assignés et les salles soient tous disponibles.

**Corollaire :** Une solution réalisable au problème complet GTP est obtenue lorsque tous les liens EAR ont des alignements simultanés.

Ceci est dérivé de la proposition 1. Autrement dit, lorsque tous les liens EAR individuels dans le système sont réalisables, la solution complète au GTP est également réalisable.

**Définition 8 :** Suivant la Définition 7 et la proposition 1, un lien EAR peut être déterminé pour une activité en recherchant des ressources appropriées et des cellules vacantes simultanées sur les maillages respectifs qui sont réservés et placés dans un lien EAR.

**Définition 8. a :** La recherche nécessite un pivot d'assignation comme base utilisée pour initier cette recherche. La recherche peut être pivotée soit sur des entités soit sur des ressources, par exemple, une entité est un pivot si elle conduit la recherche de cellule, tandis que les assignations de ressources suivent.

**Définition 8. b :** Suivant la Définition 8a, une séquence de pivot dicte la séquence d'évaluation pour le pivot d'assignation, par exemple, si l'entité est utilisée comme pivot, alors au sein de l'ensemble d'entités, les assignations peuvent être effectuées en utilisant une séquence aléatoire, arbitraire ou basée sur des règles.

**Définition 8. c :** Une méthode de séquence de recherche de cellule dicte la séquence d'évaluation des cellules en considération, par exemple, une cellule appropriée peut être recherchée de manière aléatoire ou en utilisant une séquence de cellules arbitraire ou basée sur des règles.

**Définition 8. d :** Suivant les Définitions 8a-c, une stratégie de résolution de conflit emploie une procédure intelligente pour traiter un scénario où une recherche exhaustive pour une assignation donnée ne conduit à aucune solution réalisable pour cette activité. Dans ce cas, une stratégie est nécessaire pour la résolution, par exemple, soit par un changement temporaire du pivot d'assignation, soit en abandonnant certains liens EAR voisins, soit en déplaçant un lien assigné loin de la cellule conflictuelle.

Un exemple de conflit courant et de sa résolution est illustré dans la Fig. 9, impliquant 5 cours

 (2 leçons par semaine chacun et aucun cours le jeudi) à assigner avec une règle supplémentaire de pas plus de trois cours consécutifs par jour. Les cellules grisées dans les rangées du haut et du bas sont donc interdites d'utilisation avec une assignation initiale des cours C1-C4. Pour résoudre ce conflit en utilisant une stratégie de déplacement de créneau (Slot-Shift), l'un des cours est d'abord déplacé vers le haut ou vers le bas (par exemple, C2 déplacé arbitrairement vers le bas dans ce cas) dans un créneau vacant, laissant de la place pour C5. Veuillez noter que l'alignement simultané avec d'autres maillages de ressources doit être appliqué avant de faire un changement. Alternativement, en utilisant la stratégie 2 : abandon de maillage (Drop-Mesh), tous les liens EAR sont abandonnés, et une recherche complète est effectuée pour trouver une solution complète. La deuxième stratégie est évidemment plus flexible car elle ouvre à nouveau toutes les options.

En utilisant ces concepts, nous présentons maintenant l'algorithme des méthodes de solution EAR-CA dans le Tableau 3. L'algorithme définit d'abord les entités, les ressources et leurs attributs, suivis des activités. Les activités et les ressources sont ensuite liées en définissant les ressources potentiellement disponibles. Les e-maillages et les r-maillages sont ensuite formés avec les ensembles de règles, les pivots, les stratégies nécessaires et les liens EAR vides. Une fois que toutes les constructions sont initialisées, la recherche de solution est lancée en fonction du pivot d'assignation et de leur stratégie de séquence. Pendant la recherche, tout d'abord, une cellule vacante appropriée (ou un groupe de cellules) est déterminée, pour laquelle les ressources appropriées sont recherchées parmi les ressources potentielles et disponibles. La solution est évaluée pour la conformité aux règles. Si elle est satisfaite, la solution est enregistrée en mettant à jour le lien EAR correspondant. Si cette recherche exhaustive devient inutile, une stratégie de résolution de conflit est employée. Si une solution est trouvée, elle est enregistrée, sinon l'algorithme est terminé en raison de l'absence de solution.

#### 3.3. Métaheuristiques basées sur EAR-CA

Comme nous l'avons souligné précédemment, la méthode EAR-CA de première étape trouve des solutions faisables initiales. Comme celles-ci sont basées sur la structure et les règles du GTP, on s'attend à ce qu'elles produisent des solutions de bonne qualité (ou d'élite). Une fois les solutions d'élite en place, nous avons soutenu qu'une méthode métaheuristique appropriée peut alors être utilisée pour rechercher une solution optimale ou quasi-optimale à un coût computationnel beaucoup plus faible. Alors que plusieurs approches concurrentes basées sur une solution unique et sur une population ont été proposées dans la littérature (section 2.2, Fig. 3), nous proposons l'algorithme de recuit simulé basé sur EAR-CA (EAR-CA-SA). Ce choix est motivé par la simplicité de la méthode, visant à démontrer l'intégration transparente d'une métaheuristique avec la première étape EAR-CA. De plus, le recuit simulé a montré à plusieurs reprises son succès dans le domaine de la planification des cours (Fig. 3). Enfin, nous fondons également notre choix sur l'argument du Théorème du No Free Lunch, qui suggère l'attente générale de performances comparables entre différentes approches (Adam, Alexandropoulos, Pardalos, & Vrahatis, 2019).

L'algorithme proposé est présenté dans le Tableau 4. Nous notons également que, puisqu'il s'agit d'une méthode de recherche basée sur une solution unique, nous avons parallélisé cet algorithme pour améliorer les performances. Autrement dit, au lieu de tenter l'amélioration de manière sérialisée, la parallélisation en n permet d'améliorer la même solution de départ dans n directions.

Dans l'algorithme ci-dessus, nous notons qu'une solution d'élite initiale S est obtenue en utilisant l'algorithme EAR-CA de première étape (section 3.2), c'est-à-dire formée comme l'ensemble de tous les liens (ej, aej, {assigned ri}, {(p, q)}) alignés simultanément présents dans le système. Alors que le même codage de solution est utilisé pendant le recuit simulé, nous notons qu'une solution concurrente S dans le voisinage de S est simplement celle où au moins certains des liens EAR sont partagés, c'est-à-dire que plus le partage est important, plus le voisin est proche. En ce sens, un S concurrent est obtenu en abandonnant partiellement et en trouvant des liens EAR à partir de S en utilisant l'algorithme EAR-CA de première étape. Pour conduire ce processus, nous recourons au schéma de codage de priorité (Nowling & Mauch, 2011), où les informations sur les liens à abandonner sont transmises à l'algorithme plutôt que la solution S' directement. Il est raisonnable que ces liens soient liés en termes de cours et d'instructeurs partagés. En conséquence, une année académique est identifiée au hasard, tandis que les liens EAR liés à abandonner dans cet ensemble sont sélectionnés au hasard en fonction de la température actuelle. Une fois sélectionnées, les informations sont transmises à l'algorithme qui les utilise pour trouver S', qui est ensuite employée dans le processus de recuit simulé.

Nous mentionnons également que malgré l'utilisation recommandée de stratégies de refroidissement adaptatif dans la littérature (Azizi & Zolfaghari, 2004), nous avons eu recours au recuit simulé conventionnel (CSA). Ce choix est dû au fait que la stratégie de refroidissement adaptatif a montré des améliorations marginales ou nulles (par rapport au CSA), bien que pour un coût computationnel beaucoup plus élevé. Ce comportement contre-intuitif est peut-être dû aux faibles écarts avec la borne inférieure trouvée pour les solutions initiales d'élite, rendant le réchauffement adaptatif contre-productif, amenant l'algorithme à s'échapper vers des solutions de moindre qualité.

### 4. Scénario CCTSS et son mappage

Résoudre un problème réel implique de mapper ses constructions et ses relations sur le modèle GTP avant d'employer les méthodologies de solution EAR-CA. Pour démontrer ce mappage, nous utilisons une variante du problème de planification des cours basés sur le programme avec sectionnement des étudiants (CCTSS) (Siddiqui et al., 2018). Un scénario réel de cette variante est décrit ci-dessous (notations en annexe).

Le CCTSS est un problème typique de planification des cours universitaires/collegiales. Notre scénario CCTSS englobe un ensemble de départements offrant des cours (A – indexés a). Certains de ces départements hébergent des programmes de diplôme complets, tandis que les autres sont des départements de service offrant des cours généraux à l'échelle du collège. Bien que tous les départements aient leur propre personnel enseignant, ils partagent des ressources telles que des salles de classe et des équipements. Dans notre cas, trois programmes de baccalauréat en commerce sont offerts (les structures des programmes sont montrées dans la Fig. 10). Chaque programme est structuré autour de huit semestres. Au cours de chaque semestre, un étudiant régulier doit suivre cinq cours offerts sous forme de conférences bihebdomadaires d'une heure et demie. Les cours offerts au cours des trois premiers semestres sont tous de niveau collégial, partagés entre tous les programmes. Ces cours peuvent être offerts par l'un des trois départements offrant des diplômes, à savoir, les systèmes d'information, la finance et la comptabilité, ou les départements de service, à savoir, l'économie, la gestion, le marketing, les méthodes quantitatives et le droit. Au quatrième semestre, le premier cours lié au programme est offert par les départements respectifs. C'est également à ce moment que les étudiants choisissent un programme de diplôme spécifique.

Chaque année, le collège fait face à de nouvelles inscriptions par centaines. Cela crée une situation de capacité, nécessitant que certains cours soient offerts en plusieurs sections. Par exemple, le cours ECO201 est actuellement offert en 8 à 12 sections. Ainsi, pour un département « a » offrant un ensemble de cours Ca (indexé ca), le problème est d'estimer d'abord le nombre de sections requis pour chaque cours, c'est-à-dire S(ca) ∈ S (indexé s(ca) et S étant l'ensemble de toutes les sections, ou SF(ca)⊆S(ca) et SM(ca)⊆S(ca) dans le cas de sections séparées pour les hommes et les femmes). Ainsi, le problème de planification des emplois du temps pour ce scénario consiste à créer diverses combinaisons faisables de sections pour tous les cours requis. Un concept crucial pour opérationnaliser ce sectionnement des étudiants basé sur le programme est celui des cohortes d'étudiants. Plus précisément, nous supposons des groupes d'étudiants ou cohortes Gp y (indexés gp y), formés par année académique y par programme p ∈ P, P étant l'ensemble de tous les programmes. Par exemple, gp y = 10 1 est la première cohorte de l'année 1 et du programme 0. Ainsi, les emplois du temps des cours sont construits sur ces cohortes. Plus précisément, pour chaque cohorte de l'année y et du programme p, il doit y avoir au moins une section faisable de l'ensemble de tous les cours requis. Ainsi, le nombre minimum de combinaisons cours-section disponibles pour les étudiants est égal au nombre de cohortes, c'est-à-dire |Gp y|. Une fois le nombre de sections connu, les départements respectifs élaborent des plans d'enseignement, identifiant l'ensemble des instructeurs I(ca) enseignant un cours ca.

Pour élaborer des emplois du temps pour ce qui précède, le collège dispose de plusieurs salles de classe réparties sur plusieurs bâtiments, c'est-à-dire R(b) (indexé r(b)) est l'ensemble des salles dans le bâtiment b ∈ B, chacune ayant une capacité CAPr(b). Dans notre cas, nous avons deux bâtiments. Pour répondre aux exigences du programme de deux cours d'une heure et demie par semaine, le collège utilise le schéma présenté dans le Tableau 5, c'est-à-dire que deux paires de conférences sont utilisées – une paire le dimanche et le mardi et l'autre le lundi et le mercredi. Étant donné que certains créneaux horaires ne sont pas disponibles le mercredi, les paires lundi-jeudi sont disponibles en alternative. L'utilisation de ce schéma est illustrée à la Fig. 11, qui concerne le groupe de cohortes 1 suivant les cours du semestre 1 (Fig. 10). Dans chacun de ces cours, le dernier numéro séparé par un tiret représente le numéro du groupe de cohortes.

Ainsi, planifier une section particulière s(ca) implique de choisir simultanément un instructeur I(ca), une salle r(b), un créneau horaire et une paire de jours de cours. Bien que cet exercice doive être répété pour toutes les sections, les planificateurs doivent suivre des règles spécifiques. Dans notre cas, nous appliquons 1) que les paires de conférences soient offertes dans le même créneau horaire les deux jours, 2) un maximum de deux créneaux horaires consécutifs par jour pour une cohorte (idem pour les instructeurs), 3) quatre créneaux horaires consécutifs disponibles par jour, 4) aucune conférence ne doit être organisée pendant les créneaux horaires bloqués, et 5) qu'aucune section ne soit allouée à un instructeur dans une salle en même temps, c'est-à-dire qu'aucune fusion de sections n'est autorisée. Enfin, d'autres considérations courantes incluent la non-contradiction, la complétude et l'uniformité.

#### 4.1. Mappage du CCTSS sur le GTP

Tout d'abord, le mappage de haut niveau de ce CCTSS sur les constructions du GTP est effectué, comme le montre la Fig. 12. En conséquence, l'ensemble de tous les groupes de cohortes est simplement mappé aux entités du GTP ou G→E. De même, les instructeurs et les salles sont mappés sur deux types de ressources, c'est-à-dire I→Ri & R(b ∈ B)→ Rj i ≠ j. De même, les conférences de cours pour chaque section sont équivalentes aux activités GTP, c'est-à-dire S→A. Cependant, comme les activités du GTP sont groupées par entités respectives, une représentation plus précise de ce mappage est ∀s′(ca) ∈ gp y →aej ∈ Aej, c'est-à-dire que l'ensemble de toutes les sections de cours appartenant à un groupe gp y est mappé sur Aej. Le mappage des e-maillages et r-maillages est plus direct, c'est-à-dire que les emplois du temps des groupes de cohortes sont mappés sur les e-maillages. En revanche, les emplois du temps des instructeurs et des salles sont mappés sur les r-maillages. Pour les règles, les contraintes du problème CCTSS se transforment une par une en règles GTP. Pour expliquer, considérons d'abord deux variables de décision binaires utilisées dans le problème CCTSS :

\[ x_{s(ca)}^{r(b) d t} = \begin{cases} 
1 & \text{si la section } s(ca) \text{ est affectée à la salle } r(b) \text{ pour la paire de jours } d, \text{ et le créneau horaire } t \\
0 & \text{sinon}
\end{cases} \]

\[ z_{s(ca)}^{i(ca) d t} = \begin{cases} 
1 & \text{si la section } s(ca) \text{ est affectée à l'instructeur } i(ca), \text{ pour la paire de jours } d, \text{ et le créneau horaire } t \\
0 & \text{sinon}
\end{cases} \]

En utilisant ces deux variables et d'autres notations (annexe), nous résumons les règles CCTSS et leurs descriptions dans le Tableau 6.

Avec toutes les constructions CCTSS mappées sur le modèle GTP, l'algorithme EAR-CA de première étape peut être appliqué pour trouver des solutions initiales. Cependant, pour EAR-CA-SA visant l'optimalité, nous avons besoin d'une fonction objective bien définie. Nous utilisons donc la minimisation de la version généralisée des mesures de manque de compacité (ou dispersion des cours) de Siddiqui et al., c'est-à-dire \((\sum d U_d^{gp y})^n\) et \((\sum d V_d^i)^n\) pour les cohortes d'étudiants et les instructeurs, respectivement. Pour \(n = 1\), les mesures ci-dessus se réduisent à celles proposées par Siddiqui et al. (2018). Ici, les deux mesures représentent la somme des pénalités encourues en raison de la dispersion quotidienne des cours pour les cohortes d'étudiants et les instructeurs, respectivement. Dans ces mesures, les termes \(U_d^{gp y}\) et \(V_d^i\) représentent la pénalité du jour d, déterminée en trouvant la

 différence entre les créneaux horaires du premier et du dernier cours de la journée. Par exemple, si les cours d'un jour donné commencent au premier créneau et que le dernier cours est deux créneaux plus tard, la pénalité est simplement de 2 points. La minimisation de la pénalité de dispersion résulte simplement en une augmentation de la compacité de l'emploi du temps. Comme dans notre cas, où nous utilisons \(n = 2\), l'accent mis sur la compacité est plus élevé que dans Siddiqui et al. (2018). Cette valeur de \(n\) a été imposée par la direction du collège et l'unité de planification de l'école, car plusieurs étudiants et instructeurs viennent chaque jour d'autres villes. Par conséquent, le collège accorde une très haute priorité à l'amélioration de la compacité des emplois du temps pour eux. Dans le même contexte, une pénalité supplémentaire pour une seule classe par jour est également considérée, c'est-à-dire \((U_d^{gp y})^2\), qui vise à minimiser le scénario où les gens se déplacent uniquement pour un cours par jour. Cela fait de notre variante CCTSS un modèle de programmation linéaire mixte quadratique. La fonction objective devient donc :

\[ \text{Min} \sum d \sum gp y (U_d^{gp y})^2 + \sum d \sum gp y (U_d^{gp y})^2 + \sum d \sum \forall i∈I (V_d^i)^2 \]



### 5. Analyse numérique

Pour tester la performance du cadre EAR-CA, nous avons réalisé une étude numérique approfondie. L'objectif de cette investigation est double, à savoir examiner la performance de EAR-CA en termes de qualité des solutions et de son efficacité computationnelle en fonction des changements de 1) l'échelle du problème, 2) la disponibilité des ressources, et 3) les paramètres de l'algorithme. Pour l'analyse numérique, nous avons utilisé le jeu de données fourni par Siddiqui et Raza (2020), qui offre plusieurs cas. Les deux premiers sont les cas du semestre de printemps et d'automne (considérés comme des cas de base), récemment rencontrés par une grande école de commerce (section 4). Les deux cas sont analysés dans la section 5.2. Plusieurs scénarios à l'échelle variée avec différents paramètres de conception sont fournis dans le même jeu de données pour étendre l'analyse plus loin. Ces scénarios sont multipliés à plusieurs reprises sur ces deux cas de base. Finalement, nous avons résolu six scénarios de problèmes différents (étiquetés A-F dans le tableau 7), où A et B sont les cas de base, et C-F sont les cas élargis. Pour mettre les choses en perspective, le plus grand scénario F (2000 activités (sections de cours) | 566 instructeurs | 338 salles) est environ 10 et 14 fois la taille des deux cas de base, respectivement. Pour chacun de ces scénarios, différents paramètres de l'algorithme liés aux options de pivot, à la séquence de pivot et à la base de recherche de cellules, c'est-à-dire aléatoire ou séquencée, sont testés (tableau 7). Étant donné que l'espace de solution est très grand, et que les paramètres de recherche impliquent des options aléatoires, nous nous attendons à ce que l'algorithme mène potentiellement à une solution différente à chaque fois. Ainsi, la robustesse de l'algorithme, en ce sens, est essentielle à analyser. En conséquence, nous avons généré et comparé cinq répliques pour chaque paramètre de conception de problème unique (c'est-à-dire les scénarios A-F). Au total, nous avons résolu 120 problèmes, dont les résultats sont examinés dans la section 5.3. L'algorithme est programmé en Python 3.7.6 (Spyder 4.0.1), exécuté sur un ordinateur basé sur Windows 64 bits avec un processeur Intel i7-8565U à 8 cœurs et 1,80 GHz et 16 Go de RAM.

Bien que l'analyse ci-dessus se concentre sur l'effet de l'échelle du problème, nous avons également analysé l'impact de la disponibilité des ressources sur la performance. Il est facile de voir que la rareté des ressources (plus vs moins) affecterait directement la qualité de la solution et le temps de calcul. Nous nous concentrons donc sur cette question dans la section 5.4, où nous avons varié la disponibilité des ressources (instructeurs et salles) du scénario D entre un niveau élevé et relativement bas (détails dans le tableau 8). Pour les deux ressources, le niveau de disponibilité est représenté en termes de charge moyenne sur cette ressource (c'est-à-dire le nombre moyen d'heures de contact par semaine). Par exemple, pour r1 (ou instructeurs), une moyenne de 11,2 heures de contact par semaine représente une situation de charge élevée (ou rareté) dans D2 et D2a contre 7,2 heures de contact par semaine comme situation de faible charge dans D1 et D1a. Il en va de même pour la disponibilité des salles qui varie entre 15 et ~19 heures par semaine. Finalement, soixante problèmes supplémentaires ont été résolus et analysés dans la section 5.4.

Pour analyser la qualité de la solution, nous avons utilisé la fonction objective définie en (1). De plus, comme notre scénario CCTSS diffère de celui de Siddiqui et al. (2018), c'est-à-dire en termes de contraintes et de fonction objective, nous n'avons pas pu l'utiliser comme référence. Pour surmonter cet obstacle, nous avons d'abord déterminé les limites de tous les problèmes résolus. L'écart de la solution obtenue par rapport à la borne inférieure, en tant que différence en pourcentage (c'est-à-dire changement absolu par rapport à leur moyenne), est ensuite utilisé pour analyser la qualité des solutions. En conséquence, nous discutons d'abord de l'estimation des limites proposées dans la section 5.1 ci-dessous. Cela sera suivi de l'analyse de l'échelle des problèmes de base et de la disponibilité des ressources dans les sections 5.2 à 5.4. Enfin, nous avons analysé la performance de parallel-EAR-CA-SA dans la section 5.5.

#### 5.1. Estimation des bornes du problème

Les bornes inférieures et supérieures du GTP sont déterminées en fonction des meilleures et des pires structures individuelles d'e-mesh et de r-mesh possibles pour une entité ou une ressource. Pour estimer la borne inférieure en utilisant les e-mesh pour les entités, nous déterminons d'abord les meilleures et les pires valeurs de compacité de cohorte et les mesures de pénalité pour une seule classe par jour pour une cohorte gp y en utilisant (2) et (3), respectivement. Pour les problèmes auxquels nous avons été confrontés, c'est-à-dire chaque cohorte suivant 5 cours avec 2 classes par semaine (dix créneaux horaires occupés), les e-mesh les plus et les moins compacts (selon les règles imposées) sont montrés dans la Fig. 13, ce qui donne une valeur BUgp y (la plus compacte) et WUgp y (la moins compacte) de 49 et 81 points. Il est également simple de voir que la moindre pénalité pour une seule classe par jour BUgp y sera nulle, tandis que sa valeur maximale WUgp y est.


### Ruleset for the Business School CCTSS Timetabling Scenario

No. | Rules-set (\(u_e \cup u_{ri} \in U \forall e \in E, ri \in Ri\)) | Description
--- | --- | ---
1 | Period-types, defined in set \(D\), are enforced | Lectures are to be offered only on given day-pairs in \(D\), which are also in the same timeslots. (Example is shown in Table 5)
2 | Use alternative period-types if day(s) in the desired period type are unavailable | Example in Fig. 11 shows the use of alternative MTh in place of MW.
3 | \(|T| = T\) | Use only the given number of timeslots (in the Fig. 11 scenario, \(T = 4\) timeslots are allowed).
4 | \(\sum_{t \in T} \sum_{d \in D} \sum_{r(b) \in R} x_{s(ca) r(b) dt} = 1 \forall s(ca) \in S\) | Each section must strictly be assigned one room, a day-pair (or its alternative), and a timeslot.
5 | \(\sum_{s(ca) \in S} x_{s(ca) r(b) dq} \leq 1 \forall r(b) \in R, d, t\) | Each room, for a given day-pair (or its alternative) and timeslot, can at most house one section.
6 | \(\sum_{\forall s(ca) \in S} z_{s(ca) idq} \leq 1 \forall d \in D, t, i(ca) \in I_{Ci}\) | An instructor cannot have more than one lecture during a day-pair and timeslot combination.
7 | \(\sum_{t \in T} \sum_{d \in D} \sum_{i \in I(ca)} z_{s(ca) idq} = 1 \forall s(ca) \in S\) | Exactly one instructor available to teach the course must be assigned to a course section on a selected day-pair and timeslot combination.
8 | \(\sum_{t \in T} \sum_{d \in D} \sum_{\forall s(ca) \in S} z_{s(ca) idq} = f_{ca}^i\) (and \(m_{ca}^i\)) \(i(ca \in I_{Ci})\) | The total number of female (and male) sections assigned to an instructor must be equal to his/her assignment.
9 | \(\sum_{t \in T} \sum_{d \in D} \sum_{r(b) \in R} (x_{s(ca) r(b) dt} + z_{s(ca) i(ca) dt}) \forall s(ca) \in S, i(ca) \in I\) | Room and instructor assignment for day-pair and timeslot combination must match.
10 | \(\sum_{r(b) \in R} \sum_{s(ca) \in (gp_y)} x_{s(ca) r(b) dq} \leq 1 \forall gp_y \in Gp_y, t\) | No two lectures allowed on a given day at the same time for all sections belonging to a cohort group.
11 | \(\sum_d \sum_t h_{py} x_{s(ca) r(b) dt} \leq \text{CAP}_{r(b)} \forall r(b), s(ca \in Gp_y)\) | A course section belonging to a cohort group can only be assigned to a room with enough capacity (being the size of the cohort).
12 | \(\sum_{t' = 1}^{|T| - 2} \sum_{s(ca) \in (gp_y \in Gp_y)} X_{s(ca) r(b) dq} \leq 2 \forall t'\) | Maximum 2 lectures in a row on a specific day for a cohort group.



### Traduction en Français :

**5. Analyse numérique**

Pour tester les performances du cadre EAR-CA, nous avons réalisé une étude numérique approfondie. L'objectif de cette investigation est double : examiner les performances d'EAR-CA en termes de qualité des solutions et de son efficacité computationnelle avec les changements de 1) échelle du problème, 2) disponibilité des ressources, et 3) paramètres de réglage de l'algorithme. Pour l'analyse numérique, nous avons utilisé l'ensemble de données de problèmes fourni par Siddiqui et Raza (2020), qui propose plusieurs cas. Les deux premiers sont les cas du semestre de printemps et d'automne (considérés comme cas de base), récemment rencontrés par une grande école de commerce (section 4). Ces deux cas sont analysés en section 5.2. Plusieurs scénarios amplifiés avec différents paramètres de conception sont fournis dans le même ensemble de données pour étendre davantage l'analyse. Ces scénarios sont amplifiés de manière multiple par rapport à ces deux cas de base. Finalement, nous avons résolu six scénarios de problèmes différents (étiquetés A-F dans le tableau 7), où A et B sont les cas de base, et C-F sont les cas amplifiés. Pour mettre les choses en perspective, le plus grand scénario F (2000 activités (sections de cours) | 566 instructeurs | 338 salles) est environ 10 et 14 fois la taille des deux cas de base, respectivement. Pour chacun de ces scénarios, divers paramètres algorithmiques liés aux options de pivot, à la séquence de pivot et à la base de recherche des cellules, c'est-à-dire aléatoire ou séquencée, sont testés (Tableau 7). Étant donné que l'espace de solution est très grand et que les paramètres de recherche impliquent des options aléatoires, nous nous attendons à ce que l'algorithme puisse potentiellement conduire à une solution différente à chaque fois. Ainsi, la robustesse de l'algorithme, en ce sens, est essentielle à analyser. En conséquence, nous avons généré et comparé cinq répliques pour chaque paramètre de conception de problème unique (c'est-à-dire, scénarios A-F). Au total, nous avons résolu 120 problèmes, dont les résultats sont examinés en section 5.3. L'algorithme est programmé en Python 3.7.6 (Spyder 4.0.1), exécuté sur un ordinateur 64 bits basé sur Windows avec un processeur Intel i7-8565U 1,80 GHz à 8 cœurs et 16 Go de RAM.

Alors que l'analyse ci-dessus se concentre sur l'effet de l'échelle du problème, nous avons également analysé l'impact de la disponibilité des ressources sur les performances. Il est facile de voir que la rareté des ressources (plus vs moins) affecterait directement la qualité de la solution et le temps de calcul. Nous nous concentrons donc sur cette question en section 5.4, où nous avons varié la disponibilité des ressources (instructeurs et salles) du scénario D entre élevée et relativement basse (détails dans le tableau 8). Pour les deux ressources, le niveau de disponibilité est représenté en termes de charge moyenne sur cette ressource (c'est-à-dire, heures de contact moyennes/semaine). Par exemple, pour r1 (ou instructeurs), une moyenne de 11,2 heures de contact/semaine représente une situation très chargée (ou rare) dans D2 et D2a contre 7,2 heures de contact/semaine comme une situation de faible charge dans D1 et D1a. Il en va de même pour la disponibilité des salles qui varie entre 15 et ~19 h/semaine. Finalement, soixante problèmes supplémentaires ont été résolus et analysés en section 5.4.

Pour analyser la qualité des solutions, nous avons utilisé la fonction objective définie en (1). De plus, comme notre scénario CCTSS diffère de celui de Siddiqui et al. (2018), c'est-à-dire en termes de contraintes et de fonction objective, nous n'avons pas pu l'utiliser comme référence. Pour surmonter cet obstacle, nous avons d'abord déterminé les limites de tous les problèmes résolus. L'écart de la solution obtenue avec la limite inférieure comme différence en pourcentage (c'est-à-dire, changement absolu par rapport à leur moyenne) est ensuite utilisé pour analyser la qualité des solutions. En conséquence, nous discutons d'abord de l'estimation des limites proposées en section 5.1 ci-dessous. Cela sera suivi des analyses de l'échelle du problème de base et de la disponibilité des ressources dans les sections 5.2 à 5.4. Enfin, nous avons analysé les performances de parallel-EAR-CA-SA en section 5.5.

**5.1. Estimation des limites du problème**

Les limites inférieures et supérieures du GTP sont déterminées en fonction des meilleures et pires structures individuelles des e-mesh et r-mesh possibles pour une entité ou une ressource. Pour estimer la limite inférieure en utilisant les e-mesh pour les entités, nous déterminons d'abord la meilleure et la pire compacité des cohortes et les mesures de pénalité pour une seule classe par jour pour une cohorte \(g_p^y\) en utilisant (2) et (3), respectivement. Pour les problèmes auxquels nous avons été confrontés, c'est-à-dire chaque cohorte suivant 5 cours avec 2 classes par semaine (dix créneaux horaires occupés), les e-mesh les plus compacts et les moins compacts (selon les règles imposées) sont montrés dans la Fig. 13, ce qui donne des valeurs BUgp y (valeur la plus compacte) et WUgp y (valeur la moins compacte) de 49 et 81 points. Il est également simple de voir que la pénalité la moins importante pour une seule classe par jour BUgp y sera nulle, tandis que sa valeur la plus mauvaise WUgp y est simplement la valeur de pénalité appliquée par le collège (c'est-à-dire, 16 points). Cela se produit lorsque la compacité et la pénalité pour une seule classe par jour sont considérées séparément, bien qu'elles soient liées dans la réalité. Ainsi, lorsque les deux sont considérées ensemble (comme dans la Fig. 13), une valeur totale de 64 points est réalisée.

Par conséquent, les valeurs totales les meilleures et les pires pour tous les e-mesh sont égales aux points e-mesh individuels multipliés par le nombre de e-mesh dans le système, comme indiqué dans l'équation (4)(a) & (b).

\[ BU = BU_{gp^y} \times |gp^y| \] 
(a)

\[ WU = (WU_{gp^y} + WU_{gp^y}) \times |gp^y| \] 
(b)

Pour les r-mesh, nous ne nous intéressons qu'à la compacité des horaires des instructeurs. Les meilleures et les pires valeurs pour un instructeur BVd i et WVd i dépendent du nombre de cours assignés. Par exemple, pour un instructeur enseignant quatre cours, les meilleurs et pires scénarios sont présentés dans la Fig. 14, ce qui donne des points de compacité de 16 et 81, respectivement. En conséquence, les valeurs totales de compacité meilleures et pires pour tous les r-mesh, c'est-à-dire BV et WV, sont trouvées en additionnant les valeurs de compacité déterminées séparément pour tous les instructeurs. En conséquence, les limites inférieures et supérieures, c'est-à-dire LB et UB, pour l'ensemble du problème, sont estimées en utilisant (5).

\[ LB = BU + BV \] 
\[ UB = WU + WV \]

En utilisant cela, les estimations de LB et UB pour les six scénarios sont montrées dans le tableau 9.

**5.2. Analyse des cas de base**

Tout d'abord, nous examinons les cas de base A et B en utilisant le plan expérimental suggéré dans le tableau 7. Les paramètres pour les deux scénarios sont représentés à l'aide d'une étiquette à quatre lettres dans la Fig. 15 et les figures suivantes. La première lettre de l'étiquette représente le scénario lui-même, tandis que les trois dernières lettres identifient la base de pivot, la séquence de pivot et la méthode de recherche des cellules, respectivement. Par exemple, A-CRR représente le scénario A, Cohorte comme pivot, séquence de pivot aléatoire et méthode de recherche des cellules aléatoire. De même, B-ISR représente le scénario B, Instructeur comme pivot, séquence de pivot séquencée (par ordre décroissant de la charge des instructeurs) et méthode de recherche des cellules aléatoire.

Pour examiner la qualité des solutions des cas de base, les valeurs de la fonction objective (OBV) et leurs écarts avec la LB pour tous les paramètres sont montrés dans la Fig. 15. En considérant les effets des paramètres dans les deux scénarios, nous ne voyons pas de différence significative moyenne dans les valeurs OBV, bien que des variations soient visibles parmi les répliques pour un paramètre donné. Par exemple, A-CRR montre une plus grande variation dans les deux directions par rapport à A-ISR. La variation relativement plus faible avec le pivot séquencé des instructeurs (charge lourde à légère) est compréhensible car les options pour les instructeurs avec moins de cours s'épuis

ent beaucoup plus rapidement vers la fin de la recherche de solution. En comparant les scénarios A et B, nous voyons moins de variations dans B, ce qui est attendu car il est plus contraint en termes de ressources, c'est-à-dire que la charge moyenne des instructeurs est de 9,3 heures par semaine contre 7,3 heures par semaine pour A, idem pour les salles avec 16,4 et 12,4 heures par semaine. L'écart absolu dans toutes les solutions pour les deux scénarios est montré dans la Fig. 15 (à droite), qui montre un comportement robuste pour différents paramètres. En termes d'écart en pourcentage, les écarts moyens, minimum et maximum pour le scénario A sont respectivement de 18%, 4% et 33%, tandis que pour le scénario B, ils sont de 23%, 10% et 35%. Les performances en temps de ces problèmes sont montrées dans la Fig. 16, où les temps de solution les plus longs et les plus courts sont respectivement de 7,2 et 2,6 secondes. Nous observons une convergence plus rapide dans le scénario B lorsque des pivots séquencés sont utilisés, ce qui peut être attribué aux options de créneaux horaires moins nombreuses pour les enseignants à faible charge dans la dernière partie de la recherche. Cela peut également se traduire par moins de conflits pour ces instructeurs en raison de leur charge plus faible. Pour corroborer cela, nous avons également analysé le nombre de résolutions de conflits qui sont analysés en détail en section 5.3.

**5.3. Effets de la mise à l'échelle du problème**

Pour mesurer les performances d'EAR-CA pour des problèmes amplifiés, nous avons également testé les scénarios C-F avec les mêmes paramètres expérimentaux. Les résultats pour les valeurs OBV et les écarts LB sont montrés dans les Fig. 17 et Fig. 18, respectivement. Par souci de concision, nous avons seulement comparé les écarts LB et les temps de solution par rapport aux cas de base. Les figures révèlent clairement que changer les paramètres n'a pas beaucoup d'effet sur les valeurs moyennes OBV pour tous les scénarios. Cependant, lorsque nous analysons les écarts avec la LB, nous observons que les variations des écarts LB diminuent avec l'augmentation de la taille du problème. Nous notons d'abord que l'écart moyen pour toutes les répliques à travers les scénarios s'avère être de 21,4%. Pour les variations, le plus petit scénario de taille A varie autour de sa moyenne entre 4% et 32,7%, c'est-à-dire, la plus grande gamme de 28,7%. Relativement, la plus petite gamme de 5,7% est évidente dans le plus grand scénario F.

Pour analyser ce comportement global, nous avons trouvé certains e-mesh et r-mesh optimaux (comme dans la Fig. 13), tandis que d'autres mesh ont montré des degrés variés de déviations par rapport à la compacité idéale du mesh. Nous avons également remarqué qu'il y avait un nombre variable de mesh optimaux et déviés dans chaque réplique d'un scénario donné. Dans les problèmes de petite taille, l'effet d'une augmentation ou d'une diminution d'un mesh dévié est substantiel, ce qui tend à diminuer avec l'augmentation de la taille du problème à mesure que la valeur totale de la fonction objective augmente naturellement. Rappelons que le scénario F compte 400 e-mesh et 566 (instructeur) et 338 (salle) r-mesh, tandis que le scénario A compte 28 e-mesh et seulement 58 (instructeur) et 34 (salle) r-mesh.

En termes de paramètres EAR-CA avec pivots aléatoires et/ou recherche de cellules aléatoire, nous avons généralement pu trouver les meilleures solutions. Nous rapportons également que nous avons essayé de résoudre les cas où la recherche des pivots et des cellules était séquencée. Il s'est avéré que cette stratégie a conduit à de nombreux conflits, qui ont principalement résulté en l'absence de solutions. Ainsi, ce paramètre a été exclu comme une stratégie inappropriée. Finalement, nous avons évalué toutes les instances pour le nombre de conflits et leur effet sur les valeurs OBV et les temps de solution. En comparant les OBV (Fig. 19), bien qu'il soit clair que le nombre de conflits augmente avec la taille du problème, nous avons constaté que les conflits sur les valeurs OBV étaient minimes. Bien que la séquence des cohortes soit arbitraire, nous notons que le séquençage des instructeurs a eu un impact positif sur le nombre de conflits trouvés lors de la résolution, c'est-à-dire, en moyenne, moins de conflits à résoudre (Fig. 20). Encore une fois, cela peut s'expliquer par la nature de la séquence où les instructeurs à charge élevée sont affectés en premier, suivis par des instructeurs à charge de plus en plus faible. Donc, trouver une solution faisable pour ces instructeurs à faible charge devient évidemment beaucoup plus facile lors de la recherche. Le même effet sur les temps de solution est également évident.

La comparaison des temps de solution est présentée dans la Fig. 21, qui montre un schéma croissant avec la taille du problème. Notamment, cette variation entre les répliques augmente généralement dans les scénarios plus grands. Bien que ce schéma soit évident, nous commentons la complexité de solution de la méthode EAR-CA. C'est-à-dire que pour un nombre n d'éléments dans le pivot d'affectation (par exemple, N entités ou N ressources), intuitivement pour chaque activité correspondante, l'espace de recherche exhaustif potentiel est m × n (c'est-à-dire, la structure du mesh). En cas d'absence de conflit et de scénario d'affectation à cellule unique, les itérations de recherche totales sont simplement N × m × n – ce qui signifie que l'EAR-CA a une complexité de solution de l'ordre de O(N). En conséquence, nous avons comparé les temps de solution moyens pour tous les scénarios et ajusté une ligne. Le meilleur ajustement s'est avéré être simplement un polynôme de second degré, ce qui corrobore nos commentaires ci-dessus sur la complexité computationnelle de l'algorithme EAR-CA (Fig. 22).

**5.4. Effets de la variation de la disponibilité des ressources**

Dans cette section, nous rapportons notre analyse des effets de la disponibilité des ressources sur les performances d'EAR-CA. L'ensemble de données de problèmes et le plan expérimental respectif sont déjà fournis dans le tableau 8 ci-dessus. Les données sur la qualité de la solution en termes d'écart avec LB sont fournies dans la Fig. 23. Comme prévu, les cas avec moins de charge sur les instructeurs montrent des écarts plus faibles. Plus spécifiquement, pour les problèmes D1 et D1a, où la charge moyenne des instructeurs est la plus faible, c'est-à-dire, 7,3 heures de contact/semaine, l'écart le plus bas est de 12% (une moyenne de 15% et 14%, respectivement). En revanche, les ensembles DB & DBa, et D2 & D2a montrent un écart moyen de 22%, 24%, 23% et 23%, respectivement. La différence dans l'écart LB à travers différents scénarios est à peu près proportionnelle à la différence de charge moyenne des instructeurs (c'est-à-dire ~30%). Cependant, l'effet de la disponibilité des salles s'est avéré insignifiant. Cela est dû à des restrictions d'utilisation des salles faibles, sans impact direct sur la valeur de la fonction objective (aucun terme lié aux salles dans la fonction objective). Le même schéma est également évident dans les temps de solution. Les cas de faible charge de D1 et D1a ont constamment montré des temps de convergence bas avec une moyenne de 78 secondes. Pour les cas plus contraints (DB & DBa, et D2 & D2a), les temps de solution sont proportionnellement plus élevés, c'est-à-dire, 89,05 et 93,57 secondes (Fig. 24).

**5.5. Analyse des performances d'EAR-CA-SA**

Comme discuté en section 1, l'idée d'EAR-CA est de fournir à une méta-heuristique un point de départ robuste basé sur des solutions initiales de qualité. En conséquence, nous avons proposé l'algorithme de recuit simulé parallèle basé sur EAR-CA (parallel-EAR-CA-SA) en section 3.3. Cette section analyse les performances de parallel-EAR-CA-SA. Nous avons effectué l'optimisation sur cinq solutions aléatoires par souci de concision, chacune des scénarios B, C, D et F, qui couvrent l'ensemble du spectre des cas proposés dans le tableau 7. Les performances d'EAR-CA-SA sont mesurées en suivant l'amélioration de l'écart LB au fil des itérations effectuées. L'analyse a utilisé 0,99 comme valeur du paramètre de refroidissement δ. Cette valeur est sélectionnée sur la base de nos expérimentations initiales en utilisant les valeurs 0,9, 0,95, 0,97. La valeur de 0,99 est sélectionnée car les autres faisaient que l'algorithme se terminait trop rapidement. En ce qui concerne la portée de la recherche locale, c'est-à-dire, l'étendue du voisinage, nous

 l'avons considérée proportionnelle à la température.

Avant d'analyser les résultats, nous nous référons d'abord à notre discussion en section 5.3, où nous avons suggéré que lorsque la taille du problème augmente, l'effet d'un seul mesh dévié devient plus petit. De même, pour avoir une amélioration significative dans les scénarios plus grands, plusieurs mesh déviés doivent être améliorés. Ainsi, nous nous attendons à des améliorations plus rapides dans les problèmes plus petits par rapport aux plus grands. Avec cette observation en tête, nous présentons les résultats dans la Fig. 25. Globalement, nous voyons clairement des améliorations et des convergences dans tous les scénarios en seulement trois cents itérations. Cependant, comme nous l'avons observé plus tôt, nous voyons de plus grandes améliorations dans les scénarios plus petits B et C, où les deux ont atteint l'écart LB d'environ 5%. Pour les scénarios D et F, il est réduit à environ 14% et 17%, respectivement.

En termes de temps de solution, le temps moyen pris par parallel-EAR-CA-SA pour le scénario B est d'environ 1100 secondes (18,3 minutes); pour le scénario C, il est d'environ 9942 secondes (2,7 heures); pour le scénario D, il est d'environ 17046 secondes (4,7 heures); et pour le scénario F, il est d'environ 26375 secondes (7,3 heures). Pour comparer cela avec les performances computationnelles des travaux dans la littérature pour des problèmes de planification de cours similaires, nous avons vu des convergences se produisant généralement dans des itérations allant de quelques milliers à des centaines de milliers. Par exemple, Lü et Hao (2010) ont rapporté des itérations avec des critères d'arrêt stricts atteignant des multiples de milliers, qui autrement atteignaient jusqu'à 800 000 itérations. Les problèmes considérés par eux impliquaient 30 à 130 cours et quelques centaines de contraintes (ce qui est comparable à notre plus petit scénario A impliquant 141 cours). En testant le même ensemble de problèmes avec plusieurs algorithmes, Soria-Alcaraz et al. (2014) ont rapporté la convergence en environ 500 000 itérations. En revanche, Soria-Alcaraz et al. (2016) ont montré une amélioration significative pour le même ensemble de problèmes, avec des itérations tombant à deux à trois mille. Comparativement, Thepphakorn et Pongcharoen (2020) testant des problèmes plus importants (jusqu'à 323 cours, 13 périodes/jour et 142 salles) ont démontré leur convergence dans un maximum de 900 itérations. Avec cette scène de performance existante, il est évident que notre approche novatrice basée sur des solutions de qualité montre une convergence dans une fraction des coûts computationnels ci-dessus malgré le recours à une méta-heuristique simple.



### 6. Conclusions

Nous avons développé un cadre de modélisation ontologique généralisée et une solution pour résoudre une vaste gamme de problèmes de planification des emplois du temps dans ce travail. La nécessité d'une telle généralité est profondément ancrée dans la littérature, qui souligne la fragmentation dans la gestion des problèmes de planification des emplois du temps à travers divers domaines. Cela a également conduit à un ad-hocisme limitant le développement d'approches de solution robustes de haut niveau, qui ne parviennent pas à exploiter la structure commune sous-jacente des problèmes. Ce manque de focalisation sur la structure du problème est attribué à la dépendance excessive aux métaheuristiques, qui ont initialement montré une promesse face aux lacunes computationnelles des méthodes analytiques. Cependant, les inefficacités computationnelles des métaheuristiques sont également apparues à mesure que la taille des problèmes augmentait. Malgré cela, la tendance continue de croître monotonement, ce qui a soulevé la critique susmentionnée (Lee et al., 2005; Sørensen, 2015).

Pour faire face à ces problèmes, nous contribuons d'abord en consolidant les approches de modélisation des problèmes dans divers domaines en utilisant une nouvelle ontologie de modélisation de planification des emplois du temps unifiée, c'est-à-dire GTP. Deuxièmement, nous proposons une approche de solution en deux étapes unique, qui exploite la structure commune sous-jacente des problèmes pour déterminer des solutions élites qui fournissent un point de départ robuste pour les métaheuristiques afin de trouver une solution optimale ou quasi-optimale à un coût computationnel très faible. Pour démontrer l'efficacité de la méthodologie, nous avons utilisé un problème très récent de planification des cours. Les résultats indiquent que même pour des problèmes de grande taille, les solutions initiales générées étaient presque toujours raisonnablement proches de leur borne inférieure. Pour améliorer ces solutions en utilisant une métaheuristique intégrée, nous avons employé l'algorithme parallèle-EAR-CA-SA et montré comment, avec seulement quelques centaines d'itérations, les solutions sont améliorées à un point où l'écart avec la borne inférieure descend jusqu'à 3,5 %.

La principale limitation de notre travail est qu'il n'est testé que sur un domaine majeur de problème de planification des emplois du temps, à savoir la planification des cours. De plus, son intégration est testée avec une métaheuristique simple basée sur un seul élément, tandis que pour généraliser la performance de l'intégration EAR-CA, des tests avec d'autres méthodes basées sur un seul élément et des méthodes basées sur la population sont nécessaires. Cela met également en évidence les défis significatifs rencontrés dans la direction proposée par notre travail, à savoir consolider les efforts de modélisation d'un domaine très ancien mais hautement fragmenté. Ainsi, montrer de manière concluante son efficacité interdomaines est difficile. De plus, comme il existe une vaste suite de métaheuristiques, le même défi est rencontré pour démontrer la facilité d'intégration d'EAR-CA avec toutes ces techniques. Par conséquent, en termes de développement futur, nous visons à 1) tester l'efficacité de l'approche de solution proposée pour les scénarios multi-objectifs CCTSS, ce qui implique des solutions Pareto optimales non dominées (par exemple, lorsque des objectifs supplémentaires tels que la maximisation de l'utilisation des ressources et la minimisation des coûts d'exploitation sont pris en compte); 2) personnaliser et tester l'approche dans des domaines autres que la planification des cours afin que son efficacité générale puisse être analysée; 3) développer davantage la méthode pour son intégration avec d'autres métaheuristiques et hyper-heuristiques basées sur un seul élément et sur la population; et 4) concevoir des stratégies de résolution de conflits plus intelligentes et efficaces pour améliorer encore les performances de la méthode EAR-CA.


modèle example : 


### Tableau 6 : Règles pour le scénario de planification des cours de l'école de commerce CCTSS

| N° | Ensemble de règles (ue ∪ uri ∈ U ∀e ∈ E, ri ∈ Ri ) | Description |
|----|----------------------------------------------------|-------------|
| 1  | Types de périodes, définis dans l'ensemble D, sont appliqués | Les cours doivent être offerts uniquement par paires de jours définies dans D, qui sont également aux mêmes créneaux horaires. (Exemple montré dans le Tableau 5) |
| 2  | Utiliser des types de périodes alternatifs, si le(s) jour(s) de la période souhaitée sont indisponibles | L'exemple de la Fig. 11 montre l'utilisation de MTh alternatifs à la place de MW |
| 3  | \|T\| = T | Utiliser uniquement le nombre donné de créneaux horaires (dans le scénario de la Fig. 11, T = 4 créneaux horaires sont autorisés) |
| 4  | \[\sum_{t \in T} \sum_{d \in D} \sum_{r(b) \in R} x_{s(ca)}^{r(b)} d t = 1\] ∀s(ca) ∈ S | Chaque section doit être strictement assignée à une salle, une paire de jours (ou son alternative), et un créneau horaire |
| 5  | \[\sum_{s(ca) \in S} x_{s(ca)}^{r(b)} d q ⩽1\] ∀r(b) ∈ R, d, t | Chaque salle, pour une paire de jours (ou son alternative) et créneau horaire donné, peut au maximum accueillir une section |
| 6  | \[\sum_{∀s(ca) \in S} z_{s(ca)}^{i} d q ⩽1\] ∀d ∈ D, t, i(ca) ∈ ICi | Un instructeur ne peut avoir qu'un seul cours pendant une paire de jours et une combinaison de créneaux horaires |
| 7  | \[\sum_{t \in T} \sum_{d \in D} \sum_{i \in I(ca)} z_{s(ca)}^{i} d q = 1\] ∀s(ca) ∈ S | Un instructeur disponible pour enseigner le cours doit être assigné à une section de cours sur une paire de jours sélectionnée et combinaison de créneaux horaires |
| 8  | \[\sum_{t \in T} \sum_{d \in D} \sum_{∀s(ca) \in S} z_{s(ca)}^{i} d q = f_{ca}^{i} (et m_{ca}^{i})\] i(ca) ∈ ICi | Le nombre total de sections féminines (et masculines) assignées à un instructeur doit être égal à son assignation |
| 9  | \[\sum_{t \in T} \sum_{d \in D} \sum_{r(b) \in R} (x_{s(ca)}^{r(b)} d t + z_{s(ca)}^{i(ca)} d t)\] ∀s(ca) ∈ S, i(ca) ∈ I | L'affectation de la salle et de l'instructeur pour la combinaison de la paire de jours et du créneau horaire doit correspondre |
| 10 | \[\sum_{r(b) \in R} \sum_{s(ca) \in (g_p^y)} x_{s(ca)}^{r(b)} d q ⩽1\] ∀g_p^y ∈ Gp^y, t | Aucune des deux conférences autorisées le même jour au même moment pour toutes les sections appartenant à un groupe de cohortes |
| 11 | \[\sum_{d} \sum_{t} h_p^y x_{s(ca)}^{r(b)} d t ⩽ CAP_{r(b)}\] ∀r(b), s(ca) ∈ Gp^y | Une section de cours appartenant à un groupe de cohortes ne peut être assignée qu'à une salle ayant une capacité suffisante (étant la taille de la cohorte) |
| 12 | \[\sum_{t' + 2}^{t = t'} \sum_{s(ca) \in (g_p^y \in Gp^y)} X_{s(ca)}^{r(b)}, d, q ⩽ 2\] ∀t' = 1, 2, ..., \|T\| - 2 | Maximum 2 conférences à la suite un jour spécifique pour un groupe de cohortes |

