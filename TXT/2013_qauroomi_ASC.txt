### Introduction

L'essence de l'ordonnancement réside dans la planification d'un ensemble d'événements, où cette planification consiste généralement à allouer des salles et des créneaux horaires aux événements, sous réserve de certaines contraintes spécifiées. Différents ensembles de contraintes constituent l'ensemble des sous-classes d'ordonnancement conçues pour répondre à diverses applications théoriques et pratiques, allant de l'ordonnancement des terminaux à la planification des horaires des infirmières. Un aperçu général de ces différentes sous-classes est disponible dans [1].

Une de ces sous-classes, l'ordonnancement éducatif, traite des problèmes de planification dans les écoles et les universités, avec des contraintes concernant les enseignants, les heures de cours, les installations, les conflits de programmes, les capacités des salles, etc. L'ordonnancement éducatif se divise lui-même en trois grandes variantes : le problème d'ordonnancement des cours universitaires (UCTP), le problème d'ordonnancement des examens (ETP) et le problème d'ordonnancement des lycées (HTP). Ces variantes se différencient par leurs contraintes. Par exemple, alors que dans l'ETP, les événements peuvent avoir lieu dans la même salle et le même créneau horaire tant qu'ils satisfont toutes les autres contraintes, dans l'UCTP, un seul événement peut occuper une certaine salle à un créneau horaire spécifié. De plus, alors que la durée de la période d'ordonnancement de l'UCTP (et donc le nombre de créneaux horaires) est constante, car un certain nombre de créneaux horaires ne peut pas dépasser une semaine, l'ETP peut bénéficier d'une certaine flexibilité à cet égard. Dans cet article, nous nous concentrons sur l'UCTP, mais en raison de la similitude de la structure du problème et des ensembles de contraintes, les autres variantes peuvent être sujettes à certaines des questions discutées ici avec peu de perte de généralité.

Les contraintes de l'UCTP sont divisées en deux grandes catégories : les contraintes dures et les contraintes souples, représentant respectivement des questions de faisabilité et de préférence. Toute violation d'une contrainte dure entraîne un emploi du temps irréalisable. La violation la plus fréquente est lorsque deux événements partageant des participants sont programmés en même temps, ce qui entraîne un conflit d'événements. Clairement, un emploi du temps contenant de tels conflits est inacceptable. Les contraintes souples, en revanche, sont davantage des considérations qui, lorsqu'elles sont prises en compte, peuvent produire des emplois du temps plus accommodants pour l'institution, les participants ou les deux. Ainsi, nous pourrions formuler une contrainte souple typique pour décourager la programmation de plus de deux cours consécutifs pour un étudiant.

La NP-complétude du problème d'ordonnancement a été explicitement établie par Even et al. dans [2], rendant la tâche de résoudre le problème particulièrement attrayante et stimulante à parts égales pour une multitude d'approches algorithmiques. Des enquêtes générales sur la performance des algorithmes peuvent être trouvées dans [3-5]. Dans cet article, nous nous concentrons sur les problèmes d'ordonnancement des cours dont les solutions se conforment à un cadre évolutif général.

Les métaheuristiques ont été largement utilisées pour résoudre les problèmes d'ordonnancement. Dans le cadre de l'intelligence en essaim, des algorithmes d'optimisation par colonie de fourmis ont été proposés, comme ceux de [6,7], qui utilisent des fourmis pour construire l'affectation complète des événements aux créneaux horaires en utilisant des heuristiques et des informations sur les phéromones. Les emplois du temps sont ensuite améliorés à l'aide d'une procédure de recherche locale, et la matrice de phéromones est mise à jour en conséquence pour l'itération suivante. En particulier, Socha et al. comparent et analysent deux systèmes de fourmis différents dans [6], puis comparent le MMAS (le système de fourmis le plus performant) avec d'autres algorithmes, concluant que dans les grandes instances, le MMAS surpasse les concurrents. Parmi les approches les plus novatrices, l'optimisation par accouplement des abeilles a également été appliquée à la fois à l'ordonnancement des examens et des cours dans [8]. Les auteurs testent leur algorithme contre les ensembles de référence de Carter et Socha et obtiennent des résultats "compétitifs, sinon meilleurs". L'heuristique populaire de recuit simulé (SA) a été mise en œuvre dans [9-11] entre autres. En particulier, [10] utilise un algorithme SA en deux phases avec quelques heuristiques et une nouvelle structure de voisinage qui permute entre des paires de créneaux horaires, au lieu de deux affectations. Les auteurs rapportent que, lorsqu'ils sont testés sur deux ensembles de référence standard, la nouvelle heuristique de voisinage améliore les performances du SA. La recherche tabou est également appliquée dans [12,13]. Dans [13], une recherche tabou adaptative est utilisée pour intégrer une structure de voisinage originale à double chaînes de Kempe, un opérateur de perturbation guidé par des pénalités et un mécanisme de recherche adaptatif, pour obtenir des résultats plus efficaces.

Plus spécifiquement, les algorithmes évolutionnaires (EAs) ont été appliqués aux problèmes d'ordonnancement avec des degrés de succès variables dans les travaux de [14-19] entre autres. Par exemple, [14] est parmi les premiers à remédier aux faibles performances d'un algorithme génétique, comparé à d'autres méthodes conventionnelles, en utilisant un codage par regroupement. En général, cependant, les EAs emploient d'autres techniques pour compenser leur échec potentiel à traiter les optima locaux. Par exemple, [17] tire parti des heuristiques spécifiques au domaine dans une structure évolutive par ailleurs pour améliorer les performances. Travaillant sur des problèmes réels, plutôt qu'artificiels, Beligiannis et al. utilisent un algorithme évolutif pour résoudre des problèmes d'ordonnancement dans les lycées grecs dans [18].

En tant que sous-classe des EAs, les algorithmes mémétiques ont été utilisés pour résoudre l'ordonnancement dans [19-24] entre autres. Comme l'une des premières applications, dans [20], Burke et al. utilisent l'un des deux opérateurs de mutation, à savoir "léger" et "lourd", pour reprogrammer une sélection aléatoire d'événements ou perturber des créneaux horaires entiers d'événements, respectivement. Le processus évolutif incorpore également une montée de colline. Alkan et Ozcan ont utilisé un certain nombre d'opérations de croisement en un point et uniformes, une fonction de fitness pondérée, et encore une procédure de montée de colline pour réduire les violations dans [21]. Une application récente et réussie d'un algorithme mémétique à l'ordonnancement de laboratoires est [23], où les préférences des étudiants sont également prises en compte.

Bien que des ensembles de référence standard pour l'ordonnancement existent [25-27], il a été raisonnablement argumenté dans [5] que "il y a une confusion dans le domaine" à cet égard, au point que réaliser des comparaisons significatives ou une reproductibilité pourrait être compromis. En partie pour surmonter de tels obstacles, deux compétitions internationales d'ordonnancement ont été organisées récemment. La dernière consistait en des ensembles de problèmes d'ordonnancement des examens et des cours. McCollum et al. fournissent un aperçu de cet événement dans [28]. Des informations sur les algorithmes gagnants peuvent également être trouvées dans [29].

En général, cependant, les packages de référence standard sont généralement conçus en tenant compte des contraintes dures et souples, négliger les contraintes souples pour se concentrer sur la faisabilité n'est guère un défi. En fait, beaucoup se concentrent principalement sur les contraintes souples, au point que des solutions faisables sont trouvées si rapidement que toute comparaison et analyse devient insignifiante. Dans une telle situation, les instances de référence fournies dans [30] abordent la question de savoir comment différents algorithmes se comporteraient lorsque l'accent est mis sur la faisabilité. Cela est réalisé grâce à un choix minutieux d'un sous-ensemble délibéré d'un ensemble plus large d'instances de problèmes qui se sont révélées particulièrement difficiles, mais qui ont au moins une solution faisable. La première utilisation substantielle de ce package était en relation avec le travail de [19], qui a mis en œuvre deux algorithmes. L'un de ces algorithmes utilisait principalement des opérateurs génétiques de regroupement, tandis que l'autre mettait en œuvre une recherche locale. Les deux algorithmes ont été augmentés par une recherche locale. Les résultats moyens de ces deux algorithmes ont été améliorés par la suite dans le cadre d'un algorithme de recuit simulé [31]. Dans cet article, nous nous limiterons à ces "instances difficiles", ainsi qu'à une discussion sur la performance des trois algorithmes qui les ont utilisés.

L'article est organisé de la manière suivante : une description détaillée du problème est donnée dans la section suivante, suivie de l'aperçu de notre algorithme mémétique dans la section 3, où nous passons en revue

 les détails sur la structure générale de l'algorithme et la conception de l'opérateur. La section 4 inclut une discussion plus ciblée sur la fonction de fitness. Dans la section 5, les paramètres expérimentaux sont fournis et l'algorithme proposé est mis à l'épreuve. Des comparaisons avec trois autres algorithmes suivent, démontrant la performance supérieure de notre algorithme mémétique dans deux des trois ensembles d'instances. L'accent est ensuite mis sur les algorithmes évolutionnaires, avec une analyse de la conception de l'opérateur et le développement de nouvelles mesures et tests supplémentaires. Nous concluons en tirant des conclusions et en suggérant des pistes possibles pour les recherches futures dans la section 8.

### Traduction

Énoncé formellement, le problème d'ordonnancement particulier que nous étudions ici, initialement formulé pour la première compétition d'ordonnancement [32], comporte quatre paramètres : T, un ensemble fini de créneaux horaires ; R, un ensemble fini de salles ; E, un ensemble fini d'événements ; et C, un ensemble fini de contraintes. Il s'agit d'attribuer des créneaux horaires et des salles aux événements de manière à satisfaire les contraintes requises. Ces contraintes sont divisées en deux catégories : les contraintes dures et les contraintes souples. De manière générale, les contraintes dures représentent des exigences obligatoires qui, lorsqu'elles sont satisfaites, produisent des emplois du temps "fonctionnels". Les contraintes souples, quant à elles, tiennent compte des préférences des enseignants, des étudiants, etc. Bien qu'une solution optimale prenne en compte à la fois les contraintes dures et souples, notre algorithme ignore les contraintes souples et se concentre sur les contraintes dures pour des raisons qui seront expliquées dans la section 5.

En termes mathématiques, une fonction à valeurs binaires \( h : S \rightarrow \{0, 1\} \) peut être associée à chaque contrainte dure. Pour chaque solution \( s \in S \), la fonction est définie par :
\[ h(s) = 
\begin{cases} 
1 & \text{si } s \ne satisfait pas la contrainte \\
0 & \text{sinon}
\end{cases}
\]

Soit \( S \) l'ensemble de toutes les solutions à un problème d'ordonnancement donné. Une solution faisable est toute solution \( s \in S \) qui satisfait toutes les contraintes dures. Ainsi, la fonction objective peut être formulée comme suit :
\[ F(S) = \sum_{i=1}^{n} h_i(S) \]
où les contraintes dures sont données par \( h_1, h_2, \ldots, h_n \). Notre formulation particulière inclut les contraintes dures suivantes :
- **H1** : Aucun étudiant n'est autorisé à assister à plus d'un événement à un moment donné.
- **H2** : Un seul événement est prévu pour chaque salle et chaque créneau horaire.
- **H3** : Toutes les caractéristiques requises par l'événement doivent être satisfaites par la salle, qui doit avoir une capacité adéquate.

En plus de la description générale du problème, il peut être utile de discuter d'une perspective de résolution de problèmes qui influencera notre construction de solutions plus tard, à savoir l'idée d'interpréter l'ordonnancement comme un problème de regroupement. Dans [33], Falkenauer définit les problèmes de regroupement comme ceux où la tâche consiste à partitionner un ensemble d'objets \( U \) en une collection de sous-ensembles mutuellement disjoints \( u_i \) de \( U \), où
\[ \bigcup u_i = U \text{ et } u_i \cap u_j = \emptyset, \text{ pour } i \ne j \]
selon un ensemble de contraintes spécifiques au problème qui définissent les groupements autorisés. Des cas familiers où ce critère s'applique incluent l'emballage de bacs, la coloration de graphes et l'ordonnancement. Un algorithme génétique de regroupement correspondant (GGA), conçu par Falkenauer, est basé sur l'idée que, bien que les opérateurs génétiques traditionnels conviennent bien à un problème typique non basé sur le regroupement et que nous puissions juger de la qualité d'une affectation indépendamment des autres affectations, ces mêmes opérateurs échouent dans le cas des problèmes de regroupement, et la qualité de toute affectation dans un tel problème dépend des autres affectations.

À la lumière de ce problème, Falkenauer soutient que lorsqu'il s'agit de problèmes de regroupement, les opérateurs génétiques traditionnels, orientés objet, tels que la recombinaison et la mutation, tendent à fragmenter, plutôt qu'à améliorer, les blocs de construction produits lors des tours précédents de l'algorithme. Par conséquent, ce qui distingue les opérateurs basés sur des groupes de leurs homologues traditionnels basés sur des objets est l'accent mis sur les groupes par opposition aux individus. De plus, les opérateurs basés sur des groupes sont plus efficaces puisque, en se concentrant sur les objets (par opposition aux groupes), l'espace de recherche d'un problème de regroupement devient beaucoup plus grand qu'il ne devrait l'être, en raison d'un encodage qui fait que l'algorithme consacre du temps à des solutions qu'il a déjà examinées, mais qu'il ne parvient pas à reconnaître.

Par exemple, dans le cas de notre problème d'ordonnancement, les événements constituent les "objets" de la formulation de regroupement et déterminer si un étudiant est programmé pour assister à deux événements en même temps (indiqué par la contrainte H1 ci-dessus) nécessite de vérifier les autres affectations du même créneau horaire. Ainsi, un opérateur de recombinaison qui se concentre sur des individus, plutôt que sur des groupements d'événements, passe à côté de l'essentiel. Lewis et Paechter appliquent ces idées au cas de l'ordonnancement dans [19] et, afin d'examiner les mérites des opérateurs basés sur des groupes, s'éloignent des opérateurs génétiques orientés objet et se concentrent sur les créneaux horaires comme bloc de construction approprié du problème. Ainsi, des créneaux horaires entiers sont soumis à des opérateurs génétiques tels que la recombinaison et la mutation. Nous examinerons les effets de ces décisions sur la qualité de la solution plus tard. Ce qui est important de noter pour l'instant, c'est que notre algorithme utilise une formulation de regroupement et nous aborderons cela dans la section suivante.



### 3. Schéma de l'algorithme

Nous construisons les emplois du temps sous forme de matrice bidimensionnelle où les lignes représentent les salles et les colonnes représentent les créneaux horaires. Ainsi, l'intersection de la salle \(i\) et du créneau \(j\) dans la matrice \(R\) correspond soit à un certain événement spécifié par son numéro, disons \(n\) (\(r_{ij} = n\)), soit à une vacance. Il est important de noter que, hormis le maintien d'une liste des événements non programmés pour chaque emploi du temps, cette approche n'autorise aucune relaxation des contraintes du problème. Par conséquent, un événement est autorisé à une certaine place dans l'emploi du temps uniquement s'il ne viole aucune des contraintes dures définies dans la Section 2. De plus, nous n'autorisons aucune relaxation concernant le nombre de créneaux horaires, ce qui signifie que dès le départ, le nombre de créneaux horaires attribués est fixé au nombre total de créneaux utilisables (qui est de 45 dans ce cas). Cela contraste avec certains autres algorithmes d'ordonnancement, notamment dans [19], qui relâchent d'abord la contrainte sur le nombre de créneaux horaires et éliminent plus tard les créneaux supplémentaires.

Notre algorithme appartient à la famille des algorithmes mémétiques (MAs). L'idée de combiner la recherche locale avec des algorithmes génétiques, diversement caractérisés comme des algorithmes mémétiques, lamarckiens, hybrides ou culturels [34], améliore le processus de transition simple qui se produit dans un algorithme génétique en incluant des connaissances spécifiques au problème telles que des heuristiques, des techniques de recherche locale, etc. L'idée est de guider la recherche locale par la capacité des algorithmes génétiques à explorer des recoins obscurs de l'espace de conception, et la force d'une telle recherche est mieux exploitée lorsque les capacités exploratoires d'un algorithme génétique sont complétées par la puissance exploitative de la recherche locale dans un compromis réussi.

Dans ce cadre, la structure de l'algorithme est décrite dans la Figure 1. La première phase, dénotée par la méthode Initialize, génère la population initiale. Notre objectif principal ici est de créer de la diversité. À cette fin, la procédure mélange d'abord les listes de créneaux et d'événements pour chaque emploi du temps en utilisant l'algorithme de mélange de Fisher-Yates [35]. Après avoir choisi un élément de chaque liste, l'algorithme vérifie la conformité entre les caractéristiques du créneau choisi et les exigences de l'événement candidat, et une affectation est effectuée en cas de correspondance. Ainsi, notre procédure d'initialisation est de nature gourmande, similaire à celle utilisée dans [36]. À la fin de la méthode Initialize, plusieurs emplois du temps plutôt clairsemés sont créés, chacun accompagné d'une liste d'événements non programmés.

Une fois l'initialisation terminée, la population passe par les trois étapes de recombinaison, mutation et recherche locale de manière répétée. En commençant par la recombinaison, il y a quatre étapes clés dans le processus : Sélection, Injection, Suppression des doublons et Réinsertion.

1. **Sélection** : Les limites de début et de fin sont choisies aléatoirement à partir de l'intersection des salles et des créneaux horaires pour déterminer une plage.
2. **Injection** : La plage de créneaux définie à la première étape est remplacée chez un parent en utilisant la sélection correspondante de l'autre.
3. **Suppression des doublons** : Tout doublon possible qui pourrait être apparu est supprimé du parent hôte.
4. **Réinsertion** : Les événements non programmés sont choisis au hasard et réinsérés dans l'emploi du temps. Dans le cas où un événement peut être programmé dans plusieurs créneaux candidats, l'un d'eux est choisi au hasard.

Enfin, pour la construction du deuxième descendant, les parents inversent leurs rôles. La Figure 2 illustre ce processus avec un exemple.

Ensuite, vient l'opérateur de mutation, où un nombre spécifié de créneaux horaires sont sélectionnés au hasard et vidés entièrement dans la liste des événements non programmés. Cette liste est ensuite mélangée et parcourue, et à chaque étape, pour l'événement non programmé de choix, une recherche d'un créneau faisable est effectuée. Si plus d'un créneau est disponible, le choix se fait aléatoirement.

À ce stade, la partie des opérateurs génétiques est terminée et l'emploi du temps est soumis à une procédure de recherche locale, mise en œuvre par deux opérateurs. Le premier tente simplement de changer le regroupement des événements en recherchant des créneaux aléatoires alternatifs pour un événement déjà programmé. Les créneaux alternatifs possibles constituent des ensembles indépendants de nœuds dans le graphe des conflits correspondant. Cependant, étant donné que trouver des regroupements sans conflit maximaux (ensembles indépendants de nœuds) est un problème NP-Complet en soi, le processus est limité à l'échange d'événements entre les regroupements réalisés dans l'emploi du temps actuel.

Lorsque l'emploi du temps est transmis au deuxième opérateur de recherche locale, l'algorithme a fait tout ce qu'il pouvait pour améliorer le regroupement des événements. Par conséquent, aucun créneau faisable et inoccupé n'existe pour les événements sur la liste des non programmés. Cette situation capture un optimum local courant, car une telle structure relationnelle pour l'emploi du temps ne peut pas donner de solution et l'emploi du temps doit être modifié s'il veut s'améliorer. Cela signifie que, pour échapper aux optima locaux, il doit descendre du sommet local et tolérer quelques "mauvais" mouvements, du moins temporairement. Étant donné que l'opérateur utilise une définition de voisinage de grande taille, il pourrait être considéré comme une recherche de voisinage à très grande échelle [37]. Comme illustré par un exemple dans la Figure 3, la deuxième procédure de recherche locale fonctionne en trois étapes : Sélection de créneau, Déplacement et Réinsertion.

1. **Sélection de créneau** : La liste des événements non programmés est mélangée et l'emploi du temps est recherché pour trouver des créneaux possibles pour les programmer. Comme noté précédemment, aucun créneau faisable n'est disponible dans l'emploi du temps à ce stade, la recherche se concentre donc sur les créneaux qui sont soit faisables mais occupés, soit en conflit avec d'autres événements programmés dans le même créneau horaire.
2. **Déplacement** : Pour l'événement choisi, le "meilleur" emplacement potentiel est sélectionné. "Meilleur" désigne ici une heuristique qui privilégie un créneau ayant le moins d'étudiants en conflit avec lui (note : étudiants, pas événements). C'est ce qu'on appelle couramment l'heuristique de la Valeur la Moins Contraignante (LCV). Avant d'insérer un événement non programmé, le créneau désigné est vidé ainsi que les créneaux contenant des événements en conflit avec l'événement de remplacement.
3. **Réinsertion** : Après que les étapes ci-dessus ont été effectuées pour tous les événements initialement non programmés, une nouvelle liste d'événements non programmés émerge. Ceux-ci peuvent être programmés de manière faisable dans d'autres créneaux vacants de l'emploi du temps. Ainsi, en tant qu'étape finale, une recherche exhaustive est effectuée pour insérer autant de ces événements que possible.

Cela conclut la description de l'algorithme. Dans la section suivante, la fonction de fitness et ses paramètres sont discutés.


### 4. Analyse de la fonction de fitness

Un choix naturel pour évaluer les emplois du temps pourrait être de compter le nombre d'événements non programmés. Cependant, comme le montre un exemple dans la Figure 4, une telle fonction de fitness est trop simpliste car elle ne parvient pas à différencier entre des solutions similaires en apparence qui ont néanmoins des structures relationnelles différentes. Par conséquent, en plus de compter les événements non programmés, il est souhaitable que la fonction de fitness distingue les emplois du temps d'autres manières, afin de maintenir une pression de sélection sur de plus longues durées. De plus, pour les problèmes fortement contraints (comme c'est le cas pour de nombreuses instances d'ordonnancement), il serait souhaitable que l'algorithme puisse apprendre de son expérience afin d'évaluer les solutions de manière plus dynamique. En somme, les critères suivants doivent être satisfaits :

1. Prendre en compte la structure de la solution ;
2. Apprendre de l'expérience ;
3. Garder les coûts de calcul aussi bas que possible.

Le compromis optimal entre les deux premiers critères ajoute suffisamment de détails pour distinguer les états relationnellement dissemblables tout en maintenant le coût de calcul abordable. Pour répondre au premier critère, tout décalage possible entre les installations et la capacité de chaque salle, d'une part, et les exigences de l'événement programmé, d'autre part, peut être pris en compte. Pour répondre au deuxième critère, nous avons introduit un nouveau paramètre basé sur l'intuition que les événements qui se retrouvent plus souvent sur la liste des non programmés sont plus difficiles à programmer et devraient se voir accorder une priorité plus élevée. Pour mettre cela en œuvre, pour chaque événement, un schéma d'adaptation de poids [38] a été implémenté via un indice qui est d'abord initialisé au nombre initial d'étudiants en conflit. Pour chaque événement, ce nombre est égal au nombre d'étudiants qui participent à un autre événement, ce qui, s'il est programmé dans le même créneau horaire que l'événement actuel, entraînera un conflit. L'indice est également incrémenté chaque fois que l'événement est déplacé par le deuxième opérateur de recherche locale. Étant donné que ce paramètre est ensuite partagé parmi tous les individus de la population, il peut servir d'estimation de la difficulté générale à programmer un événement spécifique. Cela aide progressivement l'algorithme à établir des priorités, car lors du calcul des valeurs de fitness, cela permet à la fonction de fitness de pénaliser les emplois du temps qui ont des événements à priorité plus élevée sur leur liste d'événements non programmés. De cette façon, l'« expérience » cumulative de la population dans la gestion d'un ensemble d'événements est prise en compte.

Ces observations sont capturées dans l'équation ci-dessous :
\[ f = \sum_{i=1}^{n} \left[ p_{ei} \left( \sum_{j=1}^{m} r_{fe_{ij}}(1 - e_{fij}) + (c_{rei} - a_{ei}) \right) + (1 - p_{ei})(w_{i} + a_{ei}) \right] \]

La fonction implémente simplement les idées notées ci-dessus et consiste essentiellement en deux parties traitant respectivement des propriétés des événements programmés et non programmés. Tout au long de l'équation, \(n\) désigne le nombre total d'événements. Dans la première moitié, qui traite des événements programmés, le booléen \(p_{ei}\) indique si l'événement \(i\) est programmé dans l'emploi du temps actuel, \(m\) désigne le nombre total de caractéristiques, le booléen \(r_{fe_{ij}}\) indique si la salle pour l'événement \(i\) possède la caractéristique \(j\), et le booléen \(e_{fij}\) indique si l'événement \(i\) lui-même nécessite la caractéristique \(j\) ou non. Pour conclure le calcul du décalage, la différence entre la capacité de la salle occupée par l'événement \(i\) et la taille de l'événement, capturée par \(c_{rei} - a_{ei}\), est également ajoutée. En passant à la deuxième moitié, qui traite des événements non programmés, \(w_{i}\) représente l'indice d'adaptation du poids pour l'événement \(i\), tandis que \(a_{i}\) désigne le nombre de participants à l'événement \(i\).

Bien que l'équation calcule une sorte de distance, où des valeurs plus basses sont jugées plus souhaitables, une solution faisable ne produira probablement pas une distance de zéro. La raison en est que, tandis que la deuxième partie de l'équation évalue à zéro une fois que tous les événements ont été programmés, la première partie produira probablement une valeur positive pour tenir compte des décalages de taille et d'installations qui pourraient se produire même dans un emploi du temps faisable.





### 5. Configuration expérimentale

Comme mentionné précédemment, les instances de benchmarking fournies dans [30] abordent la question de savoir comment différents algorithmes se comporteraient lorsque l'accent est mis sur la faisabilité. Celles-ci comprennent trois ensembles de 20 instances "difficiles" de petite taille (environ 200 événements et 5 salles), de taille moyenne (400 événements et 10 salles) et de grande taille (1000 événements et 25 salles). Les instances sont qualifiées de "difficiles" puisque Lewis et Paechter rapportent dans [19] qu'elles sont un sous-ensemble délibéré d'un ensemble plus vaste qui a été "problématique pour trouver la faisabilité". Cependant, toutes ces instances ont au moins une solution faisable, ce qui signifie que les événements peuvent potentiellement être répartis dans les 45 créneaux horaires alloués tout en respectant les contraintes dures mentionnées précédemment dans la section 2. Plus d'informations sur la manière dont les instances particulières ont été générées peuvent être trouvées dans [19].

Pour une comparaison adéquate avec d'autres algorithmes, les tests ont été effectués sur un processeur à 2,66 GHz avec 1 Go de mémoire. De plus, deux ensembles de limites de temps (un ensemble avec 30, 200 et 800 s, et un autre avec 200, 500 et 1000 s) ont été imposés pour les comparaisons avec trois autres algorithmes sur des instances de petite, moyenne et grande tailles. Pour vérifier l'intégrité de notre implémentation particulière et des solutions produites, le programme de test fourni dans [39] a été utilisé.

Le processus de sélection est basé sur le classement, préférant stochastiquement les emplois du temps avec des valeurs de fitness plus élevées. Les valeurs des paramètres adoptées ont principalement été choisies délibérément pour approfondir nos analyses. Le taux de recombinaison (rr) spécifie le pourcentage d'individus produits en utilisant l'opérateur de recombinaison. Le reste de chaque génération est peuplé en faisant des copies des individus les mieux classés. Les descendants produits par recombinaison passent ensuite par le processus de mutation, qui s'applique à un nombre de créneaux horaires spécifié par le taux de mutation (mr). Le premier opérateur de recherche locale est appliqué à tous les descendants produits par recombinaison. Le taux de recherche locale (lr) désigne le nombre d'individus, encore une fois sélectionnés en fonction de leur classement, qui sont soumis au deuxième opérateur de recherche locale décrit précédemment.



### 6. Analyse de performance

Étant donné que des limites de temps différentes ont été utilisées dans [19] et [31] comme critères d'arrêt, nous avons dû diviser le tableau en deux ensembles de colonnes. MA est l'algorithme mémétique introduit dans cet article, GGA est un algorithme génétique de regroupement basé sur la formulation de regroupement de [19] mentionnée précédemment et similaire à notre algorithme à bien des égards, H est un algorithme heuristique de recherche locale monothread qui améliore le regroupement des créneaux horaires individuels, et HSA fait référence au recuit simulé hybride, défini dans [31], qui utilise une combinaison de relaxation de problème, de chaînes de Kempe et d'heuristiques de graphes, ainsi que le recuit simulé pour aborder le problème.

Les résultats pour GGA et l'algorithme H sont basés sur des tests effectués sur un processeur Pentium à 2,66 GHz avec 1 Go de mémoire, tandis que ceux de HSA sont basés sur un processeur Pentium à 3,2 GHz (les informations sur la mémoire pour la configuration de HSA n'ont pas été fournies par [31]) et enfin MA, comme mentionné précédemment, a été testé sur un processeur à 2,66 GHz avec 1 Go de mémoire. Les Figures 5 à 7 montrent les valeurs moyennes et minimales pour ces quatre algorithmes dans le cas des 20 instances de petite, moyenne et grande taille, respectivement. Les paramètres statistiques suivants ont également été calculés :
- la moyenne, qui est considérée comme indicative de la qualité générale de la performance, tandis que l'écart type peut quantifier dans quelle mesure la valeur moyenne est fiable pour représenter la performance à travers chaque ensemble d'instances ;
- le coefficient de variation, qui peut fournir une mesure normalisée lorsqu'on se concentre sur la dispersion indépendante de la moyenne ;
- la faisabilité minimum/maximum, qui compte le nombre d'instances de chaque taille où la faisabilité est atteinte dans au moins une exécution / toutes les exécutions ;
- le meilleur performeur, qui garde trace du nombre d'instances pour lesquelles chaque algorithme a obtenu les meilleurs résultats (surligné en gris dans le tableau) par rapport aux autres algorithmes.

En examinant les résultats généraux pour le petit ensemble, il est évident que MA (rr = 0,8, ls = 0,5) surpasse les autres en moyenne. En fait, si on lui accordait plus de temps, la configuration optimale pourrait surpasser les autres dans toutes les instances, comme le démontre le compte des meilleurs performeurs. Les valeurs de l'écart type et de la faisabilité minimum/maximum montrent également de meilleurs résultats pour cet algorithme. La performance de MA (rr = 0,5, ls = 0,5), de l'algorithme H, de HSA et de GGA suivent respectivement. Notez que la performance de HSA, bien qu'il dispose de beaucoup plus de temps, n'est meilleure que celle de GGA parmi les algorithmes plus contraints en temps et se compare mal aux deux autres. HSA et GGA sont également les seuls algorithmes qui échouent à atteindre la faisabilité minimale pour l'instance n° 14. Pour MA, les 170 secondes supplémentaires de la limite de temps moins contraignante apportent de légères améliorations dans toutes les instances où il n'a pas réussi à atteindre la faisabilité maximale dans les 30 premières secondes.

En passant aux résultats pour l'ensemble moyen, nous pouvons voir que MA (rr = 0,8, ls = 0,5) surpasse à nouveau les autres selon les valeurs moyennes et l'écart type. Cependant, ici, la différence entre les valeurs calculées pour les deux variantes de MA est trop petite pour mériter une explication substantielle. L'algorithme H et GGA suivent dans la limite de temps plus courte, et HSA vient ensuite dans la limite de temps plus longue. Comme dans le cas des petites instances, comparer la performance de MA soumise aux deux limites de temps montre des améliorations légères mais robustes avec le temps supplémentaire.

Enfin, les résultats pour le grand ensemble sont un peu plus compliqués. Dans le cas de la limite de temps de 800 secondes à gauche, la tendance des ensembles petits et moyens continue et MA (rr = 0,5, ls = 0,5) a mieux performé que GGA et l'algorithme H selon les mesures de moyenne, de faisabilité et de performance. Cependant, en passant aux configurations moins contraignantes à droite, il est clair que HSA performe mieux que MA en termes de valeurs moyennes et d'écart type de loin. La performance supérieure de HSA est particulièrement évidente dans les instances individuelles où aucun des algorithmes n'a pu atteindre la faisabilité.

Pour expliquer la baisse significative de la performance de MA, nous avons essayé d'observer son comportement et avons constaté que, bien que l'algorithme change activement entre différentes alternatives dans l'espace de solution dans le cas des petites et moyennes instances (ce qui entraîne une meilleure exploration et de meilleurs résultats), il n'a pas réussi à le faire dans le cas des grandes instances. En fait, les exécutions ont rarement montré des améliorations significatives après les premières 600 secondes environ, attendant plutôt des mutations aléatoires qui pourraient ou non entraîner une descente d'un optimum local. Cela suggère un taux de convergence trop élevé. Cette suggestion est également soutenue par le paramètre de faisabilité maximale, dont la valeur pour MA (rr = 0,5, ls = 0,5) est plus élevée que celle de HSA. En fait, c'est le seul cas parmi nos tests où le meilleur performeur (HSA ici) n'affiche pas de faisabilité maximale. La différence entre la faisabilité minimale et maximale pour HSA dans les grandes instances (15 - 9 = 6) est la plus élevée de tous nos tests, et pointe dans la même direction.

Mais le taux de convergence n'est peut-être pas toute l'histoire. La question de l'extension des algorithmes génétiques dans le contexte du même ensemble de problèmes a été précédemment abordée dans [19]. Les résultats de cette étude sont représentés par les algorithmes GGA et H, et, sur la base de ces résultats, une hypothèse est développée concernant les tendances destructrices de l'opérateur de recombinaison de groupe à mesure que la taille des instances augmente. Des résultats empiriques ultérieurs corroborent cela, amenant les auteurs à postuler que les opérateurs basés sur les groupes peuvent avoir "des pièges dans certains cas". L'idée que la recombinaison peut être de plus en plus destructrice à mesure que la taille des instances augmente est partiellement suggérée par notre choix des paramètres les plus performants de MA à travers les trois ensembles : (rr = 0,8) est un bon performeur dans le cas des petites et moyennes instances, mais quand nous arrivons aux grandes instances, il est remplacé par une recombinaison beaucoup plus faible (rr = 0,2). Néanmoins, comme le montrent les résultats, il y a une différence remarquable entre la performance des deux algorithmes et, selon un test signé de Wilcoxon, ils ont des distributions sous-jacentes significativement différentes. Pour effectuer une analyse plus approfondie à cet égard, nous fournirons d'abord un aperçu plus détaillé de GGA tel que décrit dans [19], et nous tenterons également de reprendre l'analyse là où Lewis et Paechter l'ont laissée.







### 7. Recombinaison basée sur des groupes et ordonnancement

Dans le cadre d'un processus évolutionnaire, [19] adopte une solution à longueur variable, ce qui signifie qu'un nombre variable de créneaux horaires est autorisé au départ. Cela contraste avec notre MA qui dispose d'un nombre fixe de créneaux horaires dès le début. Par la suite, GGA impose une réduction du nombre de créneaux horaires utilisés, en supprimant les créneaux horaires supplémentaires grâce à l'utilisation de la distance à la faisabilité comme mesure de fitness. Ainsi, la fonction de fitness peut être calculée comme le nombre d'événements dont les créneaux horaires correspondants doivent être supprimés, pour empêcher l'emploi du temps de dépasser le nombre autorisé de créneaux horaires. Plus d'informations sur GGA peuvent être trouvées dans [19]. Ici, nous avons tendance à nous abstraire des détails non pertinents et à nous concentrer sur l'opérateur de recombinaison en tant qu'outil pour explorer d'autres points concernant l'applicabilité des opérateurs génétiques à l'ordonnancement.

GGA est basé sur la conception de l'ordonnancement comme un problème de regroupement, tel que défini précédemment, et dans ce cas particulier, les auteurs désignent des créneaux horaires entiers comme les groupes qui devraient être soumis à des opérateurs évolutionnaires. L'opérateur de recombinaison passe par les quatre étapes de sélection de points, d'injection, de suppression de doublons par adaptation, et enfin de reconstruction, et est similaire à l'opérateur de recombinaison discuté précédemment à certains égards. Néanmoins, les créneaux horaires sont jugés être les blocs de construction appropriés du problème, donc, par exemple, la sélection de points dans GGA se limite au début de chaque créneau horaire seulement.

Le choix des créneaux horaires a toutefois des répercussions plus sérieuses. En expliquant le fonctionnement de l'opérateur de recombinaison de MA, nous avons noté que lorsque des doublons apparaissent et doivent être supprimés, l'algorithme parcourt l'ensemble de l'emploi du temps pour supprimer les doublons dès qu'il les repère. L'opérateur de recombinaison de GGA ne traite pas la question de la même manière : il supprime l'ensemble du créneau horaire contenant un événement en double, et crée de nouveaux créneaux horaires à partir de zéro pour accueillir les événements non programmés. Ainsi, les deux procédures de suppression de doublons présentent des différences subtiles. La logique derrière la conception de l'opérateur de MA est qu'une fois les doublons supprimés, de nouveaux créneaux sont ouverts, ce qui signifie de nouvelles opportunités pour que les événements non programmés puissent s'insérer, ce qui, à son tour, se traduit par moins d'événements non programmés si nous avons un nombre fixe de créneaux horaires (comme MA), ou moins de créneaux horaires supplémentaires dans le cas d'une solution à longueur variable (comme GGA). En considérant les créneaux nouvellement ouverts, MA refuse de se limiter à l'évolution des seuls créneaux horaires, et pourrait même se ramifier pour évoluer en lignées séparées au sein d'un même créneau horaire. L'engagement de GGA à faire évoluer des créneaux horaires à partir de zéro, d'autre part, l'empêche de "creuser à l'intérieur" des créneaux horaires et d'améliorer les créneaux horaires existants dont les doublons ont été supprimés. Ainsi, dans le cas de GGA, la suppression des événements en double tout en conservant le créneau horaire peut aboutir à des descendants qui, comme le soulignent à juste titre Lewis et Paechter, "seraient en fait de mauvaise qualité car ils utiliseraient presque certainement plus de créneaux horaires que l'un ou l'autre des deux parents." Avec ce cadre théorique, nous avons décidé de comparer la performance des deux recombinaisons en calculant le nombre net d'événements qui ont été supprimés après chaque application de l'opérateur de recombinaison. Construit de cette manière, une quantité positive de réparation signifie que l'opérateur a, en somme, supprimé des événements de l'emploi du temps, tandis qu'une valeur de réparation négative signifie que l'opérateur a, en somme, réussi à programmer plus d'événements qu'il n'en a supprimés.

Pour suivre les événements non programmés dans le cas de MA, il suffit de compter le nombre d'événements qui occupent la liste des non programmés. Dans le cas de GGA, puisqu'il ne conserve pas une telle liste, le nombre d'événements occupant tous les "créneaux horaires supplémentaires" est calculé à la place, où "les créneaux horaires supplémentaires" sont jugés être ceux qui sont les moins densément remplis.

Pour approfondir l'analyse, le niveau de diversité maintenu par chaque opérateur de recombinaison est également mesuré en fonction des observations suivantes :
- potentiellement, chaque paire de deux événements que nous pourrions choisir peut coexister dans un créneau horaire, à moins que cette coexistence ne soit interdite par des contraintes de conflit. Ainsi, en soustrayant de toutes les paires d'événements possibles, toutes les paires qui incluent des événements en conflit, on obtient une limite théorique pour le nombre de paires d'événements légales ;
- la diversité pourrait être opérationnalisée en termes de diversité des paires d'événements, c'est-à-dire en mesurant le degré auquel les paires d'événements possibles sont "réalisées" dans la population actuelle.

Une mesure de la distance à la diversité est calculée en utilisant la formule suivante :
\[ f = \sum_{i=1}^{n} \sum_{j=2}^{n} [1 - r_{ij}] \]
où le nombre total d'événements est noté \(n\), et \(r_{ij}\) vaut 1 lorsque les événements \(i\) et \(j\) sont en conflit ou lorsqu'il existe un créneau horaire dans la population qui contient les deux événements, et 0 sinon.

Les résultats de nos mesures pour une instance particulièrement difficile de chaque taille sont présentés dans la Figure 8. Notez que les deux opérateurs de recombinaison ont été réduits à leurs minimums essentiels, et divers paramètres algorithmiques (c'est-à-dire le nombre initial de la population, les taux des opérateurs, etc.) ont été égalisés. De plus, comme critères de fitness, les deux algorithmes comptent le nombre d'événements non programmés. Pour la formulation à taille fixe de MA, cela signifie simplement compter les événements sur la liste des non programmés, mais pour la formulation à taille variable de GGA, comme mentionné précédemment, cela équivaut à compter le nombre d'événements dont les créneaux horaires correspondants doivent être supprimés pour empêcher l'emploi du temps de dépasser le nombre autorisé de créneaux horaires. Le contrôle des variables telles que les taux des paramètres et l'évaluation de la fitness nous permet de faire des comparaisons significatives entre les deux opérateurs.

En regardant la Figure 8, la première chose à noter est que, comme prévu, la quantité de réparation requise augmente avec la taille des instances pour les deux algorithmes. Cela est prévisible, car à mesure que les créneaux horaires augmentent en taille, la probabilité qu'un créneau horaire, injecté à partir d'un autre emploi du temps, puisse partager des événements avec plus de créneaux horaires de l'emploi du temps hôte augmente, ce qui, à son tour, augmente le coût de l'opérateur de recombinaison. Cependant, un examen plus approfondi révèle que la différence relative entre la réparation mesurée pour les deux recombinaisons augmente également de manière significative, et la réparation encourue dans le cas de notre opérateur dépend beaucoup moins de la taille de l'instance. Quantitativement, comme illustré par la Figure 9, par rapport à notre recombinaison, la recombinaison basée sur des groupes nécessite respectivement 6,25/1,25 = 5, 74,87/5,72 ≈ 13,7, et 451,7/12,41 ≈ 36 fois plus de réparations pour les trois tailles d'instances.

Concernant le niveau de diversité, les valeurs pour MA sont plus élevées dans toutes les trois instances, indiquant une distance à la diversité plus élevée et par conséquent une diversité moyenne de la population plus faible. Il est certain que, en raison de leurs différentes méthodes d'initialisation, les deux algorithmes empruntent des chemins très différents dès le départ, mais, sauf pour la petite instance où son niveau de diversité diminue en moyenne au cours des générations suivantes, le niveau de diversité déjà plus élevé de GGA ne diminue pas en moyenne avec le temps dans le cas des instances moyennes et grandes. On peut soutenir que les implications de maintenir un niveau de diversité élevé/faible dépendent en partie de la quantité de réparations qu'il entraîne pour l'opérateur, ce qui est évident à la lumière de la relation "en miroir" entre la diversité et les réparations requises, évidente dans la Figure 8, pour les deux algorithmes. La corrélation élevée entre la diversité et les réparations, comme le démontre la Figure 9, peut être expliquée en examinant le processus par lequel toute réparation peut augmenter la diversité. La quantité de réparations requises détermine essentiellement la quantité de travail

 que l'algorithme doit recommencer à zéro, et puisque les deux algorithmes procèdent de cette manière en utilisant des processus essentiellement aléatoires, il est naturel qu'ils travaillent à augmenter la diversité, comme tout processus de randomisation est tenu de le faire. Ainsi, le niveau élevé de réparations met GGA dans une situation désavantageuse de deux manières : une temporaire (c'est-à-dire en supportant le coût des réparations), mais aussi une permanente (c'est-à-dire en maintenant un niveau élevé de diversité dans la génération suivante).

En résumé, nous pensons que la différence dans les résultats obtenus de MA et GGA provient, au moins en partie, de l'engagement réducteur de GGA à reconnaître les créneaux horaires comme le seul bloc de construction approprié du problème. Comme mentionné précédemment, l'intuition derrière les formulations de regroupement était de pouvoir transmettre des structures significatives d'une génération à l'autre, mais la quantité élevée de réparations que GGA subit est une conséquence directe de cet engagement mentionné à faire évoluer uniquement les créneaux horaires. Ainsi, on peut soutenir que, bien que l'idée de faire évoluer des affectations d'événements optimales soit dénuée de sens (puisque l'optimalité de toute affectation unique dépend en partie d'autres affectations), faire évoluer des créneaux horaires entiers pourrait imposer une restriction artificielle sur le regroupement des événements, comme discuté précédemment. Nous proposons plutôt que le niveau de granularité alterne entre les regroupements arbitraires d'événements dans les opérateurs de recombinaison, de recherche locale et de mutation.

Pour mesurer les tendances destructrices de chaque algorithme, le GGA susmentionné (complet avec opérateurs de recombinaison et de mutation, heuristiques et fonction de fitness) a été développé. En tant que mesure de l'impact de ces opérateurs de recombinaison sur le processus de recherche, nous avons décidé de quantifier la quantité de réparations.



### 8. Conclusion

L'objectif de cet article était d'examiner l'applicabilité des algorithmes évolutionnaires au problème de l'ordonnancement des cours. En conséquence, nous avons tenté de développer un algorithme mémétique pour résoudre un ensemble prédéfini de problèmes de référence. En comparaison avec trois autres algorithmes mentionnés dans l'article, notre algorithme mémétique a mieux performé dans le cas des instances petites et moyennes, mais a été surpassé par l'un des algorithmes dans le cas des grandes instances. Par conséquent, nous pourrions continuer cette recherche en nous concentrant sur la résolution d'un grand problème (n ≈ 1000) en le divisant en sous-problèmes constitutifs (n ≈ 500) qui pourraient être mieux gérés par l'algorithme. Cela implique, bien sûr, un certain nombre d'hypothèses concernant les contraintes du problème et le paysage des solutions qui pourraient ne pas toujours être viables. Néanmoins, le succès de l'algorithme dans le traitement des problèmes de taille moyenne (n ≈ 400) en fait une piste de recherche future digne d'intérêt.

Après une analyse générale de la performance algorithmique, nous nous sommes concentrés sur la différence entre les résultats obtenus par notre propre algorithme et un algorithme génétique basé sur des groupes. Nous avons mis en œuvre l'algorithme basé sur des groupes avec une mesure de diversité. L'analyse qui a suivi a corroboré l'hypothèse de [19] concernant les "pièges" de l'algorithme de regroupement. Nous avons soutenu qu'un focus exclusif sur l'évolution des créneaux horaires entrave effectivement la transmission des structures évoluées d'une génération à l'autre, et avons attribué la meilleure performance de l'algorithme mémétique à l'utilisation d'un schéma de sélection multi-niveaux en général, et à un opérateur de recombinaison qui ne se limitait pas aux groupes en particulier.
