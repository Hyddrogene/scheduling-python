Cette section fournit une analyse comparative des méthodes d'apprentissage par renforcement utilisées dans la planification dynamique des tâches dans les premières études. Les mérites et démérites des techniques identifiées en termes de planification des tâches distribuées ont été discutés dans le Tableau 1. Les cadres de planification des tâches dynamiques mis en œuvre peuvent être évalués dans le Tableau 2.

En passant en revue les recherches antérieures dans ce domaine technique, il a été possible de réaliser certaines croyances que la communauté de recherche entretient à propos de l'apprentissage par renforcement. Certains pensent que la discipline actuelle de l'apprentissage par renforcement prend beaucoup de temps pour converger et apprendre quelque chose de significatif. Certains croient que la seule façon d'utiliser les techniques d'apprentissage par renforcement de manière pratique est soit par un pré-entraînement dans un simulateur, soit en transférant les connaissances acquises entre différents domaines. Comparées à d'autres techniques d'apprentissage automatique, ces philosophies restreignent les techniques de résoudre des problèmes réels et certains hésitent à poursuivre l'étude dans ce domaine. Cela pourrait être la raison pour laquelle ce domaine connaît un tel succès dans des jeux vidéo comme Atari ou des jeux de stratégie comme Go, où les règles sont connues et prédéfinies.

Lors de l'examen des recherches menées sur la planification dynamique des tâches, il est noté qu'un large éventail de recherches a été accompagné au fil des ans. Parmi toutes les approches entreprises, l'apprentissage par renforcement a pu fournir une direction réussie par le biais d'une planification des tâches idéale, mais elles sont encore au niveau de la recherche. On envisage que la plupart des recherches réussies ont utilisé des techniques sans modèle, des réseaux Q profonds et des algorithmes de gradient de politique dans la planification dynamique des tâches, tandis qu'un nombre substantiel de recherches a été mené sur les performances de planification de l'apprentissage par renforcement à agent unique contre multi-agents. Comme observé dans les recherches récentes, il existe une faisabilité croissante à appliquer l'apprentissage par renforcement basé sur des modèles dans la planification dynamique, en particulier dans des environnements en constante évolution et imprévisibles. En résumé, il peut être compris que la planification optimisée et dynamique des tâches dans des environnements incertains est un problème difficile en termes d'agents, de dynamiques environnementales, de critères de planification et de complexité du scénario. Certaines des solutions mises en œuvre présentent une faible performance d'exécution des tâches en raison de l'incertitude des environnements. La planification dynamique des tâches dans un environnement incertain et en constante évolution, où de nombreux agents sont impliqués de manière interactive, a été peu étudiée par les travaux antérieurs. De nombreux problèmes de généralisation sont rencontrés par de nombreux groupes de recherche. Le temps de calcul inacceptable pour une mise en œuvre en temps réel et la complexité des algorithmes sont également des problèmes.



