### 3.1.1.2 Ordre d'affectation des cours

Matijas et al., 2009 ont étudié le problème de la planification des exercices de laboratoire (LETP) — un sous-ensemble du problème général d'ordonnancement des cours universitaires (UCTP). L'efficacité du graphe de construction a de nouveau été citée comme une préoccupation majeure, notamment car l'objectif des auteurs était de mettre en œuvre leur système pour la planification réelle dans leur institution, l'Université de Zagreb. La construction du graphe ici différait de celle d'Ayob et Jaradat, 2009 avec l'introduction de "nœuds de quai" — une paire ordonnée de salle et de période. Le graphe de construction était tripartite, avec des ensembles indépendants L de conférences, D de nœuds de quai et S d'étudiants. Des arêtes étaient utilisées pour connecter les nœuds dans L avec D et dans D avec S, donnant une borne supérieure sur le nombre d'arêtes de (l + s)pr, où l, p, r et s désignent respectivement le nombre de conférences, de périodes, de salles et d'étudiants. Le prétraitement réduisait généralement le graphe en éliminant les arêtes connues a priori comme invalides. Le maintien de la faisabilité était assuré par des "barrières de contraintes", propres à chaque fourmi virtuelle. Bien que le graphe de construction restait essentiellement statique, certaines arêtes devenaient invisibles pour les fourmis virtuelles chaque fois que les traverser rendait une affectation partielle non réalisable dans ce chemin particulier. Cela était réalisé en forçant la valeur heuristique à zéro lorsque nécessaire. L'ordre dans lequel les événements — exercices de laboratoire dans cette étude, mais également les conférences — étaient planifiés n'était pas fixe, mais réinitialisé avant chaque nouvelle itération. Le nouvel ordre était déterminé en référence au meilleur chemin de l'itération précédente. Les événements ayant plus d'étudiants non planifiés dans ce chemin recevaient une priorité plus élevée dans la prochaine itération.

L'idée d'un ordre dynamique des conférences apparaît également dans Mayer et al., 2012, dont l'approche a remporté la 4ème place pour la piste 2 de l'ITC2007 (post-inscription). Ici, les phéromones étaient associées aux nœuds plutôt qu'aux arêtes. Cela est permis car, bien que l'ordre des affectations de conférences puisse avoir un impact direct sur l'efficacité d'un algorithme, il n'a aucune signification dans le contexte du calendrier finalisé. Par conséquent, les nœuds, ainsi que leurs valeurs de phéromones, peuvent être mélangés pendant une exécution. L'approche adoptée consistait à générer une permutation aléatoire uniforme des conférences, πe, à chaque itération. La priorité des périodes et des salles était également randomisée, mais avec des poids correspondant à leurs valeurs de phéromones actuelles. De cette manière, l'élément probabiliste de l'ACO était déplacé et les affectations étaient techniquement faites de manière déterministe et gourmande. La randomisation naïve de πe semblerait moins sophistiquée que dans Matijas et al., 2009 et pourtant il y a une incertitude dans la littérature concernant la meilleure méthode et même l'importance de l'ordre des conférences.

Patrick et Godswill, 2016 font partie des rares équipes de recherche à avoir utilisé le benchmark de la piste 3 de l'ITC2007 dans leur approche de planification basée sur l'ACO comme problème à objectif unique. Ce qui est particulièrement intéressant pour cette revue est leur graphe de construction initial, qui était bipartite et entièrement connecté. Une partie était composée de nœuds de conférence tandis que l'autre comprenait des nœuds combinés salle-période.



Les parcours commençaient par une conférence aléatoire et il n'y avait pas d'ordre fixe pour les affectations de conférences. Les auteurs ont conclu que l'expressivité permise par cette représentation valait le temps de calcul plus élevé, trouvant également que 8 était le nombre optimal de fourmis virtuelles pour leur système. Une interprétation récente de l'ACO par Fotovvati et Mirghaderi, 2023, a conservé le même style de graphe de construction bipartite, tout en introduisant un biais dynamique dans les règles de mise à jour des phéromones, ce qui a aidé à la convergence.

Thepphakorn et Pongcharoen, 2012, ont également expérimenté l'ACO sur l'ITC2007. Leur approche de l'ordre des affectations utilisait des heuristiques basées sur la coloration des graphes, à savoir l'ordre aléatoire (RO), le plus grand nombre d'inscriptions en premier (LE), le plus grand degré en premier (LD), le plus grand degré coloré en premier (LCD), le plus grand degré pondéré en premier (LWD) et le degré de saturation (SD) (Burke et al., 2007). Ces heuristiques fonctionnent en déterminant un niveau de priorité pour chaque événement (équivalent à un cours ou une conférence) et en les ordonnant en conséquence. LE, par exemple, attache une priorité plus élevée aux conférences de cours ayant un plus grand nombre d'étudiants inscrits, tandis que la méthode proposée par les auteurs — le plus grand degré de période non permise en premier (LUPD) — utilise le nombre de périodes indisponibles de manière similaire. En employant une variante de l'ACO connue sous le nom de système de fourmis basé sur le rang (AS-Rank) (Bullnheimer et al., 1999), le graphe de construction a été déterminé heuristiquement lors d'une phase d'initialisation avant que l'optimisation ne soit autorisée à procéder normalement. Six techniques heuristiques ont été testées : RO et LE, ainsi que LUPD — seul et combiné en série et en parallèle avec LE. Le pourcentage de solutions réalisables sur une exécution AS-Rank à itération limitée a été utilisé comme principale métrique de performance. Les résultats ont montré des performances supérieures des heuristiques hybrides LUPD+LE, tant en termes de métrique de faisabilité que de vitesse de calcul.

D'autres praticiens utilisant des ACO ont impliqué l'utilisation de heuristiques simples similaires pour l'ordre des événements. Dans Lutuksin et al., 2009, la description brève est la suivante : "trier les cours (C) selon la priorité des événements et la taille des étudiants", sans plus de détails. Pendant ce temps, dans Munirah et al., 2019, des résultats expérimentaux ont été présentés avec et sans "priorité", où, "la priorité est donnée à plusieurs tests pour déterminer l'allocation de l'emploi du temps des cours pour un grand nombre d'étudiants à planifier dans le temps prescrit".




Les parcours commençaient par une conférence aléatoire et il n'y avait pas d'ordre fixe pour les affectations de conférences. Les auteurs ont conclu que l'expressivité permise par cette représentation valait le temps de calcul plus élevé, trouvant également que 8 était le nombre optimal de fourmis virtuelles pour leur système. Une interprétation récente de l'ACO par Fotovvati et Mirghaderi, 2023, a conservé le même style de graphe de construction bipartite, tout en introduisant un biais dynamique dans les règles de mise à jour des phéromones, ce qui a aidé à la convergence.

Thepphakorn et Pongcharoen, 2012, ont également expérimenté l'ACO sur l'ITC2007. Leur approche de l'ordre des affectations utilisait des heuristiques basées sur la coloration des graphes, à savoir l'ordre aléatoire (RO), le plus grand nombre d'inscriptions en premier (LE), le plus grand degré en premier (LD), le plus grand degré coloré en premier (LCD), le plus grand degré pondéré en premier (LWD) et le degré de saturation (SD) (Burke et al., 2007). Ces heuristiques fonctionnent en déterminant un niveau de priorité pour chaque événement (équivalent à un cours ou une conférence) et en les ordonnant en conséquence. LE, par exemple, attache une priorité plus élevée aux conférences de cours ayant un plus grand nombre d'étudiants inscrits, tandis que la méthode proposée par les auteurs — le plus grand degré de période non permise en premier (LUPD) — utilise le nombre de périodes indisponibles de manière similaire. En employant une variante de l'ACO connue sous le nom de système de fourmis basé sur le rang (AS-Rank) (Bullnheimer et al., 1999), le graphe de construction a été déterminé heuristiquement lors d'une phase d'initialisation avant que l'optimisation ne soit autorisée à procéder normalement. Six techniques heuristiques ont été testées : RO et LE, ainsi que LUPD — seul et combiné en série et en parallèle avec LE. Le pourcentage de solutions réalisables sur une exécution AS-Rank à itération limitée a été utilisé comme principale métrique de performance. Les résultats ont montré des performances supérieures des heuristiques hybrides LUPD+LE, tant en termes de métrique de faisabilité que de vitesse de calcul.

D'autres praticiens utilisant des ACO ont impliqué l'utilisation de heuristiques simples similaires pour l'ordre des événements. Dans Lutuksin et al., 2009, la description brève est la suivante : "trier les cours (C) selon la priorité des événements et la taille des étudiants", sans plus de détails. Pendant ce temps, dans Munirah et al., 2019, des résultats expérimentaux ont été présentés avec et sans "priorité", où, "la priorité est donnée à plusieurs tests pour déterminer l'allocation de l'emploi du temps des cours pour un grand nombre d'étudiants à planifier dans le temps prescrit".










Il est évident que les deux aspects que sont l'ordre d'affectation des conférences et la gestion des contraintes peuvent se compléter mutuellement dans un ACO bien conçu. Un ordre d'affectation bénéfique des conférences peut réduire le nombre de violations naturelles des contraintes strictes et ainsi diminuer la charge de calcul nécessaire pour les gérer. Cette section a passé en revue diverses approches pour ces deux éléments, appliquées à la fois à des problèmes d'emploi du temps réels et simulés, ainsi qu'à d'autres domaines problématiques connexes. Les sections suivantes couvrent les détails techniques du MMAS proposé, les composants supplémentaires et les expérimentations.














Pour faciliter la référence, cette section commence par une vue d'ensemble du système proposé, illustrée par deux schémas. Le premier schéma, la Figure 3.2, décrit le processus d'entraînement. En commençant par un ensemble de petits problèmes d'entraînement, toutes les permutations de cours possibles sont mises en file d'attente. Pour chaque permutation distincte, le MMAS (Max-Min Ant System) résout le problème correspondant. Une mesure de performance peut ainsi être attribuée à chaque permutation pour chaque problème. Ces valeurs cibles sont ensuite utilisées pour entraîner un modèle de régression, désigné par commodité par T0. La justification complète de T0 est donnée dans la section 3.2.3, tandis que la génération de son ensemble d'entraînement est décrite plus loin dans la section 3.3.2.

Le deuxième schéma, la Figure 3.3, illustre la phase d'expérimentation de référence, dans laquelle des problèmes ITC2007 non vus auparavant sont résolus. Ayant appris à identifier une bonne permutation de cours, T0 est utilisé pour informer le fonctionnement d'un algorithme génétique de base (dont les détails sont donnés dans la section 3.2.4). L'espace de permutation du problème est mappé à l'espace équivalent des permutations apprises. L'algorithme génétique recherche dans cet espace pour trouver une permutation prometteuse. Le mappage, T0, et l'algorithme génétique sont combinés et désignés comme le Prédicteur de Permutation. Comparé à cette approche, un comparateur de base connu sous le nom de Permutation Aléatoire, qui n'a aucun comportement appris et sélectionne simplement une permutation au hasard, est utilisé. Le MMAS résout le problème comme auparavant. Pour le Prédicteur de Permutation et la Permutation Aléatoire, la recherche locale peut être activée ou désactivée dans le cadre du MMAS, ce qui signifie qu'au total, quatre variantes sont testées. Celles-ci sont désignées PP-LS, PP, RP-LS et RP, comme indiqué dans la Figure 3.3.
*

Dans une enquête sur l'état de l'art, Burke et al., 1997 ont déploré à la fois l'absence d'une description standardisée du UCTP et d'un référentiel pour comparer les algorithmes de planification. C'est dans ce contexte de recherche que le Metaheuristics Network a été formé. Un projet de la Commission européenne impliquant la collaboration entre cinq instituts européens (Rossi-Doria et al., 2006) avait pour objectif de comparer empiriquement les performances de différentes métaheuristiques sur des problèmes d'optimisation combinatoire, y compris la planification universitaire. À partir des travaux de Rossi-Doria et al., 2003, une formulation standardisée du UCTP a été élaborée, permettant de comparer plus équitablement les résultats de différents chercheurs (Alhuwaishel et Hosny, 2011). Un générateur d'instances artificielles, mentionné dans Lewis et Paechter, 2005, a été créé par Ben Paechter pour refléter certains aspects de la planification à l'Université Napier. Il fonctionnait sur huit paramètres d'entrée (événements, salles, caractéristiques, caractéristiques par salle, pourcentage de caractéristiques utilisées, étudiants, événements maximum par étudiant et étudiants maximum par événement) et une graine aléatoire. Le générateur a initialement produit un ensemble de référence de douze instances de problèmes UCTP, désignées comme "faciles", "moyennes" et "difficiles" (plus tard adaptées en "petites", "moyennes" et "grandes") selon leur complexité et leur taille. Cet ensemble contient respectivement cinq, cinq et deux de chaque type et toutes ont au moins une solution parfaite. Il apparaît dans la littérature sous différents noms tels que le référentiel Paechter, l'ensemble Socha et al. (2002), le référentiel Socha ou l'ensemble Rossi-Doria et al. (2003).

### 2.2.2 Les Soixante Instances

Dans Lewis et Paechter, 2005, soixante autres instances ont été générées par le générateur Paechter. Elles ont de nouveau été subdivisées en trois ensembles de "petites", "moyennes" et "grandes", avec 20 instances dans chaque catégorie. Elles ont été mises gratuitement à disposition pour les tests et continuent d'être populaires (Song et al., 2018) avec les instances de complexité la plus élevée suscitant l'intérêt des solveurs de faisabilité pure (voir aussi la section 2.3.2.4).

### 2.2.3 Compétition Internationale de Planification 2002

Les travaux du Metaheuristic Network ont conduit à la création de la Compétition Internationale de Planification (ITC) en 2002. Pour cela, des formats de données standardisés (avec l'extension de fichier .tim) et les règles formalisées par Ben Paechter ont été utilisés. Le premier ensemble, au format post-inscription, comprenait 20 instances. Celles-ci ont été publiées aux concurrents à des intervalles échelonnés avant la compétition et sont maintenant maintenues en ligne. Elles continuent d'être un référentiel populaire, cité par exemple dans Badoni et al., 2014, où la performance de deux algorithmes a été testée.


### 2.2.4 Compétition Internationale de Planification 2007

La planification post-inscription a de nouveau été mise en avant, sous une forme légèrement modifiée, en tant que piste 2 de la deuxième édition de l'ITC en 2007. Cependant, ce qui intéresse particulièrement cette thèse est la formulation de la piste 3, basée sur le curriculum (Gaspero et al., 2007). Un ensemble de 21 instances, nominalement comp01 à comp21, ont été modélisées sur le problème réel de planification de l'Université d'Udine. Pour assurer une généralité, une gamme de besoins provenant de différentes facultés a été incluse. La taille des instances, qui compte des centaines de conférences, est principalement à l'échelle départementale. Le modèle a été construit autour d'un certain nombre d'entités et de variables, pour lesquelles nous utilisons la notation suivante :

- **Jours**. Un jour avec l'indice \(i\) est noté \(d_i\). Le nombre de jours dans une semaine disponibles pour l'enseignement est fixé par l'instance de problème. L'ensemble des jours est \(D\).
- **Créneaux horaires**. Chaque jour est divisé en un ensemble fixe et égal de créneaux horaires, \(t\).
- **Périodes**. Une période, \(p_i\), est un jour × créneau horaire. L'ensemble des périodes est \(P\).
- **Salles**. Une salle, \(r_i \in R\), est définie par sa capacité d'accueil, \(cap(r_i)\), qui ne doit pas être dépassée. À tous autres égards, toute salle est apte à accueillir toute conférence. En adoptant la terminologie utilisée par Lewis en 2006, une paire salle/période est appelée un lieu, et l'action d'attribuer une conférence à une salle/période particulière est appelée placement.
- **Conférences**. Les conférences, \(l_i\), sont des événements qui doivent être attribués à un lieu approprié. L'ensemble de toutes les conférences est \(L\).
- **Cours**. Chaque conférence appartient à un seul cours, \(C_i\), tandis qu'un cours peut comprendre plusieurs conférences. Chaque cours \(C_i\) a un nombre fixe d'étudiants inscrits, \(stud(C_i)\), ainsi que plusieurs autres propriétés. L'ensemble de tous les cours est \(C\).
- **Enseignants**. Un enseignant est pré-affecté à chaque cours, tandis que les enseignants peuvent enseigner plusieurs cours. L'ensemble unitaire consistant en l'enseignant d'un cours \(C_i\) est noté \(teach(C_i)\) et l'ensemble de tous les enseignants est \(T\).
- **Curricula**. Un curriculum, \(u_i\), est un ensemble de cours. Un cours peut appartenir à plusieurs curricula, mais doit appartenir à au moins un. Tout deux cours dans le même curriculum ont implicitement des étudiants en commun et l'ensemble de tous les curricula est \(U\).

La Figure 2.2 offre une visualisation de la relation entre les curricula \(u_1, u_2\), les cours \(C_1, \ldots, C_4\) et les conférences \(l_1, \ldots, l_{16}\) pour un exemple simplifié publié de la piste 3 de l'ITC2007. La Table 2.1, quant à elle, donne les caractéristiques des instances comp*.

La formulation prescrivait également un ensemble de contraintes strictes et souples, notées H ou S et nommées et numérotées comme suit. Les points de pénalité encourus pour les violations des contraintes souples sont également donnés.

- **H1: Toutes les conférences**. Toutes les conférences doivent être attribuées à un lieu distinct.
- **H2: Occupation des salles**. Une salle ne peut accueillir qu'une seule conférence par période.
- **H3: Conflits de curriculum**. Les conférences des cours appartenant à un même curriculum ne peuvent être planifiées dans la même période.
- **H4: Périodes indisponibles**. Une ou plusieurs périodes peuvent être prédéfinies comme indisponibles pour un cours particulier \(C_i\). Ainsi, aucune conférence appartenant à ce cours ne peut avoir lieu pendant ces périodes. Cet ensemble de périodes est noté \(unav(C_i)\), tandis que l'union de ces ensembles pour tous les cours est notée \(N\).
- **H5: Conflits d'enseignants**. Les conférences des cours partageant un même enseignant ne peuvent être planifiées dans la même période.
- **S1: Capacité des salles**. Le nombre d'étudiants doit être inférieur ou égal à la capacité de la salle accueillant la conférence. Chaque étudiant au-delà de la capacité compte pour un point de pénalité.
- **S2: Jours de travail minimum**. La planification des conférences d'un cours \(C_i\) doit être répartie sur un nombre minimum de jours distincts, noté \(mwd(C_i)\). Chaque jour en dessous de ce minimum compte pour cinq points de pénalité.
- **S3: Compacité du curriculum**. Les conférences doivent être adjacentes dans le créneau horaire à une autre conférence du même curriculum. Toute conférence "isolée" à cet égard compte pour deux points de pénalité. Le dernier créneau horaire d'un jour n'est pas considéré comme adjacent au premier créneau horaire du jour suivant.
- **S4: Stabilité des salles**. Au sein de chaque cours, les conférences doivent se tenir dans la même salle. Chaque salle distincte supplémentaire utilisée pour un cours compte pour un point de pénalité.

Le schéma de pénalités pour les contraintes souples décrit ci-dessus fournit une méthode pour exprimer la qualité globale d'une solution. Pour les solutions réalisables, le coût est simplement la somme des pénalités encourues pour les violations des contraintes souples, une valeur désignée par SCV(soft violation contrainte). Un SCV élevé reflète une faible qualité et vice versa. Dans certaines circonstances, il peut également être utile d'attribuer un coût aux solutions non réalisables. Le cas d'utilisation type pour cela est lorsqu'un processus d'optimisation est autorisé à traverser l'espace non réalisable à la recherche de régions déconnectées de faisabilité. Dans Mayer et al., 2012, la distance à la faisabilité (DTF) était définie comme la somme totale des étudiants assistant à des conférences qui n'ont pas pu être planifiées. DTF — décrite par Schaerf, 1999 comme une mesure nominale de l'ampleur de l'infaisabilité d'une solution — peut être conceptualisée et exprimée de différentes manières, telles que le nombre de cours non attribués, ou une somme totale de toutes les violations strictes. Une valeur représentant DTF peut alors être retournée comme une statistique résumée, ou amalgamée avec le SCV, à la discrétion du praticien.

Une augmentation de cet ensemble de contraintes souples a été proposée par Bonutti et al., 2012. L'objectif était à la fois d'augmenter la complexité du modèle et de traiter certaines insuffisances perçues des définitions originales. Par exemple, l'intention derrière la compacité du curriculum était de décourager les longues pauses dans la journée d'un étudiant, en pénalisant toute conférence temporellement isolée appartenant à un curriculum. Considérons une journée de 10 créneaux horaires. La planification de deux conférences communes dans les créneaux horaires 1 et 10 déclencherait, isolément, une pénalité. Si quatre conférences étaient planifiées aux créneaux horaires 1, 2, 9 et 10, aucune pénalité ne serait encourue — malgré l'existence d'une pause de taille comparable. Dans la définition alternative de la compacité du curriculum de Bonutti, nommée "Windows", les pauses indésirables encourent le même nombre de violations que leur longueur en périodes. Une liste complète des contraintes suggérées est donnée dans la Figure 2.2, ainsi que cinq configurations proposées. Malgré les mérites de celles-ci, la configuration originale, UD2, est restée la plus durable dans la littérature et est également adoptée dans notre travail.

Depuis, des ensembles d'instances de problèmes supplémentaires ont été créés pour la formulation de la piste 3. Ceux-ci sont : DDS* (7 instances), test* (4 instances) (Bonutti et al., 2012), erlangen* (6 instances), EA* (basé sur les emplois du temps EasyAcademy, initialement 12 instances en 2014, auxquelles 11 autres ont été ajoutées en 2021) et Udine* (9 instances). Parmi les problèmes de ces ensembles, les erlangen* sont de loin les plus grands, avec des comptes de cours allant de 705 à 850 et des comptes de conférences de 788 à 930.



### 2.2.5 Compétition Internationale de Planification 2011

Bien que la compétition de 2011 se soit concentrée sur la planification des lycées plutôt que des universités, elle a apporté des innovations intéressantes à travers différents domaines. Les 15 types de contraintes, allant de la répartition des conférences aux temps d'inactivité des étudiants, pouvaient être désignés comme souples ou strictes, permettant ainsi un contrôle modulaire complet de la conception du problème.

### 2.2.6 Compétition Internationale de Planification 2019

La possibilité de combiner et d'assortir les types de contraintes a été maintenue lors de la compétition de 2019. Cette année-là, une autre tendance est devenue apparente : les formulations s'éloignaient progressivement des ensembles générés artificiellement pour se tourner vers des cas réels. Comme l'ont confirmé les organisateurs à l'époque : « Nous avons déjà un accord avec dix institutions, dont l'Université Purdue aux États-Unis, l'Université Masaryk en République tchèque, l'Université de Science et de Technologie AGH en Pologne et l'Université Kultur d'Istanbul en Turquie, pour que nous puissions utiliser leurs données. » (Müller et al., 2018)

La nouvelle formulation a introduit une plus grande flexibilité que les pistes de l'ITC2007. L'inclusion de plusieurs fonctionnalités novatrices a également contribué à rapprocher la théorie de la pratique en termes de simulation des complexités d'un problème réel. Voici quelques-unes des principales différences entre l'ITC2007 et l'ITC2019 :

- **Sectionnement des étudiants**. Auparavant, les conflits entre étudiants étaient implicites par l'appartenance commune des cours à un curriculum. Ces violations sont explicites dans l'ITC2019, car chaque étudiant est défini comme une entité avec des exigences d'inscription individuelles.
- **Contraintes**. Un pool plus large de contraintes, appelées "distributions", a été introduit. La majorité d'entre elles peuvent être appliquées comme strictes ou souples, tandis qu'un plus petit nombre sont invariantes. Certaines contraintes étaient définies comme par paires, tandis que d'autres peuvent s'appliquer à plusieurs classes.
- **Enseignants**. Les enseignants sont absents de la nouvelle formulation. Ils peuvent être modélisés soit implicitement en utilisant les contraintes disponibles, soit explicitement sous la forme d'un "étudiant".
- **Cours**. Les cours sont subdivisés en "configurations" et ensuite en "sous-parties". Cela permet de modéliser différentes versions du même cours/module, par exemple licence vs master.
- **Classes**. Équivalentes aux "conférences" dans l'ITC2007, les classes existent au sein des sous-parties. Une structure hiérarchique plus complexe est rendue possible. Certaines paires de classes peuvent avoir des relations parent-enfant. Tout étudiant affecté à une classe "enfant" est également obligé de suivre la classe "parent". C'est une manière pratique de modéliser une dépendance séance de cours-laboratoire, par exemple.
- **Granularité**. Le temps est discrétisé en petits créneaux de 5 minutes, ce qui signifie que les classes peuvent avoir des heures de début décalées et potentiellement des chevauchements partiels.
- **Localisation des salles**. Les distances de déplacement sont définies entre les salles. Cela vise à encourager la prise en compte de la géographie physique du campus.
- **Variation hebdomadaire**. L'ITC2019 permet des horaires hebdomadaires différents au cours d'un semestre, plutôt qu'un emploi du temps statique répété chaque semaine.
- **Format**. En raison de leur structure en arbre, les problèmes de l'ITC2019 sont fournis au format XML plutôt qu'en fichiers texte brut .ctt.

Les contraintes strictes suivantes sont intégrées dans le système de notation :

- **H1** : Tous les étudiants doivent être affectés à une classe de chaque sous-partie d'une seule configuration.
- **H2** : Toute relation parent-enfant entre classes doit être respectée.
- **H3** : Les limites de capacité des classes en termes d'étudiants inscrits ne doivent pas être dépassées.
- **H4** : Les salles ne peuvent pas être utilisées pendant les périodes prédéfinies comme "indisponibles".
- **H5** : Les salles ne peuvent accueillir plus d'une classe à la fois.
- **H6** : Si une salle est demandée pour une classe, cette salle doit provenir du domaine de cette classe. Toutes les salles de ce domaine sont nécessairement adaptées pour accueillir la classe concernée, à la fois en termes de commodités implicites et de capacité déclarée.
- **H7** : Les classes doivent être attribuées à un horaire de leur domaine respectif.

Notez que H4 et H5 correspondent directement aux contraintes H4 et H2 de l'ITC2007 respectivement. H3 est une variation plus stricte de l'idée de capacité étudiante vue précédemment dans la contrainte S1 de l'ITC2007. Les autres sont des contraintes prescriptives relatives à la pertinence des salles et des horaires, aux précédences et au sectionnement des étudiants — des aspects qui étaient soit non définis, non explicites ou absents dans l'ITC2007.

Les contraintes souples fixes dans l'ITC2019 sont :

- **S1 : Conflits étudiants**. Un conflit se produit lorsqu'une paire de classes auxquelles un étudiant est affecté a un quelconque chevauchement horaire. Cela compte indépendamment de la durée totale et/ou si les chevauchements sont contigus ou non.
- **S2 : Pénalité de temps**. Des valeurs de pénalité de zéro ou plus sont associées à chaque horaire disponible dans le domaine horaire d'une classe.
- **S3 : Pénalité de salle**. Une pénalité équivalente est associée à chaque salle dans le domaine des salles d'une classe.

Dix-neuf autres contraintes flexibles (F1 - F19), appelées les "distributions" mentionnées ci-dessus, peuvent être traitées comme strictes ou souples, ou totalement omises. Sans reproduire les définitions complètes ici, celles-ci sont : SameStart, SameTime, DifferentTime, SameDays, DifferentDays, SameWeeks, DifferentWeeks, Overlap, NotOverlap, SameRoom, DifferentRoom, SameAttendees, Precedence, WorkDay(S), MinGap(G), MaxDays(D), MaxDayLoad(S), MaxBreaks(R, S) et MaxBlock(M, S). Sémantiquement, il y a des dispositions pour des exigences à la fois affirmatives (par exemple, certaines classes doivent commencer en même temps) et prohibitives (par exemple, certaines classes ne doivent pas commencer en même temps) autour d'entités telles que les jours, les semaines et les salles. Il est également possible d'imposer une relation de précédence entre l'horaire d'une première classe et des classes ultérieures. Enfin, le comportement de certaines contraintes dépend des arguments. Par exemple, le nombre de pauses entre les classes dépassant S créneaux horaires peut être limité par MaxBreaks à un maximum de R par jour. Le coût d'une solution est une somme pondérée des sommes des violations individuelles pour les étudiants (S1), le temps (S2), la salle (S3) plus toutes les "distributions" (F1-F19) qui ont été appliquées en tant que souples. Les quatre pondérations appliquées à ces comptes de violations sont spécifiques au problème et fournies dans l'attribut XML "optimization".


### 2.2.6.1 Interrogation des problèmes

Quelques premiers aperçus du modèle ITC2019 peuvent être obtenus en interrogeant les données du problème de la manière suivante : après le pré-traitement, les instances ont été lues dans un tableau structuré. Les variables "jours" et "semaines" ont été encodées sous forme de chaînes de bits, tandis que toutes les autres valeurs numériques ont été stockées sous forme d'entiers non signés de 16 bits. Les identifiants natifs des entités telles que les salles, les cours, les configurations, les sous-parties, les classes et les étudiants, qui peuvent être des chaînes de caractères ou des valeurs numériques, ont été mappés à des identifiants entiers séquentiels. Les comptes de variables pouvaient alors être extraits ainsi qu'un graphe orienté des relations parent-enfant des classes. La Figure 2.3 illustre les relations parent-enfant pour un exemple de problème agh-ggis-spr17.

Une forme simple de recherche aléatoire pure peut être exécutée dans l'espace des solutions qui, par la nature de son algorithme d'échantillonnage, applique automatiquement le sous-ensemble {H1, H6, H7} des contraintes strictes. Cet espace d'échantillonnage peut être encore restreint en employant une approche plus sophistiquée pour le sectionnement des étudiants, informée par le graphe orienté. L'algorithme 1 donne le pseudo-code. Par le biais de l'échantillonnage par rejet, l'algorithme prend en compte les relations parent-enfant ainsi que la sélection d'une seule classe par sous-partie d'une configuration choisie aléatoirement. Cela garantit que H2 est également respectée, et la distance à la faisabilité est par conséquent réduite.

Un ensemble de 500 solutions a été échantillonné en utilisant cette méthode, pour l'instance muni-fspsx-fal17. Les solutions ont été réécrites dans des documents XML avant d'être converties au format reconnu en utilisant des feuilles de style XSL. Celles-ci ont été évaluées en faisant des appels API programmatiques au validateur du site web ITC. Le code intégré est actuellement protégé et nous ne sommes pas au courant d'une fonction d'évaluation open source pour ce problème. Les histogrammes des quatre valeurs d'objectif agrégées, ainsi que la somme totale pondérée des coûts, sont montrés dans la Figure 2.4. Toutes les solutions échantillonnées ont une distance positive à la faisabilité. Pour contextualiser, la ligne rouge verticale montre les valeurs individuelles qui composent la meilleure solution faisable connue (à objectif unique).

Quelques intuitions très basiques sur les compromis possibles entre différentes contraintes peuvent être tirées des positions des lignes rouges. Les valeurs d'échantillon pour les conflits étudiants dans la Figure 2.4(b) sont toutes pires que le meilleur résultat connu, tandis que celles pour la pénalité de salle dans la Figure 2.4(d) sont au contraire meilleures. Développer des techniques robustes pour l'échantillonnage des solutions, aidées par une compréhension des contraintes strictes respectées et des biais potentiels présents, constitue une partie intégrante du travail technique des chapitres suivants.

### 2.2.7 Autres instances réelles

Il est évident, d'après ce qui précède, que l'ITC2019 a représenté un grand pas en avant pour combler le fossé entre la théorie de la planification et la pratique. Bien que certains aspects des problèmes réels restent non pris en compte dans ce modèle, ceux-ci sont pour la plupart ésotériques. Le moins ésotérique est peut-être le concept de priorité ou de préférence de la part des étudiants et/ou des enseignants. Certaines institutions réelles permettent aux étudiants de faire des réservations, leur donnant ainsi la priorité pour être affectés à une classe particulière. La première version du projet UniTime (UniTime, 2023) a été conçue autour des ensembles de données de l'automne (fall) et du printemps 2007 de l'Université Purdue, avec des données réparties par département. Un exemple de préférences exprimées dans UniTime peut être celles d'un enseignant, pour qui certains créneaux horaires, bâtiments ou salles sont plus attrayants que d'autres. Sept niveaux nominaux de préférence étaient disponibles : Obligatoire, fortement préféré, préféré, neutre, déconseillé, fortement déconseillé, interdit. Cela a ouvert un continuum plus large à travers les types de contraintes, allant des contraintes strictes (interdites) aux contraintes souples (le reste). Dans l'ensemble de données original du problème de planification de Purdue, il y avait environ 750 classes, 29 000 étudiants et 41 grandes salles de conférence (Vermirovsky, 2003). Toutes les données sont disponibles sous forme anonyme sur le site web d'UniTime.

D'autres utilisations précoces des problèmes de planification réels peuvent être trouvées dans Abdullah et al., 2007, Avella et Vasil’ev, 2005, Daskalaki et al., 2004, Dimopoulou et Miliotis, 2004 et Santiago-Mozos et al., 2005. Plus récemment, Maya et al., 2016 ont utilisé trois ensembles de données de différentes universités mexicaines à Zitacuaro, Valle de Morelia et Tuxtla Gutierrez, tandis que Babaei et al., 2019 ont utilisé un algorithme hybride flou et de clustering pour satisfaire les demandes multidépartementales de l'Université islamique Azad de la branche d'Ahar. La pollinisation croisée entre le réel et l'artificiel est également possible. Un grand ensemble de données réelles du Collège des Arts et des Sciences, Universiti Utara Malaysia, connu sous le nom de UUMCAS A131 (247 cours, 850 conférences, 32 salles, 350 enseignants et 20 000 étudiants) avait une telle synergie avec l'ITC2007 qu'il a été proposé comme problème de test sous cette formulation (Wahid et Hussin, 2017).

Dans cette section, un aperçu des différents modèles UCTP utilisés par les chercheurs a été présenté. Au fil du temps, une diversité d'approches a été explorée pour trouver des solutions optimales ou satisfaisantes à ces problèmes, qu'ils soient artificiels ou réels. Dans la section suivante, un examen plus complet de certaines des techniques les plus importantes sera proposé.


### 2.3 Approches pour résoudre le UCTP

En raison de la grande taille et de la complexité des instances réelles du UCTP, de nombreuses techniques d'optimisation traditionnelles sont impraticables (Garg, 2009). Un algorithme exact, garantissant l'optimalité, s'est révélé insaisissable pour tous sauf pour des cas artificiels et irréalistes de petite taille (Schaerf, 1999). Une grande partie des recherches s'est donc concentrée sur les techniques heuristiques et d'approximation, ainsi que sur l'hybridation avec des méthodes traditionnelles. L'idée fondamentale est de trouver des solutions à la fois suffisamment bonnes en pratique et réalisables avec des ressources de calcul limitées. En plus de peser ces compromis inévitables, les chercheurs se sont également penchés sur certaines considérations philosophiques intéressantes, par exemple comment gérer les différents types de contraintes. Eiselt et Laporte, 1987 ont proposé de séparer les contraintes strictes et souples afin de résoudre d'abord les premières, puis les secondes. Par la suite, les approches métaheuristiques en deux phases ont gagné en popularité et continuent de montrer des résultats prometteurs pour le UCTP (Rossi-Doria et al., 2003, Lewis et Paechter, 2005). Les métaheuristiques en une phase, comme leur nom l'indique, cherchent à résoudre les deux types de contraintes simultanément. Un autre choix de conception concerne les régions non faisables de l'espace de solution. Hertz, 1991 a délibérément inclus ces régions afin de maintenir la connectivité dans tout l'espace de recherche, tandis que d'autres ont préféré restreindre la recherche à l'espace des solutions faisables uniquement. Parmi les enquêtes complètes importantes référencées pour documenter les approches précédentes, on trouve Schaerf, 1999, Lewis, 2008b, Babaei et al., 2014, Pandey et Sharma, 2016, Chen et al., 2021 et Ceschia et al., 2023. Des enquêtes plus ciblées comme Pillay, 2016 et Ilyas et Iqbal, 2015 sont référencées respectivement pour les méthodes hyper-heuristiques et hybrides. De manière générale, la même taxonomie est utilisée que dans Chen et al., 2021, avec quelques réarrangements et expansions. Tout en reconnaissant que les catégories ne sont pas mutuellement exclusives, un aperçu non exhaustif est présenté sous les rubriques suivantes : Techniques de recherche opérationnelle (Section 2.3.1), métaheuristiques basées sur une solution unique (Section 2.3.2), métaheuristiques basées sur la population (Section 2.3.3), systèmes multi-agents (Section 2.3.4), nouvelles méthodes intelligentes (Section 2.3.5) et approches multi/plusieurs objectifs (Section 2.3.6). La présentation de ces thèmes correspond approximativement à une chronologie d'utilisation populaire, de sorte que le lecteur puisse retracer le développement des idées à travers ce chapitre.


### 2.3.1 Techniques de recherche opérationnelle

Les techniques de recherche opérationnelle ont une longue histoire d'utilisation pour la planification et d'autres problèmes de gestion des horaires. Elles ont l'avantage d'être relativement intuitives à modéliser et à mettre en œuvre. Cependant, en raison des problèmes d'échelle, elles sont souvent inefficaces pour le UCTP lorsqu'elles sont utilisées de manière strictement isolée.

### 2.3.1.1 Réduction à la coloration de graphes

Les premiers modèles de planification ont été formulés comme des problèmes de coloration de graphes, avec les cours comme nœuds, les conflits/contraintes comme arêtes, et les périodes comme couleurs. Dans cette représentation simplifiée, le nombre chromatique χ(G) indique le nombre minimum de périodes requises pour un emploi du temps réalisable pour les cours et les enseignants. La Figure 2.5 montre un exemple de solution à un problème de cinq cours avec des conflits par paires {1,2}, {2,3}, {3,4}, {4,5}, {5,1}.

Welsh et Powell, 1967, furent parmi les premiers auteurs à souligner les similitudes structurelles entre les modèles de graphes et la planification, ainsi qu'à proposer une borne supérieure améliorée pour χ(G) et un algorithme pour obtenir une coloration valide. Cependant, ce modèle précoce avait des limitations inhérentes, comme le fait de ne pas prendre en compte les capacités des salles et de ne pas permettre de contraintes supplémentaires réelles. La seconde de ces problématiques a été abordée par Neufeld et Tartar, 1974, qui ont introduit la possibilité de certaines conditions préalables pour l'affectation des cours. En imposant des restrictions sur la coloration de certains nœuds, des contraintes d'indisponibilité pouvaient être prises en compte. Parallèlement, les réunions préassignées étaient imposées en limitant certains nœuds à une seule couleur. Les auteurs ont proposé une preuve mathématique formelle que l'existence d'une coloration en |P| couleurs (où |P| est le nombre de périodes discrètes) est une condition nécessaire et suffisante pour l'existence d'une solution réalisable.

Plusieurs axes différents de recherche opérationnelle en planification ont ensuite été réunis par de Werra, 1985, qui a discuté des approches algorithmiques pour des colorations de graphes valides. Une méthode établie à cette époque était la saturation de degré (Brélaz, 1979), qui procède comme suit : initialement, le nœud avec le plus grand nombre de voisins est sélectionné et coloré. À chaque étape suivante, le nœud non coloré avec le plus grand nombre de couleurs dans son voisinage immédiat est sélectionné. La plus petite couleur (basée sur un ordre lexicographique) qui n'a pas été assignée à aucun de ses voisins est alors attribuée à ce nœud. Le processus est itéré jusqu'à sa complétion ou jusqu'à un blocage. Bien que la saturation de degré fournisse une méthode infaillible pour la coloration optimale des graphes bipartites, cycliques et en roue, elle est considérée comme une méthode heuristique lorsqu'elle est utilisée sur d'autres types de graphes plus susceptibles d'être rencontrés en planification.

Les graphes bipartites répondent bien à la méthode de saturation de degré en raison de leur structure, dans laquelle les arêtes ne connectent que des nœuds de différents ensembles disjoints, au nombre de deux. Ces propriétés pratiques ont été exploitées plus tard par Badoni et al., 2014, qui ont abordé un problème de planification scolaire non capacitaire avec une approche novatrice en deux phases. Dans la première phase, un multigraphe bipartite (dont les parties sont les conférences et les enseignants) a été utilisé pour dériver une matrice des besoins quotidiens à partir d'une matrice des besoins hebdomadaires donnée. Dans la deuxième phase, une coloration d'arêtes a été générée pour un second graphe bipartite (conférences et périodes) en utilisant l'heuristique du degré le plus élevé en premier. Ainsi, une connexion a été établie entre les trois entités (conférences, enseignants et périodes) de manière à ce que toutes les contraintes soient satisfaites. Il est à noter qu'une fois de plus, le modèle était simplifié et artificiel, sans cours, salles ou autres dépendances, et des instances de petite échelle ont été utilisées pour les tests.


### 2.3.1.3 Flux de réseau

La théorie des graphes est une fois de plus invoquée dans l'approche des flux de réseau. Dans ce paradigme, le UCTP est formulé comme un ou plusieurs réseaux de flux (Dyer et Mulvey, 1976, Mulvey, 1982, Chahal et Werra, 1989). Un graphe est généré dans lequel les arêtes ont des capacités de flux numériques, limitées à 0 ou 1. Dans Dinkel et al., 1989, les sommets étaient disposés à des niveaux intermédiaires entre une source et un puits, représentant respectivement les départements, les combinaisons enseignant/cours et salle/créneau horaire. Les arêtes étaient omises chaque fois que les co-affectations de variables particulières étaient impossibles, tandis que la propriété unimodulaire garantissait l'intégralité des solutions. Bien que le problème de flux maximum résultant puisse être résolu en temps polynomial, les auteurs ont noté que leur approche nécessitait occasionnellement une intervention humaine. Cela est dû au fait que des contraintes telles que H5 — l'interdiction d'affecter un enseignant unique à plusieurs conférences simultanément — ne pouvaient pas être automatiquement garanties.

Ces lacunes ont été davantage mises en évidence dans le travail plus récent de Kampke et al., 2019, qui a modélisé le problème de la piste 3 de l'ITC2007 en utilisant apparemment la même architecture de graphe que Dinkel et al., 1989. Les contraintes strictes H2 et H3 étaient garanties, tandis que les autres ne l'étaient pas. En raison de l'absence de retour en arrière, il était possible de se retrouver dans une situation où aucun emplacement faisable ne restait pour les conférences non affectées. Cela induisait un scénario sans issue dans lequel les seules options étaient de renvoyer un emploi du temps incomplet (violant H1) ou d'introduire des conflits (violant H4 ou H5).

Les contraintes souples n'ont pas été considérées explicitement, mais des modifications apportées à l'algorithme de résolution de base ont indirectement aidé S2 (stabilité des salles) et S3 (jours de travail minimum). Une solution était représentée par un flux valide à travers le réseau, trouvé en utilisant la méthode de Ford-Fulkerson avec recherche en largeur (Ford et Fulkerson, 1962). Les améliorations ajoutées dans Kampke et al., 2019 étaient doubles :
1. Pour chaque cours \(C \in C\), les nœuds dans la couche salle/période étaient visités par ordre croissant de capacité des salles, en commençant par la plus petite salle dont la capacité dépassait \(stud(C)\). Lorsque ces salles étaient épuisées, la recherche se poursuivait avec les salles de capacité insuffisante, cette fois par ordre décroissant.
2. La vérification de la faisabilité était mise en œuvre de manière à éviter les placements non faisables et à toujours préférer la meilleure alternative.

La première de ces modifications est une heuristique gloutonne de la même famille que celles de la section 2.3.1.2, démontrant à nouveau comment des règles apparemment simples peuvent aider à atteindre certains objectifs. Les auteurs ont déclaré que les deux contraintes servies par cette heuristique étaient presque jamais violées. Cependant, obtenir des solutions complètes ou faisables s'est avéré plus difficile, l'approche globale échouant systématiquement sur 12 des 21 instances. Dans 8 des autres (comps 01, 04, 06, 08, 09, 13, 14 et 18), des solutions faisables n'ont été trouvées que dans une fraction des essais (55%, 22%, 3%, 3%, 3%, 37%, 7% et 84% respectivement), tandis que comp11 était le seul problème à atteindre 100% sur cette métrique. Ce n'est que grâce à des étapes supplémentaires que les résultats initiaux ont pu être transformés en emplois du temps de haute qualité. Les solutions partielles de la phase de flux de réseau ont été complétées par un algorithme constructif ISCB−CTT (Kampke et al., 2015), avant qu'une optimisation supplémentaire n'ait lieu via un processus de recuit simulé utilisant une procédure de recherche adaptative gloutonne randomisée (GRASP).

Les limitations du flux de réseau en tant qu'approche de modélisation autonome sont évidentes. Un domaine connexe, offrant un environnement de modélisation plus riche pour surmonter ces limitations, est la programmation mathématique. Dans la section suivante, certaines approches importantes basées sur les entiers et les entiers mixtes sont relayées.



### 2.3.1.4 Programmation mathématique

La programmation mathématique a été utilisée pour aborder la planification en tant que problème d'affectation (ou une collection de sous-problèmes), souvent en utilisant des variables binaires pour une représentation directe. L'un des premiers exemples est Lawrie, 1969, qui a utilisé une procédure de branchement et de bornage avec des coupes de Gomory pour trouver un emploi du temps faisable pour un problème de lycée. D'autres premiers partisans incluent : Breslaw, 1976, Shin et Sullivan, 1977, McClure et Wells, 1984, Ferland et Roy, 1985, et Tripathy, 1992. Tripathy, 1984 et Carter, 1989 ont employé la relaxation lagrangienne pour résoudre des instances de problèmes de planification de cours avec jusqu'à 287 conférences. Bien que cela puisse être comparable en taille au benchmark ITC2007, ces approches ne considéraient que les conflits simples du type décrit dans la Figure 2.5. Les effets interactifs plus complexes étaient négligés car ils auraient augmenté à la fois la difficulté de modélisation et le temps d'exécution du solveur. En effet, dans Carter, 1989, la variable d'affectation était indexée uniquement par cours et salle.

Burke et al., 2010 sont allés plus loin en tentant d'inclure toutes les contraintes strictes et souples du benchmark de la piste 3 de l'ITC2007 dans un modèle de programmation linéaire en nombres entiers (ILP), nommé Monolithic. Cinq ensembles de variables de décision ont été définis comme suit :
- \(xC_{pr} \in B\). Les conférences du cours \(C\) devraient être placées en période \(p\), salle \(r\) si et seulement si cette variable est fixée à 1.
- \(vC_d \in B\). Au moins une conférence du cours \(C\) est programmée le jour \(d\) si et seulement si cette variable est fixée à 1.
- \(z_{pu} \in B\). Une conférence dans le curriculum \(u \in U\) est « isolée » en période \(p\) si et seulement si cette variable est fixée à 1.
- \(yC_r \in B\). Une conférence du cours \(C\) est programmée dans la salle \(r\) si et seulement si cette variable est fixée à 1.
- \(iC \in Z\). Cette valeur (bornée par zéro et le nombre de jours par semaine) est le nombre de jours où les conférences du cours \(C\) sont en dessous du nombre minimum recommandé de jours de travail.

Comme les propriétés des instances dans ce benchmark sont définies par cours plutôt que par conférence, l'interprétation théorique du graphe du problème est devenue supernodale. C'est-à-dire que chaque nœud de cours (semblable à ceux de la Figure 2.5) est devenu une clique de nœuds représentant ses conférences constitutives, et les arêtes entre les cliques se sont multipliées en conséquence. Les variables de décision binaires principales, \(xC_{pr}\), restaient donc indexées par cours plutôt que par conférence, tandis que les valeurs des autres variables (dépendantes) étaient déduites lors du processus de résolution. Deux équations ont été utilisées pour faire respecter H1 (toutes les conférences doivent être affectées) et H4 (aucune période indisponible ne peut être utilisée) tandis que les autres contraintes strictes étaient garanties par un ensemble d'inégalités. Les contraintes souples, formulées sous forme de six inégalités supplémentaires, présentaient un aspect plus difficile dans leur conception. La contrainte S3 (compacité du curriculum), par exemple, était exprimée par :

\[ \sum_{C \in u} \sum_{r \in R} (xC_{pr} - xC_{p-1,r} - xC_{p+1,r}) \leq z_{pu} \quad u \in U, p \in P \]

Avec la complication supplémentaire que si \(p\) coïncidait avec le premier (ou dernier) créneau horaire d'un jour, \(p-1\) (ou \(p+1\) respectivement) cessait d'exister aux fins de cette inégalité et devait être considéré comme zéro. Cela découle de la définition de S3.

La fonction objectif, qui renvoyait le coût de la solution à minimiser, était tout aussi lourde :

\[ \sum_{i=1}^{n} w_i \cdot s_i \]

où \(w\) représente les poids de pénalité pour chaque contrainte souple. Cet ILP a été résolu en utilisant ILOG CPLEX 11 Dual Simplex LP Solver, et trouve théoriquement un emploi du temps optimal, donné suffisamment de temps. Bettinelli et al., 2015 ont remarqué que l'ILP pouvait tourner pendant des jours sans résultat, et Monolithic n'était donc adapté qu'à des problèmes de taille modeste ou triviale. Seulement trois problèmes (comp01, comp05 et comp11) des 14 publiés à l'époque ont été résolus en 40 unités CPU, où 1 unité CPU ≈ 780 secondes. Monolithic s'est néanmoins avéré utile pour identifier des bornes inférieures pour le benchmark et a inspiré plusieurs développements. En excluant certaines violations en fixant leurs poids à zéro, deux variantes dérivées par les mêmes auteurs (Burke et al., 2010) ont été capables d'obtenir de meilleures bornes inférieures sur la majorité des instances, avec un ordre de grandeur d'accélération. D'autres simplifications proposées incluaient l'agrégation des salles équivalentes afin de réduire la quantité de variables qui, comme le souligne Cacchiani et al., 2013, pouvait être exponentielle en nombre. Intégrer l'ILP dans un cadre heuristique afin de localiser une zone de solutions prometteuses avant de « plonger » était une autre amélioration suggérée par Burke et al., 2010. Dans des travaux ultérieurs (Burke et al., 2012), des coupes ont été suggérées pour restreindre les bornes de recherche — l'exemple le plus simple étant le resserrement de la borne supérieure de \(iC \in Z\) à \(mwd(C)\).

Dans Cacchiani et al., 2013, qui ont également examiné le benchmark de la piste 3 de l'ITC2007, une stratégie a été proposée dans laquelle le problème principal était partitionné en sous-problèmes plus gérables. Le fait que les contraintes S1 et S4 se rapportent à l'affectation des conférences aux salles, tandis que S3 et S2 se rapportent au temps, suggérait une décomposition naturelle. Les auteurs ont constaté qu'un ILP entièrement descriptif donnait des résultats compétitifs lorsque les sous-problèmes étaient petits. Dans les cas plus grands, une relaxation linéaire a été imposée et une procédure de génération de colonnes a été utilisée pour obtenir des résultats de qualité similaire.

Il est à noter que les bornes inférieures ont été améliorées pour de nombreuses instances, tandis que certaines bornes supérieures connues ont été mathématiquement prouvées comme étant optimales. La séparation des entités de contrainte en lignes spatiales et temporelles a fait écho à l'approche de Lach et Lübbecke, 2012.

Une forme différente de partitionnement des problèmes a également été exploitée comme première étape dans l'approche diviser-pour-régner de Hao et Benlic, 2011. Cette décomposition était basée sur la minimisation du nombre de contraintes S3 relâchées reliant des paires de cours dans des parties distinctes. Elle a été créée en utilisant une recherche taboue et raffinée par une phase de perturbation, les deux étant appelées cycliquement jusqu'à ce qu'un critère d'arrêt soit atteint. Dans les étapes suivantes, des bornes inférieures ont été obtenues sur les sous-problèmes par des solveurs ILP génériques, avant d'être additionnées pour donner une solution au problème principal.

Cette fusion efficace de techniques mathématiques et métaheuristiques a également prouvé son succès dans Lindahl et al., 2018. Dans la matheuristique proposée par les auteurs, appelée approche fixer-et-optimiser, un solveur de programme en nombres entiers mixtes explorait un grand voisinage dans lequel un sous-ensemble des variables était fixé. Cela s'inspirait de la méthode du « corridor » (Sniedovich et Voß, 2006) de résolution de sous-problèmes plus petits par des méthodes exactes. La technique détient actuellement le meilleur résultat connu pour une instance du benchmark de la piste 3 de l'ITC 2007 et était très compétitive sur le reste.

En général, bien que les programmes en nombres entiers ou en nombres entiers mixtes autonomes puissent décrire exactement un problème, en pratique, ils sont les plus utiles pour prouver les bornes sur l'optimum. La programmation par satisfaction de contraintes, discutée dans la section suivante, ne se concentre pas sur une fonction objectif explicite, mais plutôt sur la recherche d'affectations cohérentes qui répondent aux contraintes spécifiées du problème.


### 2.3.1.5 Programmation par satisfaction de contraintes

Dans une formulation de programmation par satisfaction de contraintes (CSP), des conditions sont imposées sur les variables, contraignant ainsi leurs valeurs à un domaine faisable fini. Une affectation de valeurs à toutes les variables représente une solution dans laquelle chaque contrainte est satisfaite. Yoshikawa et al., 1996 ont décrit un solveur de problème de relaxation de contraintes (COASTOOL) appliqué à un problème de planification des horaires dans un lycée, tandis que Deris et al., 1999 ont utilisé le CSP en combinaison avec un algorithme génétique pour aborder le UCTP, avant de poursuivre leurs recherches dans Deris et al., 2000. D'autres contributions notables incluent Zhang et Lau, 2005. L'un des exemples les plus marquants se trouve dans le logiciel UniTime mentionné dans la section 2.2.7. Il s'agit d'un solveur open source pour les UCTP. Conçu en 2001 à l'Université Purdue, sa première phase permet la modélisation d'une instance de problème par des primitives de programmation par contraintes (contraintes, variables et valeurs). Fonctionnant avec un algorithme de recherche itérative en avant, UniTime se distingue des méthodes de recherche locale traditionnelles en incluant des solutions incomplètes (partiellement assignées mais faisables en interne) dans son espace de recherche. Les avantages évidents de cette approche sont les suivants :

1. Une recherche locale guidée par des heuristiques incluant des solutions partielles est généralement plus efficace, en termes de temps de réponse, qu'une recherche systématique ne permettant que des solutions entièrement formées.
2. Le système peut s'arrêter, démarrer ou continuer à partir de n'importe quelle solution faisable donnée, quel que soit son niveau d'incomplétude.
3. Un emploi du temps autrement faisable avec des conférences manquantes est plus significatif et interprétable qu'un emploi du temps entièrement assigné avec de multiples violations de contraintes strictes.
4. Le retour en arrière intégré signifie que le système ne souffre pas du problème dit des « erreurs précoces ». Toute décision suspectée de conduire à une impasse dans une solution partielle peut être annulée.

Dans cette phase constructive, le cyclage est évité grâce à l'utilisation de statistiques basées sur les conflits (CBS) (Müller et al., 2004). CBS est une structure de données qui enregistre les conflits précédemment rencontrés entre les variables d'affectation, ainsi que leurs fréquences. Conceptuellement similaire à la liste taboue (discutée dans une section ultérieure), CBS aide à orienter la recherche loin des régions potentiellement néfastes. Contrairement à l'approche mentionnée de Kampke et al., 2019, UniTime attend qu'une solution complète soit trouvée avant d'entrer dans sa deuxième phase. Cependant, il s'agit d'une distinction mineure. La similitude plus importante est l'émergence d'un système en plusieurs phases dans lequel la construction basée sur des règles précède l'optimisation. Pour UniTime, l'optimisation est réalisée à travers une chaîne récursive de métaheuristiques : l'ascension de colline, le grand déluge et le recuit simulé.

UniTime a été finaliste dans les trois pistes de l'ITC2007 et a remporté deux d'entre elles. Bien que les détails algorithmiques fins tels que les voisinages et les paramètres aient été redéfinis pour chaque domaine de problème, les principes sous-jacents se sont révélés encourageants et robustes.



### 2.3.1.6 Programmation logique

Alors que le CSP consiste à trouver des solutions qui satisfont un ensemble de contraintes, la programmation logique fournit un cadre pour exprimer et résoudre ces problèmes à travers des relations logiques, des prédicats et l'utilisation de mécanismes d'inférence puissants. Un cadre bien connu est la programmation par ensemble de réponses (ASP) — une approche basée sur un paradigme logique déclaratif (Marek et Truszczynski, 1999, Niemelä, 1999). Ce n'est que ces dernières années que l'ASP a été appliquée au UCTP.

Un programme logique ASP simple, \(P\), est composé de règles, de faits et de contraintes. Les règles sont de la forme mathématique :

\[ \text{Head} \leftarrow \text{Body} \]

Où les lettres symbolisent des atomes de logique classique du premier ordre, qui peuvent être des prédicats sur une ou plusieurs variables. Les faits et les contraintes sont définis par des expressions qui sont vides à droite ou à gauche du signe d'implication, respectivement. Un flux de travail typique en ASP se compose de trois étapes :
1. **Modélisation**. Le problème est formalisé et déclaré pour le parseur.
2. **Grounding**. Les variables sont éliminées.
3. **Résolution**. Les "modèles stables" ou "ensembles de réponses" éponymes sont générés. Les programmes peuvent avoir un nombre quelconque d'ensembles de réponses, avec le cas de zéro impliquant aucune solution faisable.

De manière similaire à la Figure 2.5, la Figure 2.6 montre un exemple simplifié de planification basé sur le graphe de Petersen, qui comporte 10 nœuds et 15 contraintes par paires. Pour trouver une coloration en 3 sommets — dont une solution est montrée dans la Figure 2.7 — le programme de la Liste 2.1 pourrait être utilisé. Les lignes 1 à 7 définissent le graphe par le biais de prédicats de nœuds et d'arêtes, tandis que la ligne 9 tient compte des trois couleurs. La ligne 11 spécifie qu'exactement une couleur doit être appliquée à chaque nœud et la ligne 13 est une contrainte stipulant que les nœuds adjacents doivent avoir des couleurs différentes.

```plaintext
1 node (1..10) .
2
3 edge (1 ,2) . edge (1 ,6) . edge (1 ,5) . edge (2 ,1) . edge (2 ,3) . edge (2 ,7) .
4 edge (3 ,2) . edge (3 ,4) . edge (3 ,8) . edge (4 ,3) . edge (4 ,5) . edge (4 ,9) .
5 edge (5 ,1) . edge (5 ,4) . edge (5 ,10) . edge (6 ,1) . edge (6 ,8) . edge (6 ,9) .
6 edge (7 ,2) . edge (7 ,9) . edge (7 ,10) . edge (8 ,3) . edge (8 ,6) . edge (8 ,10) .
7 edge (9 ,4) . edge (9 ,6) . edge (9 ,7) . edge (10 ,5) . edge (10 ,7) . edge (10 ,8) .
8
9 col ( r ) . col ( b ) . col ( g )
10
11 1 { colour (X , C ) : col ( C ) } 1 : - node ( X ) .
12
13 : - edge (X , Y ) , colour (X , C ) , colour (Y , C ) .
```

Liste 2.1 : Un problème de planification réduit exprimé en programmation par ensemble de réponses.

Le pouvoir expressif de l'ASP a grandi avec le support de constructions sémantiques supplémentaires, dont beaucoup sont essentielles pour modéliser le UCTP. Celles-ci incluent les littéraux conditionnels, les contraintes de cardinalité, les agrégats, les règles de choix, les poids et les opérateurs arithmétiques, ainsi que des blancs tels que dans le prédicat `penalty( , ,P)` qui sont utilisés pour sommer les valeurs des deux premiers éléments du tuple. Le solveur Clingo propose des commandes d'optimisation #maximise et #minimise, qui pourraient être employées avec l'exemple ci-dessus comme suit : `#minimise[penalty( , ,P) = P]` pour trouver une solution UCTP avec le coût de violation des contraintes souples le plus bas.

Banbara et al., 2013 décrivent un programme ASP testé contre 57 instances de problèmes dans la formulation de la piste 3 de l'ITC2007, en utilisant chacune des configurations UD1-UD5. Sur les 57 × 5 = 286 problèmes des ensembles nommés dans la Section 2.2.4, les bornes les plus connues précédemment ont été égalées ou améliorées pour 175 problèmes, tandis que l'optimalité a été prouvée pour 46.

Certains avantages de la programmation logique sont évidents. L'ASP permet une formulation compacte et lisible par l'homme. Il y a une grande "tolérance à l'élaboration" en raison de la manière dont les règles, les faits et les contraintes sont déclarés indépendamment, ce qui signifie que l'activation, la désactivation ou le changement des contraintes UCTP est facile. Le développement rapide et continu des grounders, des solveurs et des hybrides monolithiques a également montré un potentiel d'extensibilité, comme dans l'optimiseur multi-objectif à niveaux de priorité de Clingo (Banbara et al., 2019). De plus, la programmation logique supprime le besoin de réglage des paramètres requis par certaines métaheuristiques. Cependant, il reste des choix de conception à faire. Clingo offre aux utilisateurs un choix de stratégies de recherche, telles que le retour en arrière (Ward et Schlipf, 2010) et la conversion en un problème de satisfiabilité booléenne (SAT). Différentes configurations de ces stratégies ont été testées dans Banbara et al., 2013 avec des résultats mitigés.

Peut-être que le plus gros problème concerne les délais d'expiration résultant de programmes de grounding excessivement grands. Dans Banbara et al., 2013, l'instance de problème EA03 (composée de 145 cours, 65 salles dans 9 bâtiments, 65 curricula, 3 207 contraintes d'indisponibilité et 1 350 contraintes de salle) était insoluble sous la formulation UD5, en raison d'une explosion combinatoire des clauses. Une contrainte souple particulière, TravelDistance, a fait exploser la taille du programme de grounding à 7,9 Go (contre 70 Mo sans TravelDistance), tandis qu'EA07 a causé des problèmes similaires. De même, dans Banbara et al., 2019, UUMCAS A131 sur UD5 a dépassé la limite de mémoire disponible de 20 Go et une grande instance de erlangen* n'a pas pu être grounded en une journée. Ces problèmes parlent de l'impraticabilité des approches exactes pures pour les problèmes UCTP de grande taille.

Les deux sections suivantes déplacent la discussion vers le domaine des métaheuristiques, en commençant par les stratégies basées sur une solution unique.



### 2.3.2 Métaheuristiques basées sur une solution unique

Les métaheuristiques basées sur une solution unique sont des approches d'approximation dans lesquelles une solution unique (un emploi du temps, dans ce contexte) est itérativement affinée par des opérateurs heuristiques de bas niveau jusqu'à ce que certains critères d'arrêt soient atteints (Bianchi et al., 2009). Grâce à la conception et à la paramétrisation informées à la fois des opérateurs et des critères d'acceptation, un chemin est navigué à travers l'espace de recherche et l'algorithme converge vers des solutions de haute qualité, sinon optimales. Une caractéristique clé de nombreuses métaheuristiques est leur élément stochastique, qui permet d'accéder à des régions disparates de l'espace de recherche par exploration aléatoire (ou partiellement aléatoire). Les régions du paysage susceptibles de contenir des solutions prometteuses peuvent alors être exploitées davantage.

### 2.3.2.1 Recherche locale

La recherche locale est basée sur l'exploration des solutions voisines définies par une structure de voisinage. La forme la plus simple est la recherche aléatoire. Étant donné un emploi du temps de départ, une nouvelle solution est échantillonnée à partir de son voisinage, évaluée et acceptée uniquement si elle réduit le coût de la solution. La montée de colline est similaire, sauf que le voisinage est évalué de manière exhaustive et la solution la plus améliorante est choisie. Un inconvénient connu de la recherche locale est sa tendance à rester bloquée dans des optima locaux, où de meilleures solutions existent ailleurs mais ne sont pas accessibles par le voisinage immédiat. La recherche locale est donc plus couramment utilisée en conjonction avec d'autres techniques pour affiner les solutions. Des exemples incluent Joudaki et al., 2011, qui ont incorporé la recherche locale sous la forme d'un algorithme mémétique, et Yang et Jat, 2011 et Shahvali et al., 2011, qui ont tous deux proposé des algorithmes génétiques améliorés avec des capacités de recherche locale.
















### 2.4 État de l'art

Les algorithmes développés dans cette thèse sont basés sur la formulation de la piste 3 de l'ITC2007. C'est pourquoi une attention particulière est portée à ces résultats de référence lorsqu'on considère l'évolution de l'état de l'art. La majorité des auteurs ont suivi une approche à objectif unique, comme le prescrivent les règles de la compétition. Les cinq finalistes originaux étaient Müller, 2009, Lu et al., Atsuta et al., 2008, Geiger, 2008 et Clark et al., 2008, parmi lesquels le solveur multi-phase basé sur des contraintes de Müller, 2009 a été déclaré vainqueur.

La section "analyse de référence" dans Gozali et Fujimura, 2020 présente une liste incomplète, mais plus récente, des résultats de l'ITC2007, tandis que Ceschia et al., 2023 inclut les résultats les plus récents de l'état de l'art. La majorité des meilleurs résultats globaux connus sous le délai d'exécution original sont partagés entre Abdullah et Turabieh, 2012 et Kiefer et al., 2017 (dont les méthodes sont décrites dans la section 2.3.5.1). Lindahl et al., 2018 (discuté dans la section 2.3.1.4) conserve le meilleur résultat connu pour comp09.

Pour plus de commodité, toutes ces valeurs de coût sont reproduites dans le tableau 2.3, aux côtés des bornes inférieures et supérieures les plus serrées actuellement connues sur les optima. Les bornes ont été fournies par divers auteurs utilisant soit des techniques mathématiques soit des méthodes de recherche avec de grands budgets d'itération.




### 2.5.1 Formulations

Une distinction a été établie entre les benchmarks artificiels de l'UCTP, largement utilisés, et les problèmes individuels qui peuvent avoir été étudiés par un seul chercheur. Étant donné que les problèmes réels peuvent être idiosyncratiques voire uniques, l'objectif de la standardisation est de présenter des problèmes stimulants qui préservent la généralité sans trop simplifier.

Ceschia et al., 2023 ont passé en revue six benchmarks de problèmes de planification éducative. Les auteurs ont noté avec intérêt que la chronologie correspondait à des niveaux croissants de complexité. Par exemple, l'ITC2019 récent a intégré des éléments des modèles post-inscription et basés sur le curriculum, en faisant un ensemble de données riche et personnalisable. Malgré cette tendance indéniable vers une complexité et une nuance accrues, les auteurs soutiennent que les benchmarks antérieurs n'ont pas encore épuisé leur utilité. De nombreuses instances de l'ITC2007 ont été résolues de manière optimale, mais ces problèmes restent un banc d'essai utile pour l'analyse des paysages, l'efficacité computationnelle et l'investigation théorique. Notamment, le volume de travaux antérieurs et de résultats publiés (qui dépasse actuellement celui de l'ITC2019) permet de faire des comparaisons robustes pour les nouveaux algorithmes. Un inconvénient potentiel est la sur-adaptation d'un algorithme à un benchmark particulier, limitant son utilité. Cependant, les concepts issus des approches réussies (tels que le style de métaheuristique ou le profil d'un calendrier de refroidissement) peuvent fournir des informations précieuses sur la manière de traiter une gamme plus large de problèmes réels sur mesure, établissant ainsi un pont entre la théorie et la pratique.

Un autre avantage de la standardisation (et des compétitions internationales) est de fournir un point focal pour la communauté de recherche et d'encourager la collaboration et le croisement des idées. Certaines approches ont également franchi les frontières des domaines au sein de la taxonomie de la planification illustrée à la figure 1.1. Les algorithmes génétiques (GAs), appliqués au problème des examens par Colorni et al., 1992, sont maintenant populaires pour la planification des cours, par exemple.

Nous reconnaissons les problèmes persistants autour de la modélisation des contraintes et des mesures de qualité des emplois du temps. Une bonne illustration des conséquences (peut-être) involontaires des définitions strictes des contraintes est la distinction CurriculumCompactness vs. Windows de la section 2.2.4. Bien que les contraintes strictes soient généralement faciles à formaliser, les contraintes souples telles que celles-ci peuvent être plus complexes et difficiles à définir correctement. Dans certains cas, des dépendances ou d'autres relations inter-variables peuvent être nécessaires. Un aspect subjectif peut exister, ce qui est également vrai pour les mesures de qualité/coûts des emplois du temps. Classer l'importance relative ou attribuer des poids à différents objectifs n'est pas une science exacte. Les solveurs sans poids codés en dur peuvent offrir un certain soulagement ici, de même que l'approche multi-objectifs, qui ne repose pas du tout sur un objectif scalaire. Ce dernier représente un manque évident dans le domaine, car la grande majorité de la recherche sur l'UCTP a adopté une perspective à objectif unique (Ceschia et al., 2023). Une autre voie à explorer est l'inclusion de davantage d'objectifs « intangibles ». On peut trouver des exemples limités, tels que ceux de Mühlenthaler et Wanka, 2016 et Akkan et Gülcü, 2018, qui ont conçu des métriques pour l'« équité » et la « robustesse » respectivement, mais la plupart des chercheurs persistent avec les objectifs prescrits. Des objectifs holistiques supplémentaires comme ceux-ci se prêtent bien à un traitement bi- ou multi-objectifs, dans lequel une approximation du front de Pareto peut révéler les compromis existants.




### 2.5.2 Solveurs : Un résumé thématique

À travers les diverses approches appliquées au problème de l'UCTP, de nombreux contrastes thématiques sont évidents. La quête de l'optimalité — la solution exacte — est passée de mode, les techniques d'approximation étant désormais plus courantes. Bien que les métaheuristiques ne puissent pas garantir l'optimalité comme peut le faire un modèle mathématique pur, ce dernier s'est avéré impraticable pour naviguer dans les immenses espaces de recherche induits par des problèmes de taille départementale. Un compromis potentiel sur la qualité est donc considéré comme acceptable. Chen et al., 2021 ont remarqué que les techniques de recherche opérationnelle peuvent être efficaces pour générer des solutions faisables, mais restent encombrantes pour une optimisation ultérieure. Cet héritage se reflète dans la popularité des solveurs à phases multiples. La stratégie de séquencement peut varier, certains auteurs attribuant d'abord les périodes, puis les salles. D'autres abordent d'abord les contraintes strictes avant d'optimiser les contraintes souples — une méthode qui s'est avérée très efficace.

En se concentrant sur les métaheuristiques pour la planification universitaire, une méta-étude de Bashab et al., 2020 a donné un aperçu de la fréquence d'utilisation de divers types. Un échantillon de 131 articles publiés entre 2009-2020 (une période d'intérêt croissant dans le domaine) est présenté dans la figure 2.11. Les méthodes inspirées par la nature sont proéminentes, tandis que les algorithmes génétiques et les hybrides, à eux deux, représentent près de la moitié du total. En termes d'hybrides, nous avons discuté de la combinaison de méthodes exactes et inexactes, ainsi que du mélange de métaheuristiques qui se complètent. Une conclusion est qu'un hybride soigneusement conçu peut surpasser ses composants constitutifs, résultant en une recherche extraordinairement puissante.

La topologie de cet espace de recherche dépend en partie du schéma de codage. Cette revue s'est concentrée sur les codages de style direct, dont la popularité s'explique par plusieurs facteurs. Leur interprétation est très intuitive (par exemple, représenter un jour de la semaine par un entier de 0 à 4, ou de 1 à 5). Il est facile de "baker in" la satisfaction de certaines contraintes strictes en utilisant des plages de variables appropriées. De même, la conception des opérateurs génétiques et autres est grandement simplifiée. Pour ces raisons, un codage direct est adopté dans le travail technique qui suit. Il est toutefois reconnu que les codages indirects peuvent offrir une plus grande expressivité en ce qui concerne les contraintes complexes, agissant comme des "constructeurs" d'emplois du temps, à l'instar des ensembles de règles heuristiques constructives discutées précédemment.

En termes de gestion des contraintes, diverses perspectives ont émergé sur la question de l'application stricte contre la relaxation complète. Permettre des mouvements à travers l'espace non faisable, sous réserve de pénalités sévères, peut être un moyen utile de concilier les deux. Viser une application stricte peut être problématique en raison de la nature des opérateurs et du désir d'éviter des mécanismes de réparation coûteux. Alvarez-Valdes et al., 2002 ont souligné les difficultés à naviguer dans l'espace de recherche en utilisant des mouvements trop simples.



### Affectations

Les affectations qui peuvent être bonnes à certains égards pour un cours peuvent également être impossibles en raison des conflits induits entre les salles ou d'autres entités. L'ampleur et la portée des mouvements de perturbation sont tout aussi importantes. Le succès des algorithmes adaptatifs capables d'utiliser une variété de structures de voisinage suggère que les mouvements grands et petits ont chacun un rôle à jouer. Le travail de Yusoff et Roslan, 2019, indique également qu'un mutateur guidé peut aider à la convergence mieux qu'une mutation purement aléatoire. Cependant, il existe un danger à être trop prescriptif et à étouffer l'exploration. Trouver le bon équilibre nécessite également de définir judicieusement les budgets et autres paramètres pertinents.

### Sensibilité aux paramètres

Les optimiseurs peuvent être sensibles aux valeurs des paramètres individuels ou à leurs combinaisons. Les systèmes avec moins de paramètres à ajuster ou ceux qui disposent d'un réglage adaptatif des hyper-paramètres sont donc naturellement attrayants. Une tendance vers l'agnosticisme des problèmes dans la conception des solveurs est également évidente. Les instances du benchmark ITC2007 varient de grandes et fortement contraintes (comp05) à modérément faciles à résoudre (comp11). Un optimiseur polyvalent et fiable doit pouvoir fonctionner de manière cohérente à travers cette diversité de problèmes sans nécessiter de réglages explicites.

### Processus stochastiques et déterministes

Il a également été observé que les processus stochastiques et déterministes peuvent coexister ou être imbriqués au sein des composants d'un optimiseur. Les heuristiques constructives en sont un exemple, où l'ordre (et donc le choix) des cours est déterministe à chaque étape, tandis que la période d'affectation elle-même est laissée au hasard. Les algorithmes évolutifs, par leur nature, intègrent de l'aléatoire. Il faut donc tenir compte de la variance de la mesure de performance sur plusieurs répétitions de tels algorithmes. Une faible déviation par rapport à la moyenne est préférée car cela permet de faire des affirmations plus solides sur la performance attendue.

### Temps d'exécution

Il est sous-entendu par divers auteurs (Bashab et al., 2020, Wahid et Hussin, 2017, Abdelhalim et El Khayat, 2016) que le temps d'exécution n'est pas une préoccupation primordiale dans le domaine de la planification. Bien que l'efficacité améliorée par l'automatisation soit louable, les universités préparent souvent un emploi du temps des semaines ou des mois à l'avance d'un nouveau trimestre. Dans de nombreux cas, exécuter un optimiseur pendant des jours (ou plus) est peu susceptible de poser des problèmes pratiques. Une exception notable est lorsque les exigences complètes de l'emploi du temps ne sont pas connues ou peuvent faire l'objet de modifications répétées au fil du temps, ce qui est abordé dans les travaux ultérieurs sur la robustesse. Pour une comparaison équitable des algorithmes, les règles de l'ITC2007 exigent l'utilisation d'un seul cœur de CPU. Dans un cas d'utilisation réel, toutefois, les exécutions indépendantes d'algorithmes stochastiques peuvent être parallélisées sur plusieurs cœurs de CPU et le meilleur résultat peut être retenu. De plus, l'utilisation de la programmation basée sur GPU a été mentionnée comme une voie d'investigation future. Les approches évolutives sont taillées pour la parallélisation (à travers les fonctions de fitness ou les populations, par exemple) et pourraient bénéficier du nombre élevé de cœurs et de l'exécution multithread d'un GPU.

### Approches basées sur les populations

Il n'est probablement pas surprenant que les approches basées sur les populations aient souvent été préférées aux alternatives basées sur une solution unique ou sur une trajectoire. Si un taux de convergence plus lent et une charge computationnelle supplémentaire sont jugés acceptables, les solveurs basés sur les populations peuvent offrir des avantages. L'exploration globale est particulièrement importante dans les paysages complexes, et les populations sont généralement plus résilientes face aux pièges des optima locaux. Maintenir une population signifie également qu'une diversité de solutions possibles peut être fournie à un décideur sans connaissance préalable de ses préférences.

### Interaction humaine

Enfin, il est reconnu que certains processus publiés ou solveurs commerciaux pour la planification intègrent (ou nécessitent) une intervention manuelle pour les ajustements. Ceux-ci sont généralement appelés solveurs interactifs ou semi-automatiques. Dans les applications réelles, l'intervention humaine peut s'avérer utile pour gérer un emploi du temps face à des changements imprévus survenant au cours d'un semestre. Comprendre ce qui rend un emploi du temps robuste face à de tels changements est un domaine de recherche ouvert et pourrait potentiellement augmenter la fonctionnalité d'un solveur entièrement automatique. Le travail présenté dans les chapitres 4 et 5, dans lequel la robustesse est finalement incorporée dans un solveur multi-objectifs, est fondé sur ce scénario même.

En disséquant les tendances de l'UCTP dans la littérature, il apparaît que certaines méthodologies montrent plus de promesses que d'autres pour une gamme plus large de formulations. Cependant, aucune approche ou algorithme unique n'est constamment supérieur.

### Chapitres suivants

Dans le chapitre qui suit, une revue est proposée sur une métaheuristique populaire — l'ACO, qui a été bien classée dans la figure 2.11 — et une implémentation connue sous le nom de système de fourmis MAX-MIN est développée. En particulier, des recherches sont menées pour savoir comment la performance est affectée par l'ordre d'affectation des cours/lectures et si des permutations bénéfiques peuvent être prédites par l'apprentissage. Le chapitre 4 présente et développe une approche multi-objectifs, dans laquelle les violations des contraintes souples sont traitées comme des objectifs individuels. Ce travail est développé dans le chapitre 5 suivant, dans lequel les idées de diversité dans l'espace génotypique, des opérateurs de perturbation supplémentaires et, enfin, un objectif de robustesse sont étudiés. Le chapitre 6 résume la thèse et propose des directions pour les travaux futurs.

